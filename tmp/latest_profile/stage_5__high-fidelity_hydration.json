{
  "analytics": {
    "stage": "Stage 5: High-Fidelity Hydration",
    "durationMs": 8674.789165999999,
    "inputCount": 40,
    "outputCount": 40,
    "ratePer1k": 216869.72914999994,
    "timestamp": "2026-01-26T12:08:22.554Z"
  },
  "sampleData": [
    {
      "id": "https://openai.com/index/unrolling-the-codex-agent-loop",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/unrolling-the-codex-agent-loop",
      "title": "Unrolling the Codex agent loop",
      "publishedAt": "Fri, 23 Jan 2026 12:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:16.471Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div><p><a href=\"https://developers.openai.com/codex/cli\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><u><span>Codex CLI</span></u>⁠<span>(opens in a new window)</span></a><span> is our cross-platform local software agent, designed to produce high-quality, reliable software changes while operating safely and efficiently on your machine. We’ve learned a tremendous amount about how to build a world-class software agent </span><a href=\"https://openai.com/index/introducing-o3-and-o4-mini/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>since we first launched the CLI in April</span></u>⁠</a><span>. To unpack those insights, this is the first post in an ongoing series where we’ll explore various aspects of how Codex works, as well as hard-earned lessons. (For an even more granular view on how the Codex CLI is built, check out our open source repository at </span><a href=\"https://github.com/openai/codex\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>https://github.com/openai/codex</span></u>⁠<span>(opens in a new window)</span></a><span>. Many of the finer details of our design decisions are memorialized in GitHub issues and pull requests if you’d like to learn more.)</span></p><p><span>To kick off, we’ll focus on the </span><i><span>agent loop</span></i><span>, which is the core logic in Codex CLI that is responsible for orchestrating the interaction between the user, the model, and the tools the model invokes to perform meaningful software work. We hope this post gives you a good view into the role our agent (or “harness”) plays in making use of an LLM.</span></p><p><span>Before we dive in, a quick note on terminology: at OpenAI, “Codex” encompasses a suite of software agent offerings, including Codex CLI, Codex Cloud, and the Codex VS Code extension. This post focuses on the Codex </span><i><span>harness</span></i><span>, which provides the core agent loop and execution logic that underlies all Codex experiences and is surfaced through the Codex CLI. For ease here, we’ll use the terms “Codex” and “Codex CLI” interchangeably.</span></p><div id=\"the-agent-loop\"><p></p><h2><span>The agent loop</span></h2><p></p></div><p><span>At the heart of every AI agent is something called “the agent loop.” A simplified illustration of the agent loop looks like this:</span></p><p><span>To start, the agent takes </span><i><span>input</span></i><span> from the user to include in the set of textual instructions it prepares for the model known as a </span><i><span>prompt</span></i><span>.</span></p><p><span>The next step is to query the model by sending it our instructions and asking it to generate a response, a process known as </span><i><span>inference</span></i><span>. During inference, the textual prompt is first translated into a sequence of input </span><a href=\"https://platform.openai.com/docs/concepts#tokens\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><u><span>tokens</span></u>⁠<span>(opens in a new window)</span></a><span>—integers that index into the model’s vocabulary. These tokens are then used to sample the model, producing a new sequence of output tokens.</span></p><p><span>The output tokens are translated back into text, which becomes the model’s response. Because tokens are produced incrementally, this translation can happen as the model runs, which is why many LLM-based applications display streaming output. In practice, inference is usually encapsulated behind an API that operates on text, abstracting away the details of tokenization.</span></p><p><span>As the result of the inference step, the model either (1) produces a final response to the user’s original input, or (2) requests a </span><i><span>tool call</span></i><span> that the agent is expected to perform (e.g., “run </span><code><span>ls</span></code><span> and report the output”). In the case of (2), the agent executes the tool call and appends its output to the original prompt. This output is used to generate a new input that’s used to re-query the model; the agent can then take this new information into account and try again.</span></p><p><span>This process repeats until the model stops emitting tool calls and instead produces a message for the user (referred to as an </span><i><span>assistant message</span></i><span> in OpenAI models). In many cases, this message directly answers the user’s original request, but it may also be a follow-up question for the user.</span></p><p><span>Because the agent can execute tool calls that modify the local environment, its “output” is not limited to the assistant message. In many cases, the primary output of a software agent is the code it writes or edits on your machine. Nevertheless, each turn always ends with an assistant message—such as “I added the </span><code><span>architecture.md</span></code><span> you asked for”—which signals a termination state in the agent loop. From the agent’s perspective, its work is complete and control returns to the user.</span></p><p><span>The journey from </span><i><span>user input</span></i><span> to </span><i><span>agent response</span></i><span> shown in the diagram is referred to as one </span><i><span>turn</span></i><span> of a conversation (a </span><i><span>thread </span></i><span>in Codex). Though this </span><i><span>conversation turn</span></i><span> can include many iterations between the </span><b><span>model inference</span></b><span> and </span><b><span>tool calls</span></b><span>). Every time you send a new message to an existing conversation, the conversation history is included as part of the prompt for the new turn, which includes the messages and tool calls from previous turns:</span></p><p><span>This means that as the conversation grows, so does the length of the prompt used to sample the model. This length matters because every model has a </span><i><span>context window</span></i><span>, which is the maximum number of tokens it can use for one inference call. Note this window includes both input </span><i><span>and</span></i><span> output tokens. As you might imagine, an agent could decide to make hundreds of tool calls in a single turn, potentially exhausting the context window. For this reason, </span><i><span>context window management</span></i><span> is one of the agent’s many responsibilities. Now, let’s dive in to see how Codex runs the agent loop.</span></p><div id=\"model-inference\"><p></p><h2><span>Model inference</span></h2><p></p></div><p><span>The Codex CLI sends HTTP requests to the </span><a href=\"https://platform.openai.com/docs/api-reference/responses\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><u><span>Responses API</span></u>⁠<span>(opens in a new window)</span></a><span> to run model inference. We’ll examine how information flows through Codex, which uses the Responses API to drive the agent loop.</span></p><div><ul><li><a href=\"https://github.com/openai/codex/blob/d886a8646cb8d3671c3029d08ae8f13fa6536899/codex-rs/core/src/model_provider_info.rs#L141\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>When using ChatGPT login</span></u>⁠<span>(opens in a new window)</span></a><span> with the Codex CLI, it uses </span><code><span>https://chatgpt.com/backend-api/codex/responses</span></code><span> as the endpoint</span></li><li><a href=\"https://github.com/openai/codex/blob/d886a8646cb8d3671c3029d08ae8f13fa6536899/codex-rs/core/src/model_provider_info.rs#L143\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>When using API-key authentication</span></u>⁠<span>(opens in a new window)</span></a><span> with OpenAI hosted models, it uses </span><code><span>https://api.openai.com/v1/responses</span></code><span> as the endpoint</span></li><li><span>When running Codex CLI with </span><code><span>--oss</span></code><span> to use </span><a href=\"https://openai.com/index/introducing-gpt-oss/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>gpt-oss</span></u>⁠</a><span> with </span><a href=\"https://github.com/openai/codex/pull/8798\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>ollama 0.13.4+</span></u>⁠<span>(opens in a new window)</span></a><span> or </span><a href=\"https://lmstudio.ai/blog/openresponses\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>LM Studio 0.3.39+</span></u>⁠<span>(opens in a new window)</span></a><span>, it defaults to </span><code><span>http://localhost:11434/v1/responses</span></code><span> running locally on your computer</span></li><li><span>Codex CLI can be used with the Responses API hosted by a cloud provider such as Azure</span></li></ul></div><p><span>Let’s explore how Codex creates the prompt for the first inference call in a conversation.</span></p><p><span>As an end user, you don’t specify the prompt used to sample the model verbatim when you query the Responses API. Instead, you specify various input types as part of your query, and the Responses API server decides how to structure this information into a prompt that the model is designed to consume. You can think of the prompt as a “list of items”; this section will explain how your query gets transformed into that list.</span></p><p><span>In the initial prompt, every item in the list is associated with a role. The </span><code><span>role</span></code><span> indicates how much weight the associated content should have and is one of the following values (in decreasing order of priority): </span><code><span>system</span></code><span>, </span><code><span>developer</span></code><span>, </span><code><span>user</span></code><span>, </span><code><span>assistant</span></code><span>.</span></p><div><ul><li><a href=\"https://platform.openai.com/docs/api-reference/responses/create#responses_create-instructions\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><code><u><span>instructions</span></u></code>⁠<span>(opens in a new window)</span></a><span>: system (or developer) message inserted into the model’s context</span></li><li><a href=\"https://platform.openai.com/docs/api-reference/responses/create#responses_create-tools\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><code><u><span>tools</span></u></code>⁠<span>(opens in a new window)</span></a><span>: a list of tools the model may call while generating a response</span></li><li><a href=\"https://platform.openai.com/docs/api-reference/responses/create#responses_create-input\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><code><u><span>input</span></u></code>⁠<span>(opens in a new window)</span></a><span>: a list of text, image, or file inputs to the model</span></li></ul></div><p><span>The </span><code><span>tools</span></code><span> field is a list of tool definitions that conform to a schema defined by the Responses API. For Codex, this includes tools that are provided by the Codex CLI, tools that are provided by the Responses API that should be made available to Codex, as well as tools provided by the user, usually via MCP servers:</span></p><p><span>1. A message with </span><code><span>role=developer</span></code><span> that describes the sandbox that </span><i><span>applies only to the Codex-provided </span></i><code><i><span>shell</span></i></code><i><span> tool</span></i><span> defined in the </span><code><span>tools</span></code><span> section. That is, other tools, such as those provided from MCP servers, are not sandboxed by Codex and are responsible for enforcing their own guardrails.</span></p><p><span>2. (Optional) A message with </span><code><span>role=developer</span></code><span> whose contents are the </span><code><span>developer_instructions</span></code><span> value read from the user’s </span><code><span>config.toml</span></code><span> file.</span></p><p><span>3. (Optional) A message with </span><code><span>role=user</span></code><span> whose contents are the “user instructions,” which are not sourced from a single file but are </span><a href=\"https://github.com/openai/codex/blob/99f47d6e9a3546c14c43af99c7a58fa6bd130548/codex-rs/core/src/project_doc.rs#L37-L42\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>aggregated across multiple sources</span></u>⁠<span>(opens in a new window)</span></a><span>. In general, more specific instructions appear later:</span></p><div><ul><li><span>Contents of </span><code><span>AGENTS.override.md</span></code><span> and </span><code><span>AGENTS.md</span></code><span> in </span><code><span>$CODEX_HOME</span></code></li><li><span>Subject to a limit (32 KiB, by default), look in each folder from the Git/project root of the </span><code><span>cwd</span></code><span> (if it it exists) up to the </span><code><span>cwd</span></code><span> itself: add the contents of any of </span><code><span>AGENTS.override.md</span></code><span>, </span><code><span>AGENTS.md</span></code><span>, or any filename specified by </span><code><span>project_doc_fallback_filenames in config.toml</span></code></li><li><span>If any </span><a href=\"https://developers.openai.com/codex/skills/\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><u><span>skills</span></u>⁠<span>(opens in a new window)</span></a><span> have been configured:</span></li></ul></div><p><span>Once Codex has done all of the above computation to initialize the </span><code><span>input</span></code><span>, it appends the user message to start the conversation.</span></p><p><span>The previous examples focused on the content of each message, but note that each element of </span><code><span>input</span></code><span> is a JSON object with </span><code><span>type</span></code><span>, </span><a href=\"https://www.reddit.com/r/OpenAI/comments/1hgxcgi/what_is_the_purpose_of_the_new_developer_role_in/\" target=\"_blank\" rel=\"noopener noreferrer\"><code><span>role</span></code>⁠<span>(opens in a new window)</span></a><span>, and </span><code><span>content</span></code><span> as follows:</span></p><p><span>Once Codex builds up the full JSON payload to send to the Responses API, it then makes the HTTP POST request with an </span><code><span>Authorization</span></code><span> header depending on how the Responses API endpoint is configured in </span><code><span>~/.codex/config.toml</span></code><span> (additional HTTP headers and query parameters are added if specified).</span></p><p><span>When an OpenAI Responses API server receives the request, it uses the JSON to derive the prompt for the model as follows (to be sure, a custom implementation of the Responses API could make a different choice):</span></p><p><span>As you can see, the order of the first three items in the prompt is determined by the server, not the client. That said, of those three items, only the content of the </span><i><span>system message</span></i><span> is also controlled by the server, as the </span><code><span>tools</span></code><span> and </span><code><span>instructions</span></code><span> are determined by the client. These are followed by the </span><code><span>input</span></code><span> from the JSON payload to complete the prompt.</span></p><p><span>Now that we have our prompt, we are ready to sample the model.</span></p><p><span>This HTTP request to the Responses API initiates the first “turn” of a conversation in Codex. The server replies with a Server-Sent Events (</span><a href=\"https://en.wikipedia.org/wiki/Server-sent_events\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>SSE</span></u>⁠<span>(opens in a new window)</span></a><span>) stream. The </span><code><span>data</span></code><span> of each event is a JSON payload with a </span><code><span>\"type\"</span></code><span> that starts with </span><code><span>\"response\"</span></code><span>, which could be something like this (a full list of events can be found in our </span><a href=\"https://platform.openai.com/docs/api-reference/responses-streaming\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><u><span>API docs</span></u>⁠<span>(opens in a new window)</span></a><span>):</span></p><p><span>Codex </span><a href=\"https://github.com/openai/codex/blob/2a68b74b9bf16b64e285495c1b149d7d6ac8bdf4/codex-rs/codex-api/src/sse/responses.rs#L334-L342\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>consumes the stream of events</span></u>⁠<span>(opens in a new window)</span></a><span> and republishes them as internal event objects that can be used by a client. Events like </span><code><span>response.output_text.delta</span></code><span> are used to support streaming in the UI, whereas other events like </span><code><span>response.output_item.added</span></code><span> are transformed into objects that are appended to the </span><code><span>input</span></code><span> for subsequent Responses API calls.</span></p><p><span>Suppose the first request to the Responses API includes two </span><code><span>response.output_item.done</span></code><span> events: one with </span><code><span>type=reasoning</span></code><span> and one with </span><code><span>type=function_call</span></code><span>. These events must be represented in the </span><code><span>input</span></code><span> field of the JSON when we query the model again with the response to the tool call:&nbsp;</span></p><p><span>The resulting prompt used to sample the model as part of the subsequent query would look like this:</span></p><p><span>In particular, note how the old prompt </span><i><span>is an exact prefix</span></i><span> of the new prompt. This is intentional, as this makes subsequent requests much more efficient because it enables us to take advantage of </span><i><span>prompt caching</span></i><span> (which we’ll discuss in the next section on performance).</span></p><p><span>Looking back at our first diagram of the agent loop, we see that there could be many iterations between inference and tool calling. The prompt may continue to grow until we finally receive an assistant message, indicating the end of the turn:</span></p><p><span>In the Codex CLI, we present the assistant message to the user and focus the composer to indicate to the user that it’s their “turn” to continue the conversation. If the user responds, both the assistant message from the previous turn, as well as the user’s new message, must be appended to the </span><code><span>input</span></code><span> in the Responses API request to start the new turn:</span></p><p><span>Once again, because we are continuing a conversation, the length of the </span><code><span>input</span></code><span> we send to the Responses API keeps increasing:</span></p><p><span>Let’s examine what this ever-growing prompt means for performance.</span></p><p><span>You might be asking yourself, “Wait, isn’t the agent loop </span><i><span>quadratic</span></i><span> in terms of the amount of JSON sent to the Responses API over the course of the conversation?” And you would be right. While the Responses API does support an optional </span><a href=\"https://platform.openai.com/docs/api-reference/responses/create#responses_create-previous_response_id\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><code><u><span>previous_response_id</span></u></code>⁠<span>(opens in a new window)</span></a><span> parameter to mitigate this issue, Codex does not use it today, primarily to keep requests fully stateless and to support Zero Data Retention (ZDR) configurations.</span></p><div><p>Avoiding <code>previous_response_id</code> simplifies things for the provider of the Responses API because it ensures that every request is <i>stateless</i>. This also makes it straightforward to support customers who have opted into <a href=\"https://platform.openai.com/docs/guides/migrate-to-responses#4-decide-when-to-use-statefulness\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><u>Zero Data Retention (ZDR)</u>⁠<span>(opens in a new window)</span></a>, as storing the data required to support <code>previous_response_id</code> would be at odds with ZDR. Note that ZDR customers do not sacrifice the ability to benefit from proprietary reasoning messages from prior turns, as the associated <code>encrypted_content</code> can be decrypted on the server. (OpenAI persists a ZDR customer’s decryption key, but not their data.) See PRs <a href=\"https://github.com/openai/codex/pull/642\" target=\"_blank\" rel=\"noopener noreferrer\"><u>#642</u>⁠<span>(opens in a new window)</span></a> and <a href=\"https://github.com/openai/codex/pull/1641\" target=\"_blank\" rel=\"noopener noreferrer\"><u>#1641</u>⁠<span>(opens in a new window)</span></a> for the related changes to Codex to support ZDR.</p></div><p><span>Generally, the cost of sampling the model dominates the cost of network traffic, making sampling the primary target of our efficiency efforts. This is why prompt caching is so important, as it enables us to reuse computation from a previous inference call. When we get cache hits, </span><i><span>sampling the model is linear rather than quadratic</span></i><span>. Our </span><a href=\"https://platform.openai.com/docs/guides/prompt-caching#structuring-prompts\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><u><span>prompt caching </span></u>⁠<span>(opens in a new window)</span></a><span>documentation explains this in more detail:</span></p><p><i><span>Cache hits are only possible for exact prefix matches within a prompt. To realize caching benefits, place static content like instructions and examples at the beginning of your prompt, and put variable content, such as user-specific information, at the end. This also applies to images and tools, which must be identical between requests.</span></i></p><p><span>With this in mind, let’s consider what types of operations could cause a “cache miss” in Codex:</span></p><div><ul><li><span>Changing the </span><code><span>tools</span></code><span> available to the model in the middle of the conversation.</span></li><li><span>Changing the </span><code><span>model</span></code><span> that is the target of the Responses API request (in practice, this changes the third item in the original prompt, as it contains model-specific instructions).</span></li><li><span>Changing the sandbox configuration, approval mode, or current working directory.</span></li></ul></div><p><span>The Codex team must be diligent when introducing new features in the Codex CLI that could compromise prompt caching. As an example, our initial support for MCP tools introduced a </span><a href=\"https://github.com/openai/codex/pull/2611\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>bug where we failed to enumerate the tools in a consistent order</span></u>⁠<span>(opens in a new window)</span></a><span>, causing cache misses. Note that MCP tools can be particularly tricky because MCP servers can change the list of tools they provide on the fly via a </span><a href=\"https://modelcontextprotocol.io/specification/2025-11-25/server/tools#list-changed-notification\" target=\"_blank\" rel=\"noopener noreferrer\"><code><u><span>notifications/tools/list_changed</span></u></code>⁠<span>(opens in a new window)</span></a><span> notification. Honoring this notification in the middle of a long conversation can cause an expensive cache miss.</span></p><p><span>When possible, we handle configuration changes that happen mid-conversation by appending a </span><i><span>new</span></i><span> message to </span><code><span>input</span></code><span> to reflect the change rather than modifying an earlier message:</span></p><div><ul><li><span>If the sandbox configuration or approval mode changes, we </span><a href=\"https://github.com/openai/codex/blob/99f47d6e9a3546c14c43af99c7a58fa6bd130548/codex-rs/core/src/codex.rs#L1037-L1057\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>insert</span></u>⁠<span>(opens in a new window)</span></a><span> a new </span><code><span>role=developer</span></code><span> message with the same format as the original </span><code><span>&lt;permissions instructions&gt;</span></code><span> item.</span></li><li><span>If the current working directory changes, we </span><a href=\"https://github.com/openai/codex/blob/99f47d6e9a3546c14c43af99c7a58fa6bd130548/codex-rs/core/src/codex.rs#L1017-L1035\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>insert</span></u>⁠<span>(opens in a new window)</span></a><span> a new </span><code><span>role=user</span></code><span> message with the same format as the original </span><code><span>&lt;environment_context&gt;</span></code><span>.</span></li></ul></div><p><span>We go to great lengths to ensure cache hits for performance. There’s another key resource we have to manage: the context window.</span></p><p><span>Our general strategy to avoid running out of context window is to </span><i><span>compact</span></i><span> the conversation once the number of tokens exceeds some threshold. Specifically, we replace the </span><code><span>input</span></code><span> with a new, smaller list of items that is representative of the conversation, enabling the agent to continue with an understanding of what has happened thus far. An early </span><a href=\"https://github.com/openai/codex/pull/1527\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>implementation of compaction</span></u>⁠<span>(opens in a new window)</span></a><span> required the user to manually invoke the </span><code><span>/compact</span></code><span> command, which would query the Responses API using the existing conversation plus custom instructions for </span><a href=\"https://github.com/openai/codex/blob/e2c994e32a31415e87070bef28ed698968d2e549/SUMMARY.md\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>summarization</span></u>⁠<span>(opens in a new window)</span></a><span>. Codex used the resulting assistant message containing the summary </span><a href=\"https://github.com/openai/codex/blob/e2c994e32a31415e87070bef28ed698968d2e549/codex-rs/core/src/codex.rs#L1424\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>as the new </span></u><code><u><span>input</span></u></code>⁠<span>(opens in a new window)</span></a><span> for subsequent conversation turns.</span></p><p><span>Since then, the Responses API has evolved to support a special </span><a href=\"https://platform.openai.com/docs/guides/conversation-state#compaction-advanced\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><code><u><span>/responses/compact</span></u></code><u><span> endpoint</span></u>⁠<span>(opens in a new window)</span></a><span> that performs compaction more efficiently. It returns </span><a href=\"https://platform.openai.com/docs/api-reference/responses/compacted-object\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><u><span>a list of items</span></u>⁠<span>(opens in a new window)</span></a><span> that can be used in place of the previous </span><code><span>input</span></code><span> to continue the conversation while freeing up the context window. This list includes a special </span><code><span>type=compaction</span></code><span> item with an opaque </span><code><span>encrypted_content</span></code><span> item that preserves the model’s latent understanding of the original conversation. Now, Codex automatically uses this endpoint to compact the conversation when the </span><a href=\"https://github.com/openai/codex/blob/99f47d6e9a3546c14c43af99c7a58fa6bd130548/codex-rs/core/src/codex.rs#L2558-L2560\" target=\"_blank\" rel=\"noopener noreferrer\"><code><u><span>auto_compact_limit</span></u></code>⁠<span>(opens in a new window)</span></a><span> is exceeded.</span></p><div id=\"coming-next\"><p></p><h2><span>Coming next</span></h2><p></p></div><p><span>We’ve introduced the Codex agent loop and walked through how Codex crafts and manages its context when querying a model. Along the way, we highlighted practical considerations and best practices that apply to anyone building an agent loop on top of the Responses API.</span></p><p><span>While the agent loop provides the foundation for Codex, it’s only the beginning. In upcoming posts, we’ll dig into the CLI’s architecture, explore how tool use is implemented, and take a closer look at Codex’s sandboxing model.</span></p></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/scaling-postgresql",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/scaling-postgresql",
      "title": "Scaling PostgreSQL to power 800 million ChatGPT users",
      "publishedAt": "Thu, 22 Jan 2026 12:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:15.444Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div><p><span>For years, PostgreSQL has been one of the most critical, under-the-hood data systems powering core products like ChatGPT and OpenAI’s API. As our user base grows rapidly, the demands on our databases have increased exponentially, too. Over the past year, our PostgreSQL load has grown by more than 10x, and it continues to rise quickly.</span></p><p><span>Our efforts to advance our production infrastructure to sustain this growth revealed a new insight: PostgreSQL can be scaled to reliably support much larger read-heavy workloads than many previously thought possible. The system (initially created by a team of scientists at University of California, Berkeley) has enabled us to support massive global traffic with a single primary </span><a href=\"https://learn.microsoft.com/en-us/azure/postgresql/overview\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>Azure PostgreSQL flexible server instance</span></u>⁠<span>(opens in a new window)</span></a><span> and nearly 50 read replicas spread over multiple regions globally. This is the story of how we’ve scaled PostgreSQL at OpenAI to support millions of queries per second for 800 million users through rigorous optimizations and solid engineering; we’ll also cover key takeaways we learned along the way.</span></p><div id=\"cracks-in-our-initial-design\"><p></p><h2><span>Cracks in our initial design</span></h2><p></p></div><p><span>After the launch of ChatGPT, traffic grew at an unprecedented rate. To support it, we rapidly implemented extensive optimizations at both the application and PostgreSQL database layers, scaled up by increasing the instance size, and scaled out by adding more read replicas. This architecture has served us well for a long time. With ongoing improvements, it continues to provide ample runway for future growth.</span></p><p><span>It may sound surprising that a single-primary architecture can meet the demands of OpenAI’s scale; however, making this work in practice isn’t simple. We’ve seen several SEVs caused by Postgres overload, and they often follow the same pattern: an upstream issue causes a sudden spike in database load, such as widespread cache misses from a caching-layer failure, a surge of expensive multi-way joins saturating CPU, or a write storm from a new feature launch. As resource utilization climbs, query latency rises and requests begin to time out. Retries then further amplify the load, triggering a vicious cycle with the potential to degrade the entire ChatGPT and API services.</span></p><p><span>Although PostgreSQL scales well for our read-heavy workloads, we still encounter challenges during periods of high write traffic. This is largely due to PostgreSQL’s multiversion concurrency control (MVCC) implementation, which makes it less efficient for write-heavy workloads. For example, when a query updates a tuple or even a single field, the entire row is copied to create a new version. Under heavy write loads, this results in significant write amplification. It also increases read amplification, since queries must scan through multiple tuple versions (dead tuples) to retrieve the latest one. MVCC introduces additional challenges such as table and index bloat, increased index maintenance overhead, and complex autovacuum tuning. (You can find a deep-dive on these issues in a blog I wrote with Prof. Andy Pavlo at Carnegie Mellon University called </span><a href=\"https://www.cs.cmu.edu/~pavlo/blog/2023/04/the-part-of-postgresql-we-hate-the-most.html\" target=\"_blank\" rel=\"noopener noreferrer\"><i><u><span>The Part of PostgreSQL We Hate the Most</span></u></i>⁠<span>(opens in a new window)</span></a><span>, </span><a href=\"https://en.wikipedia.org/wiki/PostgreSQL#cite_note-37\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>cited</span></u>⁠<span>(opens in a new window)</span></a><span> in the PostgreSQL Wikipedia page.)</span></p><div id=\"scaling-postgresql-to-millions-of-qps\"><p></p><h2><span>Scaling PostgreSQL to millions of QPS</span></h2><p></p></div><p><span>To mitigate these limitations and reduce write pressure, we’ve migrated, and continue to migrate, shardable (i.e. workloads that can be horizontally partitioned), write-heavy workloads to sharded systems such as Azure Cosmos DB, optimizing application logic to minimize unnecessary writes. We also no longer allow adding new tables to the current PostgreSQL deployment. New workloads default to the sharded systems.</span></p><p><span>Even as our infrastructure has evolved, PostgreSQL has remained unsharded, with a single primary instance serving all writes. The primary rationale is that sharding existing application workloads would be highly complex and time-consuming, requiring changes to hundreds of application endpoints and potentially taking months or even years. Since our workloads are primarily read-heavy, and we’ve implemented extensive optimizations, the current architecture still provides ample headroom to support continued traffic growth. While we’re not ruling out sharding PostgreSQL in the future, it’s not a near-term priority given the sufficient runway we have for current and future growth.</span></p><p><span>In the following sections, we’ll dive into the challenges we faced and the extensive optimizations we implemented to address them and prevent future outages, pushing PostgreSQL to its limits and scaling it to millions of queries per second (QPS).</span></p><p><i><span>Challenge: With only one writer, a single-primary setup can’t scale writes. Heavy write spikes can quickly overload the primary and impact services like ChatGPT and our API.</span></i></p><p><span>Solution: We minimize load on the primary as much as possible—both reads and writes—to ensure it has sufficient capacity to handle write spikes. Read traffic is offloaded to replicas wherever possible. However, some read queries must remain on the primary because they’re part of write transactions. For those, we focus on ensuring they’re efficient and avoid slow queries. For write traffic, we’ve migrated shardable, write-heavy workloads to sharded systems such as Azure CosmosDB. Workloads that are harder to shard but still generate high write volume take longer to migrate, and that process is still ongoing. We also aggressively optimized our applications to reduce write load; for example, we’ve fixed application bugs that caused redundant writes and introduced lazy writes, where appropriate, to smooth traffic spikes. In addition, when backfilling table fields, we enforce strict rate limits to prevent excessive write pressure.</span></p><p><i><span>Challenge: We identified several expensive queries in PostgreSQL. In the past, sudden volume spikes in these queries would consume large amounts of CPU, slowing both ChatGPT and API requests.</span></i></p><p><span>Solution: A few expensive queries, such as those joining many tables together, can significantly degrade or even bring down the entire service. We need to continuously optimize PostgreSQL queries to ensure they’re efficient and avoid common Online Transaction Processing (OLTP) anti-patterns. For example, we once identified an extremely costly query that joined 12 tables, where spikes in this query were responsible for past high-severity SEVs. We should avoid complex multi-table joins whenever possible. If joins are necessary, we learned to consider breaking down the query and move complex join logic to the application layer instead. Many of these problematic queries are generated by Object-Relational Mapping frameworks (ORMs), so it’s important to carefully review the SQL they produce and ensure it behaves as expected. It’s also common to find long-running idle queries in PostgreSQL. Configuring timeouts like idle_in_transaction_session_timeout is essential to prevent them from blocking autovacuum.</span></p><p><i><span>Challenge: If a read replica goes down, traffic can still be routed to other replicas. However, relying on a single writer means having a single point of failure—if it goes down, the entire service is affected.</span></i></p><p><span>Solution: Most critical requests only involve read queries. To mitigate the single point of failure in the primary, we offloaded those reads from the writer to replicas, ensuring those requests can continue serving even if the primary goes down. While write operations would still fail, the impact is reduced; it’s no longer a SEV0 since reads remain available.</span></p><p><span>To mitigate primary failures, we run the primary in High-Availability (HA) mode with a hot standby, a continuously synchronized replica that is always ready to take over serving traffic. If the primary goes down or needs to be taken offline for maintenance, we can quickly promote the standby to minimize downtime. The Azure PostgreSQL team has done significant work to ensure these failovers remain safe and reliable even under very high load. To handle read replica failures, we deploy multiple replicas in each region with sufficient capacity headroom, ensuring that a single replica failure doesn’t lead to a regional outage.</span></p><p><i><span>Challenge: We often encounter situations where certain requests consume a disproportionate amount of resources on PostgreSQL instances. This can lead to degraded performance for other workloads running on the same instances. For example, a new feature launch can introduce inefficient queries that heavily consume PostgreSQL CPU, slowing down requests for other critical features.</span></i></p><p><span>Solution: To mitigate the “noisy neighbor” problem, we isolate workloads onto dedicated instances to ensure that sudden spikes in resource-intensive requests don’t impact other traffic. Specifically, we split requests into low-priority and high-priority tiers and route them to separate instances. This way, even if a low-priority workload becomes resource-intensive, it won’t degrade the performance of high-priority requests. We apply the same strategy across different products and services as well, so that activity from one product does not affect the performance or reliability of another.</span></p><p><i><span>Challenge: Each instance has a maximum connection limit (5,000 in Azure PostgreSQL). It’s easy to run out of connections or accumulate too many idle ones. We’ve previously had incidents caused by connection storms that exhausted all available connections.</span></i></p><p><span>Solution: We deployed PgBouncer as a proxy layer to pool database connections. Running it in statement or transaction pooling mode allows us to efficiently reuse connections, greatly reducing the number of active client connections. This also cuts connection setup latency: in our benchmarks, the average connection time dropped from 50 milliseconds (ms) to 5 ms. Inter-region connections and requests can be expensive, so we co-locate the proxy, clients, and replicas in the same region to minimize network overhead and connection use time. Moreover, PgBouncer must be configured carefully. Settings like idle timeouts are critical to prevent connection exhaustion.</span></p><p><i><span>Challenge: A sudden spike in cache misses can trigger a surge of reads on the PostgreSQL database, saturating CPU and slowing user requests.</span></i></p><p><span>Solution: To reduce read pressure on PostgreSQL, we use a caching layer to serve most of the read traffic. However, when cache hit rates drop unexpectedly, the burst of cache misses can push a large volume of requests directly to PostgreSQL. This sudden increase in database reads consumes significant resources, slowing down the service. To prevent overload during cache-miss storms, we implement a cache locking (and leasing) mechanism so that only a single reader that misses on a particular key fetches the data from PostgreSQL. When multiple requests miss on the same cache key, only one request acquires the lock and proceeds to retrieve the data and repopulate the cache. All other requests wait for the cache to be updated rather than all hitting PostgreSQL at once. This significantly reduces redundant database reads and protects the system from cascading load spikes.</span></p><p><i><span>Challenge: The primary streams Write Ahead Log (WAL) data to every read replica. As the number of replicas increases, the primary must ship WAL to more instances, increasing pressure on both network bandwidth and CPU. This causes higher and more unstable replica lag, which makes the system harder to scale reliably.</span></i></p><p><span>Solution: We operate nearly 50 read replicas across multiple geographic regions to minimize latency. However, with the current architecture, the primary must stream WAL to every replica. Although it currently scales well with very large instance types and high-network bandwidth, we can’t keep adding replicas indefinitely without eventually overloading the primary. To address this, we’re collaborating with the Azure PostgreSQL team on </span><a href=\"https://www.postgresql.org/docs/current/warm-standby.html#CASCADING-REPLICATION\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>cascading replication</span></u>⁠<span>(opens in a new window)</span></a><span>, where intermediate replicas relay WAL to downstream replicas. This approach allows us to scale to potentially over a hundred replicas without overwhelming the primary. However, it also introduces additional operational complexity, particularly around failover management. The feature is still in testing; we’ll ensure it’s robust and can fail over safely before rolling it out to production.</span></p><p><i><span>Challenge: A sudden traffic spike on specific endpoints, a surge of expensive queries, or a retry storm can quickly exhaust critical resources such as CPU, I/O, and connections, which causes widespread service degradation.</span></i></p><p><span>Solution: We implemented rate-limiting across multiple layers—application, connection pooler, proxy, and query—to prevent sudden traffic spikes from overwhelming database instances and triggering cascading failures. It’s also crucial to avoid overly short retry intervals, which can trigger retry storms. We also enhanced the ORM layer to support rate limiting and when necessary, fully block specific query digests. This targeted form of load shedding enables rapid recovery from sudden surges of expensive queries.</span></p><p><i><span>Challenge: Even a small schema change, such as altering a column type, can trigger </span></i><a href=\"https://www.crunchydata.com/blog/when-does-alter-table-require-a-rewrite\" target=\"_blank\" rel=\"noopener noreferrer\"><i><u><span>a full table rewrite</span></u></i>⁠<span>(opens in a new window)</span></a><i><span>. We therefore apply schema changes cautiously—limiting them to lightweight operations and avoiding any that rewrite entire tables.</span></i></p><p><span>Solution: Only lightweight schema changes are permitted, such as adding or removing certain columns that do not trigger a full table rewrite. We enforce a strict 5-second timeout on schema changes. Creating and dropping indexes concurrently is allowed. Schema changes are restricted to existing tables. If a new feature requires additional tables, they must be in alternative sharded systems such as Azure CosmosDB rather than PostgreSQL. When backfilling a table field, we apply strict rate limits to prevent write spikes. Although this process can sometimes take over a week, it ensures stability and avoids any production impact.</span></p><div id=\"results-and-the-road-ahead\"><p></p><h2><span>Results and the road ahead</span></h2><p></p></div><p><span>This effort demonstrates that with the right design and optimizations, Azure PostgreSQL can be scaled to handle the largest production workloads. PostgreSQL handles millions of QPS for read-heavy workloads, powering OpenAI’s most critical products like ChatGPT and the API platform. We added nearly 50 read replicas, while keeping replication lag near zero, maintained low-latency reads across geo-distributed regions, and built sufficient capacity headroom to support future growth.</span></p><p><span>This scaling works while still minimizing latency and improving reliability. We consistently deliver low double-digit millisecond p99 client-side latency and five-nines availability in production. And over the past 12 months, we’ve had only one SEV-0 PostgreSQL incident (it occurred during the </span><a href=\"https://newsletter.pragmaticengineer.com/p/chatgpt-images\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>viral launch</span></u>⁠<span>(opens in a new window)</span></a><span> of ChatGPT ImageGen, when write traffic suddenly surged by more than 10x as over 100 million new users signed up within a week.)</span></p><p><span>While we’re happy with how far PostgreSQL has taken us, we continue to push its limits to ensure we have sufficient runway for future growth. We’ve already migrated the shardable write-heavy workloads to our sharded systems like CosmosDB. The remaining write-heavy workloads are more challenging to shard—we’re actively migrating those as well to further offload writes from the PostgreSQL primary. We’re also working with Azure to enable cascading replication so we can safely scale to significantly more read replicas.</span></p><p><span>Looking ahead, we’ll continue to explore additional approaches to further scale, including sharded PostgreSQL or alternative distributed systems, as our infrastructure demands continue to grow.</span></p></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/praktika",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/praktika",
      "title": "Inside Praktika's conversational approach to language learning",
      "publishedAt": "Thu, 22 Jan 2026 05:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:15.880Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div><p><span>Praktika was born from a deeply personal insight: language unlocks opportunity.&nbsp;</span></p><p><span>Co-founders Adam Turaev, Anton Marin, and Ilya Chernyakov all grew up navigating new countries after their families immigrated in search of better opportunities. English quickly became essential, not just for school, but for work, mobility, and belonging.</span></p><p><span>“Learning English was never just about communication,” Turaev said. “It opened doors to international work and career growth.”&nbsp;</span></p><p><span>But traditional language education fell short. Despite years of study, the founders found that while they could read and write fluently, they struggled to speak confidently when it mattered most: at work, in meetings, and in daily life. The gap between classroom learning and real-world fluency was wider than they’d imagined.</span></p><p><a href=\"https://praktika.ai/\" target=\"_blank\" rel=\"noopener noreferrer\"><span>Praktika</span>⁠<span>(opens in a new window)</span></a><span> was built to close that gap. It’s a language learning app designed to help people build real-world fluency through daily conversations, with personalized AI tutors who guide them through interactive, goal-based lessons. Users include students preparing for exams, professionals working on job-related language skills, and immigrants building new lives in foreign countries.&nbsp;</span></p><div id=\"building-a-multi-agent-tutoring-system-that-adapts-and-improvises\"><p></p><h2><span>Building a multi-agent tutoring system that adapts and improvises&nbsp;</span></h2><p></p></div><p><span>As the product matured, Praktika moved beyond a single-model architecture into a multi-agent system designed to mirror how real tutors adapt lessons in real time.&nbsp;</span></p><p><b><span>Lesson Agent </span></b><span>is the primary conversation agent, interacting with learners as the tutor. Running on GPT‑5.2, it blends tutor personality, lesson context, learner goals, and recent conversations to deliver lessons that feel natural and unscripted. This is the point where the system starts to feel like a real tutor rather than a scripted experience.</span></p><p><span>Running continuously in the background, </span><b><span>Student Progress Agent</span></b><span> tracks the learner’s language performance across interactions. Using GPT‑5.2, this agent monitors fluency, accuracy, vocabulary usage, and recurring mistakes. This data forms a continuous feedback loop that informs both the Lesson Agent’s in-session behavior and the longer-term learning strategy, allowing the experience to evolve naturally over time.</span></p><p><b><span>Learning Planning Agent </span></b><span>focuses on shaping the learner’s long-term progression. Grounded in the learner’s individual learning goal, it uses insights from the Student Progress Agent to determine what to learn next, how to sequence skills, and which activities will be most effective. Powered by GPT‑5 Pro, its role is to continuously adapt the learning plan so progress remains personalized, efficient, and aligned with the learner’s desired outcome.</span></p><p><span>All agents share access to a persistent memory layer that stores learner goals, preferences, and past mistakes. Rather than preloading context, Praktika retrieves memory immediately after the learner speaks, ensuring responses are grounded in the most relevant, up-to-date signal.</span></p><p><span>“The system can switch to a completely different exercise if the learner isn’t feeling it,” says Turaev. “That brings the magic back. It starts to feel much closer to a real human tutor.”</span></p><div id=\"making-ai-conversations-feel-like-a-live-exchange\"><p></p><h2><span>Making AI conversations feel like a live exchange&nbsp;</span></h2><p></p></div><p><span>For conversational learning to feel natural, memory has to work the way it does in real life. Praktika’s memory layer retrieves relevant context only </span><i><span>after</span></i><span> the learner finishes speaking. That allows the tutor to respond to what was just said, not what it anticipated.</span></p><p><span>“If a learner makes a mistake right now, the tutor responds to </span><i><span>that mistake</span></i><span>, not one from yesterday,” says co-founder and CEO Adam Turaev. “That timing difference is subtle, but it’s what makes the interaction feel attentive instead of robotic.”</span></p><p><span>Speech recognition plays a similar role. Language learners hesitate, restart sentences, and pronounce words imperfectly. Praktika uses Transcription API to handle fragmented, accented, and non-native speech more reliably than traditional systems trained on fluent speech. That lets learners focus on communicating without being penalized for their beginner status.</span></p><p><span>Together, memory timing and speech recognition form a single loop: listen carefully, recall the right context, and respond immediately.</span></p><div id=\"turning-model-improvements-into-more-effective-learning-experiences\"><p></p><h2><span>Turning model improvements into more effective learning experiences</span></h2><p></p></div><p><span>Early versions of Praktika’s product paired expressive avatars with rule-based NLP and the first davinci models, but conversations still felt constrained. With the release of </span><b><span>GPT‑3.5</span></b><span>, the team experienced its first major breakthrough.</span></p><p><span>“For the first time, we could merge advanced language understanding with expressive, lifelike avatars,” says Adam Turaev. “The conversations stopped feeling scripted. They became natural, emotional, and real.”&nbsp;</span></p><p><span>As Praktika evaluated newer models, GPT‑4.1 proved to be the strongest fit across its internal evaluations measuring onboarding completion, Day-1 retention, trial-to-paid conversion, and qualitative user feedback.</span></p><p><span>“GPT‑4.1 gave us the best balance of reasoning depth, emotional nuance, and reliability,” says Turaev. “It supported multi-language conversation and complex tutoring logic at the quality we needed, significantly increasing conversation session quality.”</span></p><p><span>Those improvements translated directly into user and business results. After introducing their new long-term memory system, Praktika saw a 24% increase in Day-1 retention and doubled revenue in just a few months.</span></p><p><span>More recently, Praktika began using GPT‑5.2 models to power its architecture. GPT‑5.2 now powers the primary conversation agent, while GPT‑5.2 Pro handles supervisory reasoning and GPT‑5 mini supports continuous progress tracking. Together, these models allow the system to reason in parallel, balancing conversation quality, pedagogy, and efficiency at scale.</span></p><div id=\"exploring-new-ways-to-learn-a-language\"><p></p><h2><span>Exploring new ways to learn a language</span></h2><p></p></div><p><span>Today, Praktika supports millions of learners across nine languages, with more on the way. With its agentic foundation in place, Praktika is now focused on expanding what an AI tutor can understand, remember, and create alongside each learner.</span></p><p><span>“We’re not just teaching languages,” says Turaev. “We’re building AI that helps people feel confident using them in the real world.”</span></p></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/business/guides-and-resources/chatgpt-usage-and-adoption-patterns-at-work",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/business/guides-and-resources/chatgpt-usage-and-adoption-patterns-at-work",
      "title": "Inside GPT-5 for Work: How Businesses Use GPT-5",
      "publishedAt": "Thu, 22 Jan 2026 00:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:16.021Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div><div id=\"introduction\"><p></p><h2><span>Introduction</span></h2><p></p></div><p><span>Launched just two and a half years ago, ChatGPT is used by workers across every industry, in every job function, and at companies of every size. Today, over a quarter of U.S. workers—and 45% of those with postgraduate degrees—report using ChatGPT for work. </span></p><p><span>Enterprise tech has always followed a familiar pattern: big upfront costs, long rollouts, and slow adoption before the payoff. ChatGPT broke that mold when people ported it from their personal lives into their jobs. They didn’t need months of training or complicated onboarding; they just started using it to get meaningful work done. </span></p><p><span>Already, we see clear signals. Everyone from scientists to marketers to operators is folding ChatGPT into everyday work. From debugging code to brainstorming campaigns, it’s becoming the first step in core workflows.  </span></p><p><span>This report shares new data from our own analysis, combined with peer-reviewed sources, about who’s using ChatGPT at work, how people are putting it to use, and the ways it’s taking root inside organizations.</span></p><div id=\"methodology\"><p></p><h2><span>Methodology</span></h2><p></p></div><p><span>This report combines findings from independent third party industry-wide studies with analysis done by OpenAI on usage of ChatGPT and ChatGPT Enterprise. All analyses done by OpenAI in this report were performed on anonymized or aggregated usage data. OpenAI did not review any user or customer content (including model input or output), and did not analyze any identifiable data. All analysis of usage trends was conducted using automated content classifiers. Where the report references specific ChatGPT prompts, those ChatGPT prompts are fully synthetic examples, and not actual user or customer prompts.</span></p><div id=\"the-rise-of-ai-at-work\"><p></p><h2><span>The rise of AI at work</span></h2><p></p></div><p><span>When ChatGPT was released in November 2022, it mostly targeted a small group of AI researchers and enthusiasts. But within months, it had 100 million weekly active users, and today has over 700 million weekly active users, making it one of the world’s most visited websites.&nbsp;</span></p><p><span>Widespread personal use rapidly spread to the workplace. As the statistics show, consumer adoption is very likely advancing AI at work.&nbsp;</span></p><p><span>This is a path we've often seen before: software that gains traction with consumers makes its way into the workplace, often driven most heavily by younger employees. ChatGPT is following that same pattern, reflected in its rapid growth in weekly active users, high penetration with workers under 30, and frequent-often daily-use.</span></p><p><span>In just a few years, AI in the workplace has gone from niche to mainstream. The numbers tell the story:</span></p><div><div><div><h5>Adoption is skyrocketing...</h5><p>Today, 43% of U.S. knowledge workers use AI (Stanford), up from fewer than 1 in 10 in late 2022.</p></div><div><h5>...and ChatGPT leads the shift.</h5><p>Pew reports 28% of employed adults are using ChatGPT at work, up from only 8% two years ago. </p></div></div><div><div><h5>AI use is becoming habitual...</h5><p>More than half of workplace AI users engage four or more days a week. In the last year, daily usage has doubled (Stanford).</p></div><div><h5>...and the benefits are real.</h5><p>A Federal Reserve Bank of St. Louis study found over half of AI users save 3+ hours per week, and a Harvard study found knowledge workers using AI produced 40% higher quality work.</p></div></div><div><div><h5>Usage correlates with education...</h5><p>More than half of workplace AI users engage four or more days a week. In the last year, daily usage has doubled (Stanford).</p></div><div><h5>...and skews younger.</h5><p>Employees 18-29 are more than twice as likely to use ChatGPT at work as those over 50.&nbsp;</p></div></div></div><div id=\"who-uses-chatgpt-in-the-enterprise\"><p></p><h2><span>Who uses ChatGPT in the enterprise</span></h2><p></p></div><p><span>AI adoption isn’t unfolding evenly across the economy. Workers in some industries have moved quickly to embed ChatGPT into their operations, while others are proceeding more slowly.   By looking at which sectors are embracing the tool fastest, we can see both the near-term opportunities and the areas where adoption may take longer to gain traction.&nbsp;</span></p><div><p><span>Source: ChatGPT Free, Plus, and Pro users in the US with a professional email address; email domains mapped to industry</span></p></div><p><span>Certain industries are adopting ChatGPT at higher-than-expected rates. IT and finance lead the way, which makes sense given the tool’s strengths in coding, analysis, and information-heavy work. Manufacturing adoption points to a broader digital transformation: factories using AI for process automation, predictive maintenance, and supply chain optimization. Early investments in industrial AI may be paving the way for widespread ChatGPT use among engineers, analysts, and operations managers.</span></p><p><span>Other industries lag behind. Retail, construction, transportation, wholesale trade, and agriculture all show significantly lower adoption. In most cases, this tracks with their smaller share of knowledge workers, where the need for AI tools is less immediate.&nbsp;&nbsp;</span></p><p><span>Healthcare is a special case. Despite being one of the largest and most data-intensive sectors, adoption has been slower. Strict privacy and compliance rules and risk-averse organizational cultures may be factors. Still, we’re starting to see growth in targeted areas like clinical documentation and administrative workflows, suggesting healthcare could soon become a hotbed of AI adoption.</span></p><div><p><img alt=\"Abstract, grainy gradient image with no distinct objects. Soft pastel colors blend horizontally, transitioning from pale lavender and light pink at the top to brighter pinks and purples in the middle, and fading into deeper blue and dark gray tones near the bottom, evoking a hazy sunset or atmospheric horizon.\" data-nosnippet=\"true\" loading=\"lazy\" width=\"1208\" height=\"967\" decoding=\"async\" data-nimg=\"1\" sizes=\"(min-width: 1728px) 1728px, 100vw\" srcset=\"https://images.ctfassets.net/kftzwdyauwt9/4jw7O0DGxOqj9YcSfayg0V/60e61393888e00784b65ecfd3c020f52/Gradient_Media.png?w=640&amp;q=90&amp;fm=webp 640w, https://images.ctfassets.net/kftzwdyauwt9/4jw7O0DGxOqj9YcSfayg0V/60e61393888e00784b65ecfd3c020f52/Gradient_Media.png?w=750&amp;q=90&amp;fm=webp 750w, https://images.ctfassets.net/kftzwdyauwt9/4jw7O0DGxOqj9YcSfayg0V/60e61393888e00784b65ecfd3c020f52/Gradient_Media.png?w=828&amp;q=90&amp;fm=webp 828w, https://images.ctfassets.net/kftzwdyauwt9/4jw7O0DGxOqj9YcSfayg0V/60e61393888e00784b65ecfd3c020f52/Gradient_Media.png?w=1080&amp;q=90&amp;fm=webp 1080w, https://images.ctfassets.net/kftzwdyauwt9/4jw7O0DGxOqj9YcSfayg0V/60e61393888e00784b65ecfd3c020f52/Gradient_Media.png?w=1200&amp;q=90&amp;fm=webp 1200w, https://images.ctfassets.net/kftzwdyauwt9/4jw7O0DGxOqj9YcSfayg0V/60e61393888e00784b65ecfd3c020f52/Gradient_Media.png?w=1920&amp;q=90&amp;fm=webp 1920w, https://images.ctfassets.net/kftzwdyauwt9/4jw7O0DGxOqj9YcSfayg0V/60e61393888e00784b65ecfd3c020f52/Gradient_Media.png?w=2048&amp;q=90&amp;fm=webp 2048w, https://images.ctfassets.net/kftzwdyauwt9/4jw7O0DGxOqj9YcSfayg0V/60e61393888e00784b65ecfd3c020f52/Gradient_Media.png?w=3840&amp;q=90&amp;fm=webp 3840w\" src=\"https://images.ctfassets.net/kftzwdyauwt9/4jw7O0DGxOqj9YcSfayg0V/60e61393888e00784b65ecfd3c020f52/Gradient_Media.png?w=3840&amp;q=90&amp;fm=webp\" style=\"max-width: 100%; height: auto; border-radius: 1rem; margin: 2rem 0px;\"></p></div><p><span>Adoption patterns vary across departments, but a few themes stand out. In the first three months, four categories dominate usage: writing, research, programming, and analysis. Together, they account for the majority of messages sent. This variety highlights the flexibility of ChatGPT; teams turn to it to draft communications, gather and synthesize information, write code, and interpret data.</span></p><p><span>Technical teams are among the heaviest users, with analytics, engineering, and IT roles making up a large percentage of early usage. Programming is the top task, especially for engineering roles, but users also request a substantial amount of research and documentation help. This suggests ChatGPT is being used nearly as much for planning as for coding.&nbsp;</span></p><p><span>IT teams lean most heavily on research and troubleshooting, often using ChatGPT as an information resource before moving into automation.</span></p><div><div><h4>Analytics</h4><ol><li><p>1</p>Coding</li><li><p>2</p>Writing</li><li><p>3</p>Research</li></ol></div><div><h4>Engineering</h4><ol><li><p>1</p>Coding</li><li><p>2</p>Research</li><li><p>3</p>Writing</li></ol></div><div><h4>IT</h4><ol><li><p>1</p>Coding</li><li><p>2</p>Research</li><li><p>3</p>Writing</li></ol></div></div><p><span>Example prompt for coding</span></p><p><i><span>Note: the above synthetic prompt is an example written specifically for this report solely for illustrative purposes</span></i></p><p><span>People in go-to-market roles, including marketing, communications, sales, and customer experience, are also major adopters. These functions rely on ChatGPT primarily for writing, research, creative ideation, and media generation.&nbsp;</span></p><p><span>Across functions, the early usage pattern is consistent: AI is augmenting expertise, not replacing it. Engineers are iterating on prompts to debug code and generate unit tests. Analysts are using chain-of-thought prompting to clean and interpret datasets. Customer support teams are drafting thoughtful, brand-aligned responses. The common thread is that ChatGPT is extending the reach of specialized skills and becoming a partner in core workflows.</span></p><p><i><span>Source: Aggregated ChatGPT Enterprise department data collected during onboarding; automated<br>content classifiers</span></i></p><p><span>Interestingly, coding is spreading beyond engineering. Designers may be leaning on programming for front-end prototyping and snippet help-and they use ChatGPT for coding at a much higher rate than finance and sales. Project managers combine writing, media generation, coding, and data analysis—acting as the glue across teams. But product, operations, marketing, finance, and HR all use ChatGPT for coding to some extent.&nbsp;</span></p><p><span>We see this trend validated in a study done by Boston University and BCG, which examined the impact of ChatGPT on the technical competency of BCG consultants. The study found that consultants armed with and trained on ChatGPT score 49, 20, and 18 percentage points higher than those in the control group on the three technical tasks, and performed close to the level of real BCG data scientists on two of the three tasks.</span></p><p><span>Good writing is no longer a specialist function reserved for content teams. With ChatGPT, anyone can turn notes into clean copy and iterate quickly. Meetings, memos, and customer messages become clearer and more inclusive because everyone can express their ideas well, not just trained communicators. AI is becoming the front door for routine communication and coordination, compressing drafting, tone calibration, and versioning into a single pass.</span></p><p><span>Design teams stand out for their use of media generation, relying on it 2–4x more than other groups. The heavy use in these functions for core work tasks highlights an emerging role for ChatGPT beyond text.&nbsp;</span></p><div><ol><li><p>Writing</p></li><li><p>Research</p></li><li><p>Media generation</p></li></ol></div><p><span>All go-to-market teams use ChatGPT most for writing, research and media generation tasks, but in different ways. Here are some sample prompts indicating the type of queries we're seeing:</span></p><p><i><span>Note: the above synthetic prompts are examples written specifically for this report solely for illustrative purposes</span></i></p><div id=\"roles-shape-usage-patterns\"><p></p><h2><span>Roles shape usage patterns</span></h2><p></p></div><p><span>Early data shows a consistent trend: most departments rely on the core tools in ChatGPT, including search, data analysis, file uploads, retrieval, and canvas. Adoption of more advanced features—such as reasoning models, deep research, projects, and custom instructions—are higher among power users, including R&amp;D teams.&nbsp;The result for many employees is that ChatGPT is woven into daily workflows mainly through accessible, low-friction tasks rather than specialized use cases. </span></p><p><span>Technical functions stand out as the exception. Analytics, engineering, IT, and research roles are much heavier users of advanced capabilities. Their work often demands multi-step reasoning, large-scale data synthesis, or complex problem-solving. Engineers prompt for code generation or debugging; analysts use deep research to interpret datasets; and IT professionals query knowledge bases to resolve tickets and troubleshoot systems. Higher-powered tools naturally align with technical tasks that are structured, data-heavy, and decision-driven.</span></p><div><p>Advanced features remain underused, even where they could deliver broad impact. Technical functions stand out as much heavier users of advanced capabilities.</p><p>GPT‑5 helps solve this problem with its real‑time router that automatically decides which advanced features and tools to use based on conversation type, complexity, tool needs, and explicit intent.</p></div><p><span>Different technical teams also lean into distinct features. IT teams are more likely to use retrieval and search, treating ChatGPT as a knowledge companion for quick answers to configuration or policy questions. Engineering teams show stronger use of GPTs, programming tools, and data analysis, reflecting their more code-centric workflows. This divergence underscores that adoption depends not only on technical fluency but also on the type of work and context within each department.</span></p><p><span>Two opportunities emerge from this data. First, advanced features remain underused, even where they could deliver broad impact. Barriers may include discoverability, awareness of use cases, or the setup required to use them. </span></p><p><span>Second, early champions in analytics, IT, legal, and engineering are already pushing into more complex workflows. As enablement programs expand and product improvements lower the barrier to entry, adoption will likely shift from core daily tasks toward deeper reasoning and collaborative workflows that reshape decision-making across the enterprise.&nbsp;</span></p><div><div><h4>R&amp;D</h4><ol><li><p>1</p>Search</li><li><p>2</p>Data analysis</li><li><p>3</p>Image upload</li></ol></div><div><h4>Go-to-market</h4><ol><li><p>1</p>Search</li><li><p>2</p>Data analysis</li><li><p>3</p>Retrievel</li></ol></div><div><h4>Administrative</h4><ol><li><p>1</p>Search</li><li><p>2</p>Data analysis</li><li><p>3</p>File upload</li></ol></div></div><div id=\"chatgpt-as-an-operating-system-for-work\"><p></p><h2><span>ChatGPT as an operating system for work</span></h2><p></p></div><p><span>ChatGPT is already making workers more productive in measurable ways. Internal benchmarks show meaningful increases in productivity, driven by employees who use it to write and communicate faster, research more effectively, and reduce the effort required for repetitive tasks. Most companies are still in the early stages of adoption, but we’re beginning to see organizations embed ChatGPT at the departmental level to make entire processes more efficient.&nbsp;</span></p><p><span>Unlike traditional enterprise software, which spreads through top-down rollouts after long decision cycles and training programs, ChatGPT entered the workplace from the bottom up. Employees and small teams brought it in on their own, experimented with workflows, and demonstrated value before companies formalized procurement. This grassroots pattern has made it the fastest adopted enterprise technology in recent history.&nbsp;</span></p><p><span> That dynamic is now shifting. New capabilities, from autonomous agents to advanced coding support to decision-assist tools, are expanding the role of ChatGPT beyond personal productivity. It’s becoming a platform for entire workflows. Executives use it to shape strategy, engineers to design and debug systems, and customer support agents to evaluate complex solutions. Increasingly, ChatGPT functions as an operating system for daily work: a shared layer where decisions are made, problems are solved, and output scales.</span></p><div><p><b>ChatGPT usage: Broad and deep</b></p><p>The number of people using ChatGPT is increasing, but so is the number of inquiries per user:</p><ul><li><p>Certain power-user segments of ChatGPT Pro subscribers send upwards of 200 messages to ChatGPT per day </p></li><li><p>Usage has evolved from simple Q&amp;A to coding, data analysis, and a range of agentic workflows</p></li></ul></div><div id=\"whats-next-for-work\"><p></p><h2><span>What’s next for work</span></h2><p></p></div><p><span>Work has always evolved alongside technology. Not long ago, much of it centered on finding answers, drafting emails, and repeating solved problems. Increasingly, it’s shifting toward synthesis, creativity, and speed: work that’s improved by natural, intuitive interactions with AI.</span></p><p><span>In the years ahead, AI will embed itself into nearly every workflow. As this happens, employees will spend less time performing tasks and more time supervising and shaping AI output. The crossfunctional reach of ChatGPT means individuals will be able to take on tasks once spread across multiple departments. A product manager, for example, might use it to analyze customer feedback, test and refine a new feature, and draft the legal and marketing content needed to bring it to market.</span></p><p><span>Collaboration is moving from siloed documents and messages into shared, real-time workspaces where teams solve problems together. Features like memory are making the product more contextaware, giving employees a partner that remembers preferences, projects, and workflows unique   to them. And the ability to bring structured and unstructured data directly into ChatGPT is broadening its role as the central interface for enterprise knowledge, and GPT‑5 is accelerating this shift.</span></p><p><span>Crucially, early evidence suggests this shift not only makes workers more productive, but actually makes their work more enjoyable. It does this by shrinking time-consuming and lower-value tasks and enabling them to refocus time on meaningful, core work. In a six-month randomized field experiment across thousands of knowledge workers, access to AI cut weekly email time by 31%. Another study looked at software developers, finding AI coding tools enabled them to spend more time coding, more time on exploratory work, and less time on project-management. Together, these findings suggest that tools like ChatGPT may reduce busywork, freeing up time for more strategic, satisfying, and ultimately higher-value work.</span></p><p><span>The scale of this change echoes past technological revolutions. Electricity reshaped factory work, the internet redefined commerce and communication, and AI is now setting the stage for the next leap. The enterprises that adapt quickly and thoughtfully will capture the earliest and largest gains: faster decision cycles, productivity breakthroughs, and new opportunities across every function.</span></p></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/higgsfield",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/higgsfield",
      "title": "How Higgsfield turns simple ideas into cinematic social videos",
      "publishedAt": "Wed, 21 Jan 2026 10:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:16.177Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div><p><span>Short-form video drives modern commerce, but producing video that actually performs is harder than it looks. Clips that feel effortless on TikTok, Reels, and Shorts are built on invisible rules: hook timing, shot rhythm, camera motion, pacing, and other subtle cues that make content feel “native” to whatever is trending.</span></p><p><a href=\"https://higgsfield.ai/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>Higgsfield</span></u>⁠<span>(opens in a new window)</span></a><span> is a generative media platform that lets teams create short-form, cinematic videos from a product link, an image, or a simple idea. Using OpenAI GPT‑4.1 and GPT‑5 to plan and Sora 2 to create, the system generates roughly 4 million videos per day, turning minimal input into structured, social-first video.</span></p><div><blockquote>“Users rarely describe what a model actually needs. They describe what they want to feel. Our job is to translate that intent into something a video model can execute, using OpenAI models to turn goals into technical instructions.”</blockquote><p>—Alex Mashrabov, Co-founder and CEO, Higgsfield</p></div><div id=\"creators-describe-outcomes-not-camera-instructions\"><p></p><h2><span>Creators describe outcomes, not camera instructions</span></h2><p></p></div><p><span>People don’t think in shot lists. They say things like “make it dramatic” or “this should feel premium.” Video models, by contrast, require structured direction: timing rules, motion constraints, and visual priorities.</span></p><p><span>To bridge that gap, the Higgsfield team built what they call a cinematic logic layer to interpret creative intent and expand it into a concrete video plan before any generation happens.</span></p><p><span>When a user provides a product URL or image, the system uses GPT‑4.1 mini and GPT‑5 to infer narrative arc, pacing, camera logic, and visual emphasis. Rather than exposing users to raw prompts, Higgsfield internalizes cinematic decision-making into the system itself. Once the plan is constructed, Sora 2 renders motion, realism, and continuity based on those structured instructions.</span></p><p><span>That planning-first approach reflects the team behind the product. Higgsfield brings together engineers and experienced filmmakers, including award-winning directors, alongside leadership with deep roots in consumer media. Co-founder and CEO Alex Mashrabov previously led generative AI at Snap, where he invented Snap lenses, shaping how hundreds of millions of people interact with visual effects at scale.</span></p><div id=\"operationalizing-virality-as-a-system-not-a-guess\"><p></p><h2><span>Operationalizing virality as a system, not a guess</span></h2><p></p></div><p><span>For Higgsfield, virality is a set of measurable patterns identified using GPT‑4.1 mini and GPT‑5 to analyze short‑form social videos at scale and distill those findings into repeatable creative structures.</span></p><p><span>Internally, Higgsfield defines virality by engagement-to-reach ratio, with particular focus on share velocity. When shares begin to outpace likes, content shifts from passive consumption to active distribution.</span></p><p><span>Higgsfield encodes recurring, viral structures into a library of video presets. Each preset has a specific narrative structure, pacing style, and camera logic observed in high-performing content. Roughly 10 new presets are created each day, and older ones are cycled out as engagement wanes.</span></p><p><span>These presets power Sora 2 Trends, which lets creators generate trend-accurate videos from a single image or idea. The system applies motion logic and platform pacing automatically, producing outputs aligned to each trend without manual tuning.</span></p><p><span>Compared to Higgsfield’s earlier baseline, videos generated through this system show a 150% increase in share velocity and roughly 3x higher cognitive capture, measured through downstream engagement behavior.</span></p><div id=\"turning-product-pages-into-ads-with-click-to-ad\"><p></p><h2><span>Turning product pages into ads with Click-to-Ad</span></h2><p></p></div><p><span>Built on the same planning-first principles that guide the rest of the platform, Click-to-Ad grew out of the positive reception to Sora 2 Trends. The feature removes the “prompting barrier” by using GPT‑4.1 to interpret product intent and Sora 2 to generate videos.</span></p><p><span>Here’s how it works:</span></p><div><ol><li><span>A user pastes in a link to a product page</span></li><li><span>The system analyzes the page to extract brand intent, identify key visual anchors, and understand what matters about the product</span></li><li><span>Once the product is identified, the system maps it into one of the pre-engineered trending presets</span></li><li><span>Sora 2 generates the final video, applying each preset's complex professional standards for camera motion, rhythmic pacing, and stylistic rules</span></li></ol></div><p><span>The goal is fast, usable output that fits social platforms on the first try, and that shift changes how teams work. Users now tend to get usable video in one or two attempts, rather than iterating through five or six prompts. For marketing teams, that means campaigns can be planned around volume and variation, not trial and error.</span></p><p><span>A typical generation takes 2–5 minutes, depending on the workflow. Because the platform supports concurrent runs, teams can generate dozens of variations in an hour, making it practical to test creative directions as trends shift.</span></p><p><span>Since launching in early November, Click-to-Ad has been adopted by more than 20% of professional creators and enterprise teams on the platform, measured by whether outputs are downloaded, published, or shared as part of live campaigns.</span></p><div id=\"routing-the-right-job-to-the-right-model\"><p></p><h2><span>Routing the right job to the right model</span></h2><p></p></div><p><span>Higgsfield’s system relies on multiple OpenAI models, each selected based on the demands of the task.</span></p><p><span>For deterministic, format-constrained workflows, such as enforcing preset structure or applying known camera-motion schemas, the platform routes requests to GPT‑4.1 mini. These tasks benefit from high steerability, predictable outputs, low variance, and fast inference.</span></p><p><span>More ambiguous workflows require a different approach. When the system needs to infer intent from partial inputs, such as interpreting a product page or reconciling visual and textual signals, Higgsfield routes requests to GPT‑5, where deeper reasoning and multimodal understanding outweigh latency or cost considerations.</span></p><p><span>Routing decisions are guided by internal heuristics that weigh:</span></p><div><ul><li><span>Required reasoning depth versus acceptable latency</span></li><li><span>Output predictability versus creative latitude</span></li><li><span>Explicit versus inferred intent</span></li><li><span>Machine-consumed versus human-facing outputs</span></li></ul></div><p><span>“We don’t think of this as choosing the best model,” says Yerzat Dulat, CTO and co-founder of Higgsfield. “We think in terms of behavioral strengths. Some models are better at precision. Others are better at interpretation. The system routes accordingly.”</span></p><div id=\"pushing-the-boundaries-of-ai-video\"><p></p><h2><span>Pushing the boundaries of AI video</span></h2><p></p></div><p><span>Many of Higgsfield’s workflows would not have been viable six months ago.</span></p><p><span>Earlier image and video models struggled with consistency: characters drifted, products changed shape, and longer sequences broke down. Recent advances in OpenAI image and video models made it possible to maintain visual continuity across shots, enabling more realistic motion and longer narratives.</span></p><p><span>That shift unlocked new formats. Higgsfield recently launched Cinema Studio, a horizontal workspace designed for trailers and short films. Early creators are already producing multi-minute videos that circulate widely online, often indistinguishable from live-action footage.</span></p><p><span>As OpenAI models continue to evolve, Higgsfield’s system expands with them. New capabilities are translated into workflows that feel obvious in hindsight, but weren’t feasible before. As models mature, the work of storytelling shifts away from managing tools and toward making decisions about tone, structure, and meaning.</span></p></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/edu-for-countries",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/edu-for-countries",
      "title": "Introducing Edu for Countries",
      "publishedAt": "Wed, 21 Jan 2026 01:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:16.252Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div><p><span>The history of technology suggests that the biggest economic gains come not from invention alone, but from turning new capabilities into scaled, everyday use. But even as AI capabilities have improved, </span><a href=\"https://openai.com/index/ai-for-self-empowerment/\" target=\"_blank\" rel=\"noopener noreferrer\"><span>we see a widening “capability overhang,”</span></a><span> defined as the gap between what AI tools can do and how people are using them.</span></p><p><span>Education systems are a critical route through which this gap is closed. </span><a href=\"https://reports.weforum.org/docs/WEF_Future_of_Jobs_Report_2025.pdf?utm_source=chatgpt.com\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><span>Studies</span>⁠<span>(opens in a new window)</span></a><span> project that by 2030 nearly 40% of the core skills workers rely on today will change, driven largely by AI. By embedding AI tools, training, and research into the core infrastructure of schools and universities, education systems can evolve alongside these shifts and better prepare students to thrive in a world with AI.&nbsp;</span></p><div id=\"openais-education-for-countries\"><p></p><h2><span>OpenAI’s Education for Countries&nbsp;</span></h2><p></p></div><p><span>It is for this purpose that we are launching </span><b><span>OpenAI’s Education for Countries</span></b><span> as a new pillar of our </span><a href=\"https://openai.com/index/how-countries-can-end-the-capability-overhang/\" target=\"_blank\" rel=\"noopener noreferrer\"><span>OpenAI for Countries</span></a><span> initiative. We will work with governments and university consortia to bring AI into education systems to personalize learning, reduce administrative burden, and prepare students for the workforce. Working with Ministries of Education, partners, universities and researchers, the initiative will bring together several core elements:</span></p><div><ul><li><b><span>AI tools for learning: </span></b><span>Access to </span><a href=\"https://chatgpt.com/business/education/\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><span>ChatGPT Edu</span>⁠<span>(opens in a new window)</span></a><span>, </span><a href=\"https://openai.com/index/introducing-gpt-5-2/\" target=\"_blank\" rel=\"noopener noreferrer\"><span>GPT‑5.2</span></a><span>, </span><a href=\"https://openai.com/index/chatgpt-study-mode/\" target=\"_blank\" rel=\"noopener noreferrer\"><span>study mode</span></a><span>, and </span><a href=\"https://openai.com/index/introducing-canvas/\" target=\"_blank\" rel=\"noopener noreferrer\"><span>canvas</span></a><span> can be customized to shape how the world’s most advanced AI models are used to support local learning priorities.</span></li><li><b><span>Learning outcomes research: </span></b><span>Collaboration on large-scale, national research initiatives to understand how AI supports learning and affects teacher productivity, which can inform local policy, workforce development, and future technology design.</span></li><li><b><span>OpenAI Certifications and training: </span></b><span>Tailored training with ministries and education systems, from the </span><a href=\"https://academy.openai.com/\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><span>OpenAI Academy</span>⁠<span>(opens in a new window)</span></a><span> to ChatGPT‑based certifications, giving educators and students the practical AI skills aligned with national workforce priorities.</span></li><li><b><span>Global network of partners: </span></b><span>A growing network of governments, researchers, and education leaders who share insights, highlight successful deployments, and help shape responsible approaches to AI in education.</span></li></ul></div><div id=\"our-growing-work-with-countries-globally\"><p></p><h2><span>Our growing work with countries globally</span></h2><p></p></div><p><span>Our first cohort includes Estonia, Greece, Italy’s Conference of University Rectors (CRUI), Jordan, Kazakhstan, Slovakia, Trinidad &amp; Tobago, and the United Arab Emirates.&nbsp;</span></p><p><span>AI tools like ChatGPT Edu have already been deployed nationwide in Estonia, across public universities and secondary schools, reaching more than 30,000 students, educators, and researchers in its first year. Longitudinal research partnerships are also underway, such as a large-scale study with the University of Tartu and Stanford, to measure how AI affects learning outcomes among 20,000 students over time.</span></p><p><span>As AI is introduced at scale, rollouts typically follow a phased approach, starting by equipping educators with the tools and training they need to lead AI use in classrooms. In higher education, ChatGPT Edu is already available to students. In high schools, student access begins through small pilots developed in close collaboration with local leaders, to ensure safety and alignment with local curricula. These pilots are paired with ongoing work by OpenAI to strengthen protections for young people who use ChatGPT, including age-appropriate model behavior improvements and developing AI literacy content for educators with trusted partners like Common Sense Media.</span></p><div id=\"ensuring-ai-benefits-everyone\"><p></p><h2><span>Ensuring AI benefits everyone</span></h2><p></p></div><p><span>OpenAI’s mission is to ensure that advanced AI benefits everyone. We're building AI to help people solve hard problems because by helping with the hard problems, AI can benefit the most people possible—through more scientific discoveries, better healthcare and education, and improved productivity. This work reflects a simple belief: powerful technologies should expand opportunity for all, not exclude people from it.</span></p><p><span>OpenAI’s Education for Countries builds on this mission, as well as our work to ensure AI expands economic opportunity with </span><a href=\"https://openai.com/index/openai-certificate-courses/\" target=\"_blank\" rel=\"noopener noreferrer\"><span>OpenAI Certifications</span></a><span>, which help individuals build foundational AI skills and give clear signals to employers about their ability to use AI effectively at work.</span></p><p><span>The program also represents a step forward in OpenAI’s ongoing commitment to supporting learning with AI—complementing programs like </span><a href=\"https://openai.com/index/introducing-nextgenai/\" target=\"_blank\" rel=\"noopener noreferrer\"><span>NextGenAI</span></a><span> to accelerate research on AI and learning across universities, products to enhance how AI is used in education like </span><a href=\"https://openai.com/index/introducing-chatgpt-edu/\" target=\"_blank\" rel=\"noopener noreferrer\"><span>ChatGPT Edu</span></a><span> and </span><a href=\"https://openai.com/index/chatgpt-study-mode/\" target=\"_blank\" rel=\"noopener noreferrer\"><span>study mode</span></a><span>, and partnerships to support teacher-led AI adoption like with the </span><a href=\"https://openai.com/global-affairs/aft/\" target=\"_blank\" rel=\"noopener noreferrer\"><span>American Federation of Teachers</span></a><span> in the United States.</span></p><p><span>Our next cohort will be announced later in 2026. To learn more about how to join, contact our team.</span></p></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/how-countries-can-end-the-capability-overhang",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/how-countries-can-end-the-capability-overhang",
      "title": "How countries can end the capability overhang",
      "publishedAt": "Wed, 21 Jan 2026 01:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:15.303Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div><p><span>AI is advancing at extraordinary speed, but many countries are not yet leveraging its full potential to benefit people and drive economic growth. There is a </span><a href=\"https://openai.com/index/ai-for-self-empowerment/\" target=\"_blank\" rel=\"noopener noreferrer\"><span>capability overhang </span></a><span>between those who are taking advantage of these tools and everybody else. If that overhang continues to grow, a small number of countries will pull further ahead economically and technologically, while others risk falling behind in ways that will be difficult to reverse.</span></p><p><span>New research we’re releasing today in our report,</span><i><span> </span></i><a href=\"https://cdn.openai.com/pdf/openai-ending-the-capability-overhang.pdf\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><i><span>Ending the Capability Overhang</span></i>⁠<span>(opens in a new window)</span></a><span>, shows how large this overhang has already become. The typical power user relies on about seven times more advanced “thinking capabilities” than the typical user—using AI for more complex, multi-step work rather than simple prompts.</span></p><p><span>We also observe a clear country-level gap, and it isn’t driven by income alone. Across more than 70 countries with the highest ChatGPT usage, some countries use 3× more thinking capabilities per person than others. While large economies like the United States and India lead in total users—and smaller high-income countries like Singapore and the Netherlands stand out in population penetration—advanced AI adoption is not confined to wealthy nations. Countries such as Vietnam and Pakistan rank among the world’s top users of agentic tools, with more than 2× higher per-person use of advanced tasks like data analysis, Connectors, and Codex.</span></p><p><span>Put plainly, this means some countries are already using AI to solve harder problems and move faster, regardless of how many resources they have. These differences already translate into real productivity gains, freeing people to take on harder tasks, build new products and services, and accelerate innovation in ways that drive economic growth and improve living standards.</span></p><p><span>That’s why we launched </span><a href=\"https://openai.com/global-affairs/openai-for-countries/\" target=\"_blank\" rel=\"noopener noreferrer\"><span>OpenAI for Countries</span></a><span> last year: to help governments and institutions put AI and its gains into the hands of more people. The initiative supports countries as they move from basic use to deeper adoption—including integrating AI into education systems, workplaces, and public services in ways that raise productivity, strengthen institutions, and expand opportunity. Rather than a one-size-fits-all approach, OpenAI for Countries is built around partnerships that reflect local needs, priorities, and capacity.</span></p><p><span>Today at our OpenAI event alongside the World Economic Forum, we announced that we’re expanding this work in 2026 with new initiatives focused on education, health, AI skills training and certifications, disaster response and preparedness, cybersecurity, and start-up accelerators. They give nations a range of options for how to work with us to address their needs and priorities</span><b><span>.</span></b></p><p><span>A key focus of this expansion is helping partner nations prepare for an AI-driven world, starting with </span><a href=\"https://openai.com/index/edu-for-countries/\" target=\"_blank\" rel=\"noopener noreferrer\"><span>OpenAI’s Education for Countries</span></a><span> program. It’s designed to help governments bring AI into their education systems in ways that strengthen learning and prepare students for the jobs of the future—and to work hand-in-hand with governments to improve our models and education tools. Looking forward, I’m also excited about the prospect of finding ways to work with the creative and cultural sectors of OpenAI for Countries partners.</span></p><p><span>With more workplaces adopting AI and more employers seeking workers with AI skills, governments are increasingly treating the technology as essential education infrastructure. That means helping learners build AI skills while equipping educators with new tools and training to guide students to use the tools in ways that advance learning and critical thinking. The first set of Edu for Countries partners includes Estonia, the United Arab Emirates, Greece, Jordan, Slovakia, Kazakhstan, Trinidad &amp; Tobago, and Italy’s CRUI. Working with ministries, universities, and researchers across these systems, the program will combine expanded access to advanced AI tools, large-scale research into AI’s impact on education and learning, training and certifications for both students and educators, and a growing global community of partners working to shape responsible approaches to using AI in education.</span></p><p><span>Like Education for Countries, the rest of our new initiatives are designed to be flexible and shaped through ongoing discussions with our partners about how to translate AI capability into real-world impact. Countries have a significant opportunity to capture productivity gains by improving adoption—scaling enterprise use, building AI-ready infrastructure, and increasing AI fluency across workforces and classrooms. And as AI capabilities continue to advance, acting now gives countries the opportunity to turn that progress into concrete benefits for people everywhere.</span></p><p><span>Read more about our OpenAI for Countries expansion in the Ending the Capability Gap report </span><a href=\"https://cdn.openai.com/pdf/openai-ending-the-capability-overhang.pdf\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><span>here⁠</span>⁠<span>(opens in a new window)</span></a><span>.</span></p></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/horizon-1000",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/horizon-1000",
      "title": "Horizon 1000: Advancing AI for primary healthcare",
      "publishedAt": "Tue, 20 Jan 2026 21:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:15.544Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div id=\"main\" tabindex=\"-1\"><span aria-live=\"polite\" aria-atomic=\"true\">OpenAI</span><article><div><p>Together, with the Gates Foundation, we’re committing $50 million in funding and technology to help strengthen primary healthcare for 1,000 African clinics and their communities.</p></div><div><p><span>AI capabilities have advanced much faster than their broad, real-world deployment, leaving a growing gap between what’s possible and what people experience. These systems have become so capable that they’ve made new kinds of things possible—some we couldn’t have imagined not long ago, and some we’re still discovering. This is especially clear in healthcare, where the challenge is now </span><a href=\"https://openai.com/index/ai-clinical-copilot-penda-health/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>turning powerful models into tools</span></u>⁠</a><span> that work in everyday care.</span></p><p><span>Today, we’re announcing </span><b><span>Horizon 1000</span></b><span>, a pilot initiative with the Gates Foundation to support leaders in African countries, beginning in Rwanda, as they advance these AI capabilities for health. Together, the Gates Foundation and OpenAI are committing $50 million in funding, technology, and technical support to support their work, with the ambitious goal of reaching 1,000 primary healthcare clinics and their surrounding communities by 2028.</span></p><div><blockquote>“AI is going to be a scientific marvel no matter what, but for it to be a societal marvel, we’ve got to figure out ways that we use this incredible technology to improve people’s lives.”</blockquote><p>Sam Altman, CEO of OpenAI</p></div><p><span>Primary health care is the foundation of strong, resilient health systems, yet remains inaccessible for half the world’s population. Sub-Saharan Africa alone faces a health workforce shortfall of approximately 5.6 million workers—placing extraordinary strain on existing clinicians and underscoring the scale of unmet demand for care. Quality of care is also highly variable, and is a major driver of preventable deaths.</span></p><p><span>At the same time, many countries across Sub-Saharan Africa have been at the forefront of re-imagining how care can be delivered at scale. Governments and health leaders across the region are actively exploring how digital tools and AI can help extend the reach of their existing health workforce, improve quality, and bring more consistent care to communities.&nbsp;</span></p><p><span>Horizon 1000 aims to meet this moment by supporting African leadership and medical experts with resources and technical expertise so they can move from innovation to deployment. In practice, AI tools can help frontline health workers navigate complex guidelines and reduce administrative burden, so clinicians can spend more time on care. We also know people want more agency over their health, and many are already turning to AI to help navigate their own care.&nbsp;</span></p><p><span>We look forward to learning openly along the way and measuring success by what meaningfully improves care for patients and the health workforce who serve them.</span></p></div><section id=\"citations\" data-testid=\"citations\"><ul><li><a href=\"https://openai.com/news/?tags=2026\" target=\"_blank\" rel=\"noopener noreferrer\">2026</a></li></ul></section></article></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/stargate-community",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/stargate-community",
      "title": "Stargate Community",
      "publishedAt": "Tue, 20 Jan 2026 19:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:16.619Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div id=\"main\" tabindex=\"-1\"><span aria-live=\"polite\" aria-atomic=\"true\">OpenAI</span><article><div><p><span>OpenAI’s mission is to ensure that AGI benefits all of humanity, and in order to do that, we are working to ensure our Stargate campuses benefit the local communities that make them possible. We believe that AI </span><a href=\"https://cdn.openai.com/global-affairs/openai-infra-economics-10.09.24.pdf\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><u><span>infrastructure</span></u>⁠<span>(opens in a new window)</span></a><span> is vital for American competitiveness and economic opportunity, while boosting local economies by creating jobs and bringing in local revenue.</span></p><p><span>When we announced Stargate one year ago in January 2025, we set out to expand our U.S. AI infrastructure to 10GW by 2029—and just one year in, we are already well beyond halfway to that goal in planned capacity, with the first site in Abilene, Texas already training and serving frontier AI systems and multiple Stargate sites under development across Texas, New Mexico, Wisconsin, and Michigan. We are committed to working with communities to ensure that our Stargate campuses are built and run in a way that strengthens communities and demonstrates that we’re being good neighbors.</span></p><p><span>AI is delivering tangible benefits already, from helping hundreds of millions of people with their </span><a href=\"https://openai.com/index/introducing-chatgpt-health/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>health and wellness</span></u></a><span> every week to unlocking new scientific advances. At the same time, we are making the AI models themselves much more capable and power efficient each year. As we’ve done so, demand has increased even more, since intelligence can more affordably be applied to more important problems, and there’s no limit to the problems that people want to solve. That’s why we and the industry see a critical need for such large-scale AI infrastructure.</span></p><p><span>Partnering with local communities starts with understanding local needs. Going forward, every Stargate site will have its own locally tailored </span><b><span>Stargate Community</span></b><span> plan, driven by community input and local concerns. They will all rest on this same core premise: Stargate is a partnership with communities, and we can only achieve our mission by being good neighbors.</span></p><p><span>Across all of our </span><b><span>Stargate Community</span></b><span> plans,</span><b><span> we commit to paying our own way on energy, so that our operations don’t increase your electricity prices</span></b><span>. Every community and region has unique energy needs and grid conditions, and our commitment will be tailored to the region. Depending on the site, this can range from bringing new dedicated power and storage that the project fully funds, to adding and paying for new energy generation and transmission resources. This commitment can be grounded in specific approaches like:</span></p><div><ol><li><span>Funding the incremental generation and grid upgrades our load requires</span></li><li><span>Planning transparently and proactively with local utilities, transmission providers, state utility regulators (public utility/service commissions) and regional grid operators</span></li><li><span>Working with utilities, grid operators, and the industry to develop strategies for operating AI campuses as flexible loads, so when peak conditions or grid stress are forecast, we can reduce or curtail consumption and participate in demand-response and grid-stability programs</span></li></ol></div><p><span>We are already working with partners to deliver this on existing Stargate campuses. For example:</span></p><div><ul><li><span>In </span><a href=\"https://vantage-dc.com/news/openai-oracle-and-vantage-data-centers-announce-stargate-data-center-site-in-wisconsin/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>Wisconsin</span></u>⁠<span>(opens in a new window)</span></a><span>, our partners Oracle and Vantage are working with WEC Energy Group to develop new energy generation and capacity, including solar and battery storage.</span><b><span> </span></b><span>Our developer partners are also underwriting 100% of the power infrastructure investment through a dedicated electricity rate from WEC. The proposed rate is designed to protect other customers from price increases associated with the new investments needed to serve the facility.</span></li><li><span>In </span><a href=\"https://www.related-digital.com/michigan\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>Michigan</span></u>⁠<span>(opens in a new window)</span></a><span>, our partners Oracle and Related Digital are working with DTE Energy to supply the project’s power using existing resources, augmented by a new battery storage investment financed entirely by the project. The structure is designed to help ensure there is</span><b><span> </span></b><span>no impact on DTE’s existing customers’ energy supply or rates. DTE customers can also benefit from the project contributing its share to the fixed costs of maintaining and improving the grid.</span></li><li><span>In </span><a href=\"https://openai.com/index/stargate-sb-energy-partnership/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>Texas</span></u></a><span>, our partners SB Energy plans to fund and build new energy generation and storage to supply the majority of the power needed to run the Stargate campus we’re developing together in Milam County.</span></li></ul></div><p><span>These are early examples, and we’ll continue to find meaningful ways to benefit local communities including:</span></p><div><ul><li><b><span>Minimizing water use and protecting local ecosystems</span></b><span> by prioritizing closed-loop or low-water cooling systems. </span><b><span>AI campuses and deep learning workloads use innovations in cooling water systems design that drastically reduce the water use compared to traditional datacenters</span></b><span>. Water required by our facilities should be a fraction of a community’s overall water use. As Mayor Weldon Hurt of Abilene, Texas </span><a href=\"https://www.reporternews.com/story/news/2025/10/14/water-electricity-concerns-addressed-by-stargate-data-center-leaders-in-abilene-texas/86585222007/?gnt-cfr=1&amp;gca-cat=p&amp;gca-uir=true&amp;gca-epti=undefined&amp;gca-ft=0&amp;gca-ds=sophi\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>noted,</span></u>⁠<span>(opens in a new window)</span></a><span> the water use at the Abilene site in a year will be half as much as Abilene uses in a single day. These designs are used across all of our Stargate AI campuses - including with Oracle in </span><a href=\"https://www.crusoe.ai/resources/blog/an-inside-look-at-the-abilene-ai-data-center\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>Abilene</span></u>⁠<span>(opens in a new window)</span></a><span> and </span><a href=\"https://vantage-dc.com/news/vantage-data-centers-unveils-plans-for-frontier-a-25b-mega-campus-in-texas-to-meet-unprecedented-ai-demand/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>Shackelford County</span></u>⁠<span>(opens in a new window)</span></a><span>, Texas; </span><a href=\"https://www.stackinfra.com/about/news-press/press-releases/stack-infrastructure-delivers-digital-infrastructure-for-data-center-campus-in-new-mexico/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>Doña Ana County, New Mexico</span></u>⁠<span>(opens in a new window)</span></a><span>; </span><a href=\"https://www.related-digital.com/news/openai-oracle-and-related-digital-announce-stargate-data-center-site-in-michigan\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>Saline Township, Michigan</span></u>⁠<span>(opens in a new window)</span></a><span>, </span><a href=\"https://vantage-dc.com/news/openai-oracle-and-vantage-data-centers-announce-stargate-data-center-site-in-wisconsin/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>Port Washington, Wisconsin</span></u>⁠<span>(opens in a new window)</span></a><span> and our latest AI campus with Microsoft in </span><a href=\"https://blogs.microsoft.com/on-the-issues/2025/09/18/made-in-wisconsin-the-worlds-most-powerful-ai-datacenter/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>Mount Pleasant, Wisconsin</span></u><span>.</span>⁠<span>(opens in a new window)</span></a><span> This approach is paired with site-specific environmental and infrastructure investments developed with local partners. For example, in Wisconsin our partners will</span><a href=\"https://vantage-dc.com/news/openai-oracle-and-vantage-data-centers-announce-stargate-data-center-site-in-wisconsin/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span> invest a minimum of $175M in local infrastructure upgrades and water restoration projects</span></u><span>.</span>⁠<span>(opens in a new window)</span></a></li><li><b><span>Investing early in local jobs and workforce pathways</span></b><span> by establishing OpenAI Academies as the backbone of regional workforce development in Stargate communities. These Academies will be customized for each site, </span><a href=\"https://openai.com/index/openai-certificate-courses/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>delivering credentials </span></u></a><span>and clear pathways into high-quality jobs aligned with local employers and the region’s evolving AI economy. We will launch our first Stargate community OpenAI Academy in Abilene, Texas this spring. We are also engaging alongside our partners with labor unions and workforce partners to support the skilled trades and technical workforce needed to build and operate AI infrastructure at this scale.</span></li></ul></div><p><span>Stargate is a physical infrastructure program that requires deep partnership. We’re reliant on and grateful to the communities that make it possible, and we are committed to showing up as long-term partners.</span></p><div><p><img alt=\"Group photo of Stargate community members from various partner companies standing together indoors around a welcome sign for the Milam County Data Center Open House hosted by OpenAI. Everyone is smiling in a casual, workshop-style event space.\" data-nosnippet=\"true\" loading=\"lazy\" width=\"1999\" height=\"1474\" decoding=\"async\" data-nimg=\"1\" sizes=\"(min-width: 1728px) 1728px, 100vw\" srcset=\"https://images.ctfassets.net/kftzwdyauwt9/6yxDKx6E7sKRsEEdK5yivC/bf3f92c8c0f22b7ed714f8725f934a5a/stargate-community.jpg?w=640&amp;q=90&amp;fm=webp 640w, https://images.ctfassets.net/kftzwdyauwt9/6yxDKx6E7sKRsEEdK5yivC/bf3f92c8c0f22b7ed714f8725f934a5a/stargate-community.jpg?w=750&amp;q=90&amp;fm=webp 750w, https://images.ctfassets.net/kftzwdyauwt9/6yxDKx6E7sKRsEEdK5yivC/bf3f92c8c0f22b7ed714f8725f934a5a/stargate-community.jpg?w=828&amp;q=90&amp;fm=webp 828w, https://images.ctfassets.net/kftzwdyauwt9/6yxDKx6E7sKRsEEdK5yivC/bf3f92c8c0f22b7ed714f8725f934a5a/stargate-community.jpg?w=1080&amp;q=90&amp;fm=webp 1080w, https://images.ctfassets.net/kftzwdyauwt9/6yxDKx6E7sKRsEEdK5yivC/bf3f92c8c0f22b7ed714f8725f934a5a/stargate-community.jpg?w=1200&amp;q=90&amp;fm=webp 1200w, https://images.ctfassets.net/kftzwdyauwt9/6yxDKx6E7sKRsEEdK5yivC/bf3f92c8c0f22b7ed714f8725f934a5a/stargate-community.jpg?w=1920&amp;q=90&amp;fm=webp 1920w, https://images.ctfassets.net/kftzwdyauwt9/6yxDKx6E7sKRsEEdK5yivC/bf3f92c8c0f22b7ed714f8725f934a5a/stargate-community.jpg?w=2048&amp;q=90&amp;fm=webp 2048w, https://images.ctfassets.net/kftzwdyauwt9/6yxDKx6E7sKRsEEdK5yivC/bf3f92c8c0f22b7ed714f8725f934a5a/stargate-community.jpg?w=3840&amp;q=90&amp;fm=webp 3840w\" src=\"https://images.ctfassets.net/kftzwdyauwt9/6yxDKx6E7sKRsEEdK5yivC/bf3f92c8c0f22b7ed714f8725f934a5a/stargate-community.jpg?w=3840&amp;q=90&amp;fm=webp\" style=\"max-width: 100%; height: auto; border-radius: 1rem; margin: 2rem 0px;\"></p></div></div><section id=\"citations\" data-testid=\"citations\"><ul><li><a href=\"https://openai.com/news/?tags=partnerships\" target=\"_blank\" rel=\"noopener noreferrer\">Partnerships</a></li><li><a href=\"https://openai.com/news/?tags=community\" target=\"_blank\" rel=\"noopener noreferrer\">Community</a></li><li><a href=\"https://openai.com/news/?tags=2026\" target=\"_blank\" rel=\"noopener noreferrer\">2026</a></li></ul></section></article></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/cisco",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/cisco",
      "title": "Cisco and OpenAI redefine enterprise engineering with AI agents",
      "publishedAt": "Tue, 20 Jan 2026 11:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:16.541Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div><p><span>For decades, Cisco has built and operated some of the world’s most complex, mission-critical software systems. As generative AI matured from experimentation to real operational capability, Cisco leaned into what it knows best: </span><b><span>scaling advanced technology inside demanding, real-world environments</span></b><span>.</span></p><p><span>That mindset led Cisco to begin working closely with OpenAI around Codex, helping define what enterprise-grade AI for software engineering should look like in practice—and how Codex could be applied to real, large-scale engineering work inside complex production environments.</span></p><p><span>Rather than treat Codex as a standalone developer tool, Cisco began integrating it directly into production engineering workflows, exposing it to massive multi-repository systems, C/C++-heavy codebases, and the security, compliance, and governance requirements of a global enterprise.</span></p><p><span>In the process, Cisco helped shape Codex into something fundamentally different from a developer productivity tool: </span><b><span>an AI engineering teammate capable of operating at enterprise scale</span></b><span>.</span></p><div><blockquote>\"I’ve loved discovering new opportunities to integrate Codex into Cisco's enterprise software lifecycle workflows. Collaborating with the OpenAI team to get Codex enterprise production ready has been rewarding as well.\"</blockquote><p>—Ching Ho, a member of Cisco's engineering leadership</p></div><div id=\"evaluating-agentic-ai-in-complex-codebases\"><p></p><h2><span>Evaluating agentic AI in complex codebases</span></h2><p></p></div><p><span>Cisco already runs a mature engineering organization with multiple AI initiatives in flight. What made Codex compelling wasn’t code completion or surface-level automation, but </span><b><span>agency</span></b><span>. Codex demonstrated the ability to:</span></p><div><ul><li><span>Understand and reason across </span><b><span>large, interconnected repositories</span></b></li><li><span>Work fluently in </span><b><span>complex languages</span></b></li><li><span>Execute real workflows through </span><b><span>CLI-based, autonomous compile-test-fix loops</span></b></li><li><span>Operate within existing </span><b><span>review, security, and governance frameworks</span></b></li></ul></div><p><span>By working directly with OpenAI, Cisco engineers were able to give feedback on how these capabilities behaved in real environments, shaping areas like workflow orchestration, security controls, and support for long-running engineering tasks—all of which are critical for enterprise use.</span></p><div id=\"using-codex-for-critical-engineering-workflows\"><p></p><h2><span>Using Codex for critical engineering workflows</span></h2><p></p></div><p><span>Once Codex was embedded into everyday engineering work, teams began applying it to some of their most challenging and time-consuming workflows:</span></p><p><b><span>Cross-repo build optimization</span></b><span>: Codex analyzed build logs and dependency graphs across more than 15 interconnected repositories, identifying inefficiencies. The result: a ~20% reduction in build times and more than </span><b><span>1,500 engineering hours saved per month</span></b><span> across global environments.</span></p><p><b><span>Defect remediation at scale (CodeWatch)</span></b><span>: Using Codex-CLI, Cisco automated defect repair with iterative, agentic execution on large-scale C/C++ codebases. What once took weeks of manual effort now completes in hours, delivering a </span><b><span>10-15× increase in defect resolution throughput</span></b><span> and freeing engineers to focus on design and validation.</span></p><p><b><span>Framework migrations in days, not weeks</span></b><span>: When Splunk teams needed to migrate multiple UIs from React 18 to 19, Codex handled the bulk of repetitive changes autonomously, compressing weeks of work into days and allowing engineers to concentrate on judgment-heavy decisions.</span></p><div><blockquote>“The biggest gains came when we stopped thinking about Codex as a tool and started treating it as part of the team. We use Codex to generate and follow a plan document, allowing the reviewing team to more easily understand both the process and the code generated.”</blockquote><p>—Ryan Brady, a Principal Engineer in Cisco's Splunk group</p></div><div id=\"shaping-codexs-roadmap-for-the-enterprise\"><p></p><h2><span>Shaping Codex's roadmap for the enterprise</span></h2><p></p></div><p><span>Cisco provided continuous feedback from real production use that helped OpenAI accelerate Codex’s readiness for large enterprises—particularly in areas like compliance, long-running task management, and integration with existing development pipelines.</span></p><p><span>For Cisco, the collaboration established a repeatable model for adopting next-generation AI: </span><b><span>deep technical partnership, real workloads, and leadership alignment from day one</span></b><span>.</span></p><div><blockquote>“Codex has become a meaningful part of how we think about AI-assisted development and operations going forward.”</blockquote><p>—Brad Murphy, a VP leading Cisco’s Splunk Engineering team</p></div><p><span>In the months ahead, Cisco and OpenAI will continue to collaborate closely on Codex and beyond to advance their shared mission of AI-native engineering at enterprise scale.</span></p></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/servicenow-powers-actionable-enterprise-ai-with-openai",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/servicenow-powers-actionable-enterprise-ai-with-openai",
      "title": "ServiceNow powers actionable enterprise AI with OpenAI",
      "publishedAt": "Tue, 20 Jan 2026 05:45:00 GMT",
      "fetchedAt": "2026-01-26T12:08:17.283Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div><p><b><span>Key takeaways:&nbsp;</span></b></p><div><ul><li><span>Multi-year agreement expands ServiceNow customer access to OpenAI frontier models.&nbsp;</span></li><li><span>OpenAI models will be a preferred intelligence capability for enterprises that run more than 80 billion workflows each year in ServiceNow.&nbsp;</span></li><li><span>OpenAI will support direct speech-to-speech and native voice technology in ServiceNow.&nbsp;</span></li></ul></div><p><span>ServiceNow, the AI control tower for business reinvention, today announced OpenAI will be a preferred intelligence capability for enterprises that run more than 80 billion workflows each year on its platform.</span></p><p><span>Enterprises around the world use ServiceNow to orchestrate workflows that keep their systems and operations running smoothly. In complex environments where technology is spread across many systems, teams, and vendors, ServiceNow ties everything together—helping organizations spot issues early, route work to the right people, manage approvals, and resolve challenges quickly so the business keeps moving.</span></p><p><span>ServiceNow’s AI Platform brings OpenAI models like GPT‑5.2 directly into these enterprise workflows, so AI can understand what’s happening, help decide what to do next, and take action within a customer’s secure infrastructure. With OpenAI, ServiceNow will unlock a new level of automation for the world’s largest companies enabling enterprise intelligence at scale for any function or department including IT, finance, sales, human resources, and more.&nbsp;</span></p><p><span>“ServiceNow leads the market in AI-powered workflows, setting the enterprise standard for real-world AI outcomes,” said Amit Zavery, president, chief operating officer, and chief product officer at ServiceNow. “Together, Service and OpenAI are building the future of AI experiences: deploying AI that takes end-to-end action in complex enterprise environments—not sandboxes. As companies shift experimenting with AI to deploying it at scale, they need the power of multiple AI leaders working together, to deliver faster, better outcomes. Bringing together our engineering teams and our respective technologies will drive faster value for customers and more intuitive ways of working with AI.”&nbsp;</span></p><p><span>“ServiceNow is helping enterprises bring agentic AI into workflows that are secure, scalable, and designed to deliver measurable outcomes,” said Brad Lightcap, chief operating officer at OpenAI. “With OpenAI frontier models and multimodal capabilities in ServiceNow, enterprises across every industry will benefit from intelligence that handles work end to end in even the most complex environments.”</span></p><p><span>With OpenAI, the ServiceNow AI Platform will leverage frontier intelligence like GPT‑5.2 so customers can understand more about what’s happening and take action inside enterprise workflows.</span></p><div id=\"powering-actionable-ai-workflows-for-enterprise-customers\"><p></p><h2><span>Powering actionable AI workflows for enterprise customers&nbsp;</span></h2><p></p></div><p><span>ServiceNow and OpenAI will support enterprises in adopting AI systems that can reason across tasks and carry out work with little human intervention. Customers can leverage OpenAI models alongside with ServiceNow workflows:&nbsp;</span></p><div><ul><li><b><span>AI assistance</span></b><span> that lets employees ask questions in natural language and get clear, actionable answers based on real enterprise data.</span></li><li><b><span>AI-powered summarization and content generation</span></b><span> for incidents, cases, knowledge articles, and service interactions, helping teams resolve issues faster with less manual effort.</span></li><li><b><span>Developer and admin tools</span></b><span> that turn intent into workflows, logic, and automation, dramatically speeding how business processes are built and updated.</span></li><li><b><span>Intelligent search and discovery</span></b><span> that pulls the right information from across enterprise systems exactly when it’s needed.</span></li></ul></div><p><span>For example, employees use ServiceNow in an intuitive experience where data, models, AI modalities, and workflows converge to ask for what they need in plain language, like “I need to view my benefits” or “this customer issue needs to be escalated.”</span></p><p><span>With GPT‑5.2 built directly into the ServiceNow AI Platform, those requests aren’t just answered—they’re acted on. The model pairs with the ServiceNow workflow engine, where it can access enterprise data, respect governance and permissions, and provide insights to trigger real actions. GPT‑5.2 helps add more context, decide what should happen next, and, via the ServiceNow platform, move work through approvals, and updates until it’s done. To employees it feels like chatting with a smart coworker; behind the scenes, it’s AI running real enterprise workflows end to end.</span></p><p><span>Looking ahead, ServiceNow and OpenAI will build toward more natural, multimodal experiences, where users can talk, type, or use visuals to interact with AI agents seamlessly.</span></p><div><p><span>ServiceNow extends OpenAI’s work with the world’s largest and most established enterprises, including </span><a href=\"https://openai.com/index/accenture-partnership/\" target=\"_blank\" rel=\"noopener noreferrer\"><span>Accenture</span></a><span>, </span><a href=\"https://corporate.walmart.com/news/2025/10/14/walmart-partners-with-openai-to-create-ai-first-shopping-experiences\" target=\"_blank\" rel=\"noopener noreferrer\"><span>Walmart</span>⁠<span>(opens in a new window)</span></a><span>, </span><a href=\"https://newsroom.paypal-corp.com/2025-10-28-OpenAI-and-PayPal-Team-Up-to-Power-Instant-Checkout-and-Agentic-Commerce-in-ChatGPT\" target=\"_blank\" rel=\"noopener noreferrer\"><span>PayPal</span>⁠<span>(opens in a new window)</span></a><span>, </span><a href=\"https://openai.com/index/intuit-partnership/\" target=\"_blank\" rel=\"noopener noreferrer\"><span>Intuit</span></a><span>, </span><a href=\"https://openai.com/index/target-partnership/\" target=\"_blank\" rel=\"noopener noreferrer\"><span>Target</span></a><span>, </span><a href=\"https://corporate.thermofisher.com/us/en/index/newsroom/Our-stories/Thermo-fisher-scientific-open-ai-collaboration.html\" target=\"_blank\" rel=\"noopener noreferrer\"><span>Thermo Fisher</span>⁠<span>(opens in a new window)</span></a><span>, </span><a href=\"https://openai.com/index/bny/\" target=\"_blank\" rel=\"noopener noreferrer\"><span>BNY</span></a><span>, </span><a href=\"https://openai.com/index/morgan-stanley/\" target=\"_blank\" rel=\"noopener noreferrer\"><span>Morgan Stanley</span></a><span>, </span><a href=\"https://openai.com/index/bbva-collaboration-expansion/\" target=\"_blank\" rel=\"noopener noreferrer\"><span>BBVA</span></a><span>, and many more.&nbsp;</span></p></div></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/our-approach-to-age-prediction",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/our-approach-to-age-prediction",
      "title": "Our approach to age prediction",
      "publishedAt": "Tue, 20 Jan 2026 00:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:16.099Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div><p><span>We’re rolling out age prediction on ChatGPT consumer plans to help determine whether an account likely belongs to someone under 18, so the right experience and safeguards can be applied to teens. As we’ve outlined in our </span><a href=\"https://openai.com/index/introducing-the-teen-safety-blueprint/\" target=\"_blank\" rel=\"noopener noreferrer\"><span>Teen Safety Blueprint</span>⁠</a><span> and </span><a href=\"https://openai.com/index/updating-model-spec-with-teen-protections/\" target=\"_blank\" rel=\"noopener noreferrer\"><span>Under-18 Principles for Model Behavior</span>⁠</a><span>, young people deserve technology that both expands opportunity and protects their well-being.</span></p><p><span>Age prediction builds on protections already in place. Teens who tell us they are under 18 when they sign up automatically receive additional safeguards to reduce exposure to sensitive or potentially harmful content. This also enables us to treat adults like adults and use our tools in the way that they want, within the bounds of safety. </span></p><p><span>We previously shared our </span><a href=\"https://openai.com/index/building-towards-age-prediction/\" target=\"_blank\" rel=\"noopener noreferrer\"><span>early plans</span>⁠</a><span> for age prediction, and today we’re sharing more detail as the rollout is underway.</span></p><div id=\"how-age-prediction-works\"><p></p><h2><span>How age prediction works</span></h2><p></p></div><p><span>ChatGPT uses an age prediction model to help estimate whether an account likely belongs to someone under 18. The model looks at a combination of behavioral and account-level signals, including how long an account has existed, typical times of day when someone is active, usage patterns over time, and a user’s stated age. Deploying age prediction helps us learn which signals improve accuracy, and we use those learnings to continuously refine the model over time.</span></p><p><span>Users who are incorrectly placed in the under-18 experience will always have a fast, simple way to confirm their age and restore their full access with a selfie through Persona, a secure identity-verification service. Users can check if safeguards have been added to their account and start this process at any time by going to Settings &gt; Account.</span></p><p><span>When the age prediction model estimates that an account may belong to someone under 18, ChatGPT automatically applies </span><a href=\"https://openai.com/index/updating-model-spec-with-teen-protections/\" target=\"_blank\" rel=\"noopener noreferrer\"><span>additional protections</span>⁠</a><span> designed to reduce exposure to sensitive content, such as:</span></p><div><ul><li><span>Graphic violence or gory content</span></li><li><span>Viral challenges that could encourage risky or harmful behavior in minors</span></li><li><span>Sexual, romantic, or violent role play</span></li><li><span>Depictions of self-harm</span></li><li><span>Content that promotes extreme beauty standards, unhealthy dieting, or body shaming</span></li></ul></div><p><span>This approach is guided by expert input and rooted in academic literature about the science of child development and acknowledges known teen differences in risk perception, impulse control, peer influence, and emotional regulation. While these content restrictions help reduce teens’ exposure to sensitive material, we are focused on continually improving these protections, especially to address attempts to bypass our safeguards. When we are not confident about someone’s age or have incomplete information, we default to a safer experience.</span></p><p><span>In addition to these safeguards, parents can choose to customize their teen’s experience further through </span><a href=\"https://help.openai.com/en/articles/12315553-parental-controls-on-chatgpt-faq\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><span>parental controls</span>⁠<span>(opens in a new window)</span></a><span> including setting quiet hours when ChatGPT can not be used, controlling features such as memory or model training, and receiving notifications if signs of acute distress are detected.</span></p><div id=\"whats-next\"><p></p><h2><span>What’s next</span></h2><p></p></div><p><span>We’re learning from the initial rollout and continuing to improve the accuracy of age prediction over time. We will closely track rollout and use those signals to guide ongoing improvements.</span></p><p><span>In the EU, age prediction will roll out in the coming weeks to account for regional requirements. For more detail, visit our </span><a href=\"https://help.openai.com/en/articles/12652064-age-prediction-in-chatgpt\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><span>help page</span>⁠<span>(opens in a new window)</span></a><span>. </span></p><p><span>While this is an important milestone, our work to support teen safety is ongoing. We’ll continue to share updates on our progress and what we’re learning, in dialogue with experts including the American Psychological Association, ConnectSafely, and </span><a href=\"https://openai.com/index/strengthening-chatgpt-responses-in-sensitive-conversations/\" target=\"_blank\" rel=\"noopener noreferrer\"><span>Global Physicians Network</span>⁠</a><span>.</span></p></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/ai-for-self-empowerment",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/ai-for-self-empowerment",
      "title": "AI for self empowerment",
      "publishedAt": "Sun, 18 Jan 2026 12:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:17.188Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div><p><span>We create AI in order to empower people. As AI technology progress continues, a key focus for the world should be tracking the </span><b><span>capability overhang</span></b><span>: the gap between what AI systems can now do and the value most people, businesses, and countries are actually capturing from them at scale.</span></p><p><span>One challenge with even understanding the size of the capability overhang is that AI is a very counterintuitive technology which continually plays out in surprising ways. We launched ChatGPT three years ago with an at-the-time frontier AI that could answer simple questions and were shocked by how quickly it was adopted and the creative ways people found to apply it in their lives. We now have frontier AI which can reason and act across increasingly complex tasks, from building software to performing mathematical research, and we don’t yet know the full extent of how it will integrate into business or personal life. As Alan Kay said, “the best way to predict the future is to invent it,” and we’ve found that it takes a community of people exploring these tools to find amazing new uses that their capability has unlocked.</span></p><p><span>Just like with the invention of computers themselves, those who lean into the technology can reach a new level of individual productivity. For example, ChatGPT usage data reveals that the typical power user uses 7x more thinking power (and thus 7x more compute power) than the typical user. Power users don’t just use AI more—they use AI more comprehensively, applying the most advanced capabilities across a wider range of tasks—and as a result they can produce more economically valuable work.</span></p><p><span>We expect AI to enable some incredible high-level outcomes—such as double-digit GDP growth, affordable and effective healthcare, and rapid scientific advancement—which can raise the floor for everyone, while also raising the ceiling in new ways. The better we manage the capability overhang as a society, the more people will empower themselves to create new economic opportunities by fully exploring what these new tools can do.</span></p><div id=\"openais-principles-for-managing-the-capability-overhang\"><p></p><h2><span>OpenAI’s principles for managing the capability overhang</span></h2><p></p></div><p><span>In periods of rapid change, accurate information creates agency. People, businesses, and policymakers can make better decisions given well-grounded, credible data about what is actually happening: which roles are growing or shrinking, how AI is being used in practice, and where productivity gains are emerging. We don’t have all the answers, but we can help.</span></p><p><span>That’s why we’ve been making public our foundational economic data based insights, including a measurement of how AI tools match up in terms of performance with humans across a range of tasks.</span></p><p><span>As AI increasingly shapes economic outcomes, access to core tools will help people achieve more. Businesses need practical ways to adopt AI at scale, and countries need a systematic way for AI to be applied in their jurisdiction. AI’s usefulness will scale directly with compute power, and thus every individual, business, and country needs ways of accessing their own slice of compute.</span></p><p><span>That’s why we </span><b><span>created a free tier of ChatGPT, supported by advertising</span></b><span>, towards our mission of ensuring AGI benefits all of humanity. We created a new industry-standard API for developers to build applications on top of. We work with many institutions large and small—including governments—to support broad access to frontier AI capabilities.</span></p><p><span>We build tools which people can use to shape the future. That means putting a lot of power into people’s hands and designing our tools to be customizable and usable in ways we didn’t anticipate.</span></p><p><span>This approach is useful for </span><b><span>everyday users and small businesses</span></b><span>, where we design product experiences that help people do more than they thought they could—optimize a family’s budget, find jobs more effectively, develop ideas for a future business. It’s also useful for unlocking the next wave of innovation, such as for</span><b><span> builders and founders</span></b><span>, where we support high-agency people who turn frontier capability into new companies, products, and markets.</span></p><p><span>We believe this is how the Intelligence Age can expand opportunity for everyone, and result in a future that is much better than the present. We believe everyone should focus on managing the capability overhang so as many people, businesses, and countries empower themselves to participate in and benefit from the positive transformative potential of AI.</span></p></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/a-business-that-scales-with-the-value-of-intelligence",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/a-business-that-scales-with-the-value-of-intelligence",
      "title": "A business that scales with the value of intelligence",
      "publishedAt": "Sun, 18 Jan 2026 10:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:17.463Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div><p><span>We launched ChatGPT as a research preview to understand what would happen if we put frontier intelligence directly in people’s hands.</span></p><p><span>What followed was broad adoption and deep usage on a scale that no one predicted.</span></p><p><span>More than experimenting with AI, people folded ChatGPT into their lives. Students started using it to untangle homework they were stuck on late at night. Parents started using it to plan trips and manage budgets. Writers used it to break through blank pages. More and more, people used it to understand their lives. People used ChatGPT to help make sense of health symptoms, prepare for doctor visits, and navigate complex decisions. People used it to think more clearly when they were tired, stressed, or unsure.</span></p><p><span>Then they brought that leverage to work.</span></p><p><span>At first, it showed up in small ways. A draft refined before a meeting. A spreadsheet checked one more time. A customer email rewritten to land the right tone. Very quickly, it became part of daily workflows. Engineers reasoned through code faster. Marketers shaped campaigns with sharper insight. Finance teams modeled scenarios with greater clarity. Managers prepared for hard conversations with better context.</span></p><p><span>What began as a tool for curiosity became infrastructure that helps people create more, decide faster, and operate at a higher level.</span></p><p><span>That transition sits at the heart of how we build OpenAI. We are a research and deployment company. Our job is to close the distance between where intelligence is advancing and how individuals, companies, and countries actually adopt and use it.</span></p><p><span>As ChatGPT became a tool people rely on every day to get real work done, we followed a simple and enduring principle: our business model should scale with the value intelligence delivers.</span></p><p><span>We have applied that principle deliberately. As people demanded more capability and reliability, we introduced consumer subscriptions. As AI moved into teams and workflows, we created workplace subscriptions and added usage-based pricing so costs scale with real work getting done. We also built a platform business, enabling developers and enterprises to embed intelligence through our APIs, where spend grows in direct proportion to outcomes delivered.</span></p><p><span>More recently, we have applied the same principle to commerce. People come to ChatGPT not just to ask questions, but to decide what to do next. What to buy. Where to go. Which option to choose. Helping people move from exploration to action creates value for users and for the partners who serve them. Advertising follows the same arc. When people are close to a decision, relevant options have real value, as long as they are clearly labeled and genuinely useful.</span></p><p><span>Across every path, we apply the same standard. Monetization should feel native to the experience. If it does not add value, it does not belong.</span></p><p><span>Both our Weekly Active User (WAU) and Daily Active User (DAU) figures continue to produce all-time highs. This growth is driven by a flywheel across compute, frontier research, products, and monetization. Investment in compute powers leading-edge research and step-change gains in model capability. Stronger models unlock better products and broader adoption of the OpenAI platform. Adoption drives revenue, and revenue funds the next wave of compute and innovation. The cycle compounds.</span></p><p><span>Looking back on the past three years, our ability to serve customers—as measured by revenue—directly tracks available compute: Compute grew 3X year over year or 9.5X from 2023 to 2025: 0.2 GW in 2023, 0.6 GW in 2024, and ~1.9 GW in 2025. While revenue followed the same curve growing 3X year over year, or 10X from 2023 to 2025: $2B ARR in 2023, $6B in 2024, and $20B+ in 2025. This is never-before-seen growth at such scale. And we firmly believe that more compute in these periods would have led to faster customer adoption and monetization.</span></p><p><span>Compute is the scarcest resource in AI. Three years ago, we relied on a single compute provider. Today, we are working with providers across a diversified ecosystem. That shift gives us resilience and, critically, compute certainty. We can plan, finance, and deploy capacity with confidence in a market where access to compute defines who can scale.</span></p><p><span>This turns compute from a fixed constraint into an actively managed portfolio. We train frontier models on premium hardware when capability matters most. We serve high-volume workloads on lower-cost infrastructure when efficiency matters more than raw scale. Latency drops. Throughput improves. And we can deliver useful intelligence at costs measured in cents per million tokens. That is what makes AI viable for everyday workflows, not just elite use cases.</span></p><p><span>On top of this compute layer sits a product platform that spans text, images, voice, code, and APIs. Individuals and organizations use it to think, create, and operate more effectively. The next phase is agents and workflow automation that run continuously, carry context over time, and take action across tools. For individuals, that means AI that manages projects, coordinates plans, and executes tasks. For organizations, it becomes an operating layer for knowledge work.</span></p><p><span>As these systems move from novelty to habit, usage becomes deeper and more persistent. That predictability strengthens the economics of the platform and supports long-term investment.</span></p><p><span>The business model closes the loop. We began with subscriptions. Today we operate a multi-tier system that includes consumer and team subscriptions, a free ad- and commerce-supported tier that drives broad adoption, and usage-based APIs tied to production workloads. Where this goes next will extend beyond what we already sell. As intelligence moves into scientific research, drug discovery, energy systems, and financial modeling, new economic models will emerge. Licensing, IP-based agreements, and outcome-based pricing will share in the value created. That is how the internet evolved. Intelligence will follow the same path.</span></p><p><span>This system requires discipline. Securing world-class compute requires commitments made years in advance, and growth does not move in a perfectly smooth line. At times, capacity leads usage. At other times, usage leads capacity. We manage that by keeping the balance sheet light, partnering rather than owning, and structuring contracts with flexibility across providers and hardware types. Capital is committed in tranches against real demand signals. That lets us lean forward when growth is there without locking in more of the future than the market has earned.</span></p><p><span>That discipline sets up our focus for 2026: practical adoption. The priority is closing the gap between what AI now makes possible and how people, companies, and countries are using it day to day. The opportunity is large and immediate, especially in health, science, and enterprise, where better intelligence translates directly into better outcomes.</span></p><p><span>Infrastructure expands what we can deliver. Innovation expands what intelligence can do. Adoption expands who can use it. Revenue funds the next leap. This is how intelligence scales and becomes a foundation for the global economy.</span></p></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/the-truth-elon-left-out",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/the-truth-elon-left-out",
      "title": "The truth left out from Elon Musk’s recent court filing",
      "publishedAt": "Fri, 16 Jan 2026 12:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:16.724Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div><div><p><img alt=\"Screenshot comparing a court filing summary with a longer excerpt of Elon Musk's actual 2017 remarks. The top box shows the filing's claiming that Elon wanted OpenAI to remain &quot;essentially philanthropic,&quot; while the lower section highlights Musk's original comments about transitioning from a nonprofit to a &quot;B-corp&quot; or &quot;C-corp&quot;. Blue highlights mark text shown in the court filing; red highlights mark added emphasis.\" data-nosnippet=\"true\" loading=\"lazy\" width=\"2262\" height=\"1682\" decoding=\"async\" data-nimg=\"1\" sizes=\"(min-width: 1728px) 1728px, 100vw\" srcset=\"https://images.ctfassets.net/kftzwdyauwt9/i5W4vlX7N6jkUkFXJZq69/e56406ab7a6c8c1873539ab8967d11e3/OAI_Court_Filing_Excerpt_v3.1.png?w=640&amp;q=90&amp;fm=webp 640w, https://images.ctfassets.net/kftzwdyauwt9/i5W4vlX7N6jkUkFXJZq69/e56406ab7a6c8c1873539ab8967d11e3/OAI_Court_Filing_Excerpt_v3.1.png?w=750&amp;q=90&amp;fm=webp 750w, https://images.ctfassets.net/kftzwdyauwt9/i5W4vlX7N6jkUkFXJZq69/e56406ab7a6c8c1873539ab8967d11e3/OAI_Court_Filing_Excerpt_v3.1.png?w=828&amp;q=90&amp;fm=webp 828w, https://images.ctfassets.net/kftzwdyauwt9/i5W4vlX7N6jkUkFXJZq69/e56406ab7a6c8c1873539ab8967d11e3/OAI_Court_Filing_Excerpt_v3.1.png?w=1080&amp;q=90&amp;fm=webp 1080w, https://images.ctfassets.net/kftzwdyauwt9/i5W4vlX7N6jkUkFXJZq69/e56406ab7a6c8c1873539ab8967d11e3/OAI_Court_Filing_Excerpt_v3.1.png?w=1200&amp;q=90&amp;fm=webp 1200w, https://images.ctfassets.net/kftzwdyauwt9/i5W4vlX7N6jkUkFXJZq69/e56406ab7a6c8c1873539ab8967d11e3/OAI_Court_Filing_Excerpt_v3.1.png?w=1920&amp;q=90&amp;fm=webp 1920w, https://images.ctfassets.net/kftzwdyauwt9/i5W4vlX7N6jkUkFXJZq69/e56406ab7a6c8c1873539ab8967d11e3/OAI_Court_Filing_Excerpt_v3.1.png?w=2048&amp;q=90&amp;fm=webp 2048w, https://images.ctfassets.net/kftzwdyauwt9/i5W4vlX7N6jkUkFXJZq69/e56406ab7a6c8c1873539ab8967d11e3/OAI_Court_Filing_Excerpt_v3.1.png?w=3840&amp;q=90&amp;fm=webp 3840w\" src=\"https://images.ctfassets.net/kftzwdyauwt9/i5W4vlX7N6jkUkFXJZq69/e56406ab7a6c8c1873539ab8967d11e3/OAI_Court_Filing_Excerpt_v3.1.png?w=3840&amp;q=90&amp;fm=webp\" style=\"max-width: 100%; height: auto; border-radius: 1rem; margin: 2rem 0px;\"></p></div><p><span>In his </span><a href=\"https://storage.courtlistener.com/recap/gov.uscourts.cand.433688/gov.uscourts.cand.433688.379.59.pdf\" target=\"_blank\" rel=\"noopener noreferrer\"><span>latest court filing</span>⁠<span>(opens in a new window)</span></a><span>, Elon cherry-picks and publishes snippets from Greg Brockman's private journal entries (obtained as part of legal discovery) which, when read with the surrounding context, tell a very different story from what Elon claims.</span></p><p><span>The truth is that we and Elon agreed in 2017 that a for-profit structure would be the next phase for OpenAI; negotiations ended when we refused to give him full control; we rejected his offer to merge OpenAI into Tesla; we tried to find another path to achieve the mission together; and then he quit OpenAI, encouraging us to find our own path to raising billions of dollars, without which he gave us a </span><a href=\"https://openai.com/index/elon-musk-wanted-an-openai-for-profit/#december-2018-elon-told-us-to-raise-billions-per-year-immediately-or-forget-it\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>0% chance</span></u>⁠</a><span> of success.</span></p><p><span>We did find a path to advance the mission, with OpenAI </span><a href=\"https://openai.com/index/built-to-benefit-everyone/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>now structured</span></u>⁠</a><span> as two main entities: a public benefit corporation (“PBC”) and a controlling non-profit which owns equity in the PBC currently valued at approximately $130 billion.</span></p><div><blockquote><p><span>In 2017, OpenAI’s founders became concerned that developing artificial general intelligence would require more resources than a nonprofit could raise through charitable donations. Ex. 38. They discussed numerous ideas, including creating a for-profit arm for OpenAI, collaborating with an existing company, or restructuring in some other manner. Ex. 1 (Musk Tr.) at 70:6-72:15, 78:18- 79:14, 95:8-16. Musk insisted that any new entity “support[ ] the nonprofit’s mission” and that OpenAI remain</span><span> </span><span>“essentially [a] philanthropic endeavor.”</span></p></blockquote></div><div data-shaded-container=\"true\"><!--$--><div><p><span>gdb: over upcoming weeks, how much of your time should we plan for?<br></span><span>elon:</span><span> coming weeks, top priority. </span><span>gotta figure out how do we transition from non-profit to something which is</span><span> </span><span>essentially philanthropic endeavor</span><span> </span><span>and is B-corp or C-corp or something.</span><span> must tell the story and not lose moral high ground. </span><span>absolutely vital.</span><span><br>gdb: yep.<br>elon: need to understand B-corp situation.<br>ilya: i have *some* thoughts. one idea keep non-profit and have C-corp. but i find this approach less appealing because in some sense C-corp not beholden. one formulation: mission is to minimize ex risk by building friendly AGI. could be a way to go. like it more as a new entity beholden to the mission.<br>elon: agree, does sound better. never even heard of a B-corp until sam brought it up.</span><b><span> </span></b><span>it does sound like right move. would not shut down </span><span>the non-profit, should still exist in some form.</span><span><br>ilya: no opinion, as long as the main entity has something fundamentally philanthropic.<br>elon: any prefs on name?<br>ilya: openai. continuity of the mission. it’s all the same.<br>elon: i agree, think that makes sense. well, i’m pretty excited about doing this with you.<br>ilya: all that’s left is the minor thing of actually doing it.<br>elon: alright cool. </span><span>i’m gonna figure out the details of the whole B-corp thing tomorrow</span><span> and get that process under way. and then let’s just stay in frequent touch. make this happen as quickly as possible.</span></p></div><!--/$--><!--$--><div><p><span>blue</span><span> </span><i><span>= quote shown in Elon’s court filing<br></span></i><span>red</span><span> </span><i><span>= emphasis added</span></i></p></div><!--/$--></div><p><span>Elon did not think that OpenAI needed to remain solely a non-profit. As the context shows, he agreed that OpenAI needed both a non-profit and a for-profit entity—the exact structure OpenAI has today, and that Elon is now suing OpenAI over. At the time, he said only that the non-profit should continue to exist “in some form.” Ilya, not Elon, suggested that the for-profit should have a connection to the OpenAI mission. Shortly after this call, Elon actually </span><a href=\"https://openai.com/index/elon-musk-wanted-an-openai-for-profit/#september-2017-elon-created-the-public-benefit-corporation-called-open-artificial-intelligence-technologies-inc\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>created</span></u>⁠</a><span> an OpenAI PBC (or “B-corp”).</span></p><p><span>Despite his court filings glossing over the details of these negotiations, they were quite intense and involved deeply personal conversations. Elon said he wanted to accumulate $80B for a self-sustaining city on Mars, and that he needed and deserved majority equity. He said that he needed full control since he’d been burned by not having it in the past, and when we discussed succession he surprised us by talking about his children controlling AGI.</span></p><p><span>Discussions about OpenAI’s structure—and Elon’s proposal that he lead and control it—progressed to the point that Elon asked us to check references with people who had worked closely with him. One pointed out the parallel to Elon’s Mars ambitions, which had started as a </span><a href=\"https://spaceref.com/status-report/marsnow-19-profile-elon-musk-life-to-mars-foundation/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>philanthropic project</span></u>⁠<span>(opens in a new window)</span></a><span> and grew into a commercial one, and mentioned that Elon tends to vilify people who quit his companies.</span></p><p><span>Even before negotiations over OpenAI’s structure began, Elon leveraged OpenAI for the benefit of his for-profit ventures. For example, in early 2017, he asked OpenAI to send a team to help fix Tesla’s self-driving program, Autopilot. Scott Gray, Ilya, Greg, and Andrej Karpathy ended up devoting significant time to improving Autopilot, which led to Elon recruiting Andrej to join Tesla fulltime. Elon never truly treated OpenAI as an independent non-profit.</span></p><div><blockquote><p><span>But Musk’s co-founders secretly had other plans. On November 6, 2017, following a meeting with Sutskever, Brockman admitted in his private diary that their </span><span>“conclusion is we truly want the [for-profit] b-corp.”</span></p></blockquote></div><div data-shaded-container=\"true\"><!--$--><div><p><span>- sam: </span><span>say, we wanna do openai, if you wanna do inside tesla it’s ok. if weirdly competitive should revisit.</span><span><br>- ilya: was thinking, what we need.<br>- sam: depends how strategically dependent we would be, could imagine a way that works.<br>- ilya: earlier more concerned. don’t want only strategy to be tesla. wouldn’t wanna be locked into something that only tesla<br>- gdb: why be ok with similar effort in tesla?<br>- sam: won’t work, how little<br>- sam: would like E to stay on board, not go to war. don’t think will work, tell him that. maybe he’ll get the hardware built inside of tesla.<br>- ilya: if i’m truly honest, </span><span>my prefs here are not set in stone, some pref for b-corp. can be made happy in the non-profit world.</span><span><br>- sam: what involvement?<br>- sam: would prefer to have him involved,<br>- sam: much more interested in you two being motivated + non-distracted. normally have one thing. that’s moved around a lot. thing i care about the most optimum output. if you guys are not gonna be happy with the non-profit, let’s figure out the b-corp. If you don’t want e involved,<br>&nbsp;- do have weakly held views, but pale in comparison to making you guys fully happy.</span></p><p><span>** ilya and me breakfast</span></p><p><span>- </span><span>conclusion is we truly want the b-corp.</span><span> honestly we also want to get back to work. but it’s not super clear how we get there.</span></p></div><!--/$--><!--$--><div><p><span>blue</span><span> </span><i><span>= quote shown in Elon’s court filing<br></span></i><span>red</span><span> </span><i><span>= emphasis added</span></i></p></div><!--/$--></div><p><span>While Greg and Ilya preferred the B-corp, they were open to pursuing the mission within the standalone non-profit—if it could raise enough funding. Their preference was not a secret; the thinking was shared with Elon and his key staff.</span></p><p><span>Negotiations over an OpenAI for-profit with Elon fell through in September 2017 because we wouldn’t give him absolute control. In early October, Shivon Zilis (at the time Elon’s liaison to OpenAI) told Greg and Ilya that Elon wanted them to “commit” to the non-profit by giving him two more board seats, agreeing not to quit for some period of time, and signing a non-solicit.&nbsp;Greg and Ilya considered this proposal but </span><b><span>never agreed</span></b><span> to it given their concerns about the non-profit’s ability to raise enough capital and Elon’s ability to start a competitor without consequence. They discussed with Shivon whether a B-corp with Elon was still possible; she said Elon had told her it could be but not for the next 6-9 months—enough time to get some distance from these intense negotiations and for Elon to focus on Tesla’s Model 3 ramp-up.</span></p><p><span>A few weeks later, Elon told us that it was a happy accident that the for-profit hadn’t materialized as it never could have obtained enough funding. Instead, he began trying to convince us that merging OpenAI into Tesla was the only path to the mission, and it seemed that he would pursue an AGI competitor within Tesla regardless.</span></p><p><span>By early November, we were trying to figure out a path forward that didn’t involve ending up at Tesla. The November 6, 2017 entry goes through more thinking (e.g. “</span><b><span>the answer is that we would have preferred the b-corp, but we prefer the non-profit to tesla.</span></b><span>”) and eventually comes to an ordered list of options for how to move forward: “</span><b><span>[1] b-corp w/ appropriate control structure [2] non-profit w/ lots of funding [3] us quitting and doing our own thing</span></b><span>”.</span></p><div><blockquote><p><span>Brockman privately fretted about concealing their plans from Musk. </span><span>“[C]annot say that we are committed to the non-profit. don’t wanna say that we’re committed. if three months later we’re doing b-corp then it was a lie.”</span><span> Ex. 43 at 2. </span><span>“[Musk’s] story will correctly be that we weren’t honest with him in the end about still wanting to do the for profit.”</span><span> Id. at 3. </span><span>“[I]t’d be wrong to steal the non-profit from [Musk]. to convert to a b-corp without him. that’d be pretty morally bankrupt.”</span></p></blockquote></div><div data-shaded-container=\"true\"><!--$--><div><p><span>if he starts the competitor</span><span> going to be less</span></p><p><span>cannot say that we are committed to the non-profit. don’t wanna say that we’re committed. if three months later we’re doing b-corp then it was a lie.</span></p><p><span>statement must be, right now we have these concerns, </span><span>we want to see the fundraising landscape.</span><span> hypothesis: after robot hand and 5v5, totally diff world. we need, upper bound, $150M for 2018. and right now more dollars wouldn’t make us move faster.</span></p><p><span>and so... what does this all mean. </span><span>we are really unsure if he starts a competitor that it’s going to end in a good place.</span><span> btw right now we have this great position of power. but it can of course all go away very quickly.</span></p><p><span>- policy gradients</span></p><p><span>ok, so not feeling so great about all of this. the true answer is that we want him out.</span></p><p><span>ok, so talking to shivon: </span><span>she’s like how are you gonna raise the billions in the non-profit? of course E isn’t going to put personal money on the line, not going to give up on mars.</span></p><p><span>if he really thinks that AGI is gonna happen first and be the craziest thing and change everything, then why not?</span></p><p><span>can’t see us turning this into a for-profit without a very nasty fight. i’m just thinking about the office and we’re in the office. </span><span>and his story will correctly be that we weren’t honest with him in the end about still wanting to do the for profit</span><span> </span><span>just without him.</span></p><p><span>[...]</span></p><p><span>he was like, </span><span>ok. fundraising is super hard. i’ve worked hard on gates and he didn’t even stop by office. dustin donated but doesn’t even show up personally. that’s how much these people care about this. didn’t expect to be 70% of the cash contributions. thought I’d be one piece. there are all these names on the website and they haven’t contributed. reality check, no one’s gonna give you this amount of money, when they see the $10B they’re going to run to the hills.</span></p><p><span>one of the two of us is not based in reality.</span></p><p><span>but he was super supportive of us trying, and said, go ahead, I’m happy with anyone (even people who just want to get access to me), happy to give them any fancy title, diminish my own title, etc.. if you can do it then i want to learn from you. I’m very good at getting people to part with their money, i could raise $lb for tesla in a week and have you seen that company’s financials?</span></p><p><span>anyway, definitely the best this meeting could have gone. </span><span>we now have a focus, a goal, and if we accomplish it, then we’ll really have shown him that we can outperform him even at something he’s great at. if we fail, well, we’ll deal with it then.</span></p><p><span>btw another realization from this is that </span><span>it’d be wrong to steal the non-profit from him. to convert to a b-corp without him. that’d be pretty morally bankrupt.</span><span> and he’s really not an idiot.</span></p></div><!--/$--><!--$--><div><p><span>purple</span><span> = </span><i><span>Greg’s notes on what Elon said</span></i><span><br></span><span>blue</span><span> </span><i><span>= quote shown in Elon’s court filing<br></span></i><span>red</span><span> </span><i><span>= emphasis added</span></i></p></div><!--/$--></div><p><span>In this entry, Greg and Ilya were still considering Elon’s demand that they “commit” to the non-profit by accepting his terms—something that, again, </span><b><span>never happened</span></b><span>. There was a chance that Elon would resume funding if they agreed to his conditions; there was a chance he’d resign to go create an AGI competitor within Tesla. Greg and Ilya felt that it’d be deceptive to accept Elon’s terms and then later create a for-profit without his approval. So they were nervous that, by accepting, they’d get stuck in a structure that was unable to raise sufficient capital while Elon left to pursue his own AGI project at Tesla. </span><b><span>That’s why they decided not to accept his proposal</span></b><span>. No one ever lied to Elon; no one ever told him they’d accepted his terms when they hadn’t.</span></p><p><span>In early November, we privately considered removing Elon from our board. We didn’t want to merge OpenAI into Tesla as he’d suggested, and he seemed intent on starting a competitor there regardless of what we did. Elon’s departure from OpenAI would have made it easier to figure out some structure for raising capital because it would have removed the impasse caused by his need for absolute control.</span></p><p><span>Still, though we felt Elon’s departure would have facilitated pursuit of the mission, we didn’t remove him. We kept trying to find a path forward with him because we thought that was the moral thing to do (per the same journal entry: “</span><b><span>Ilya feeling like we morally should not be kicking elon out, and should be trying to make the non-profit work and convince him to stay</span></b><span>”). We decided to try fundraising in the non-profit for a time to see what was achievable. We said that we’d need to eventually raise $10B, which Elon said seemed totally impossible. It was clear we’d need a long-term solution other than remaining solely a non-profit indefinitely.</span></p><div id=\"what-happened-next\"><p></p><h2><span>What happened next</span></h2><p></p></div><p><span>Over the next few months, we worked on fundraising and brainstorming ideas to obtain the next level of funding necessary to pursue and achieve the mission. In mid-January 2018, Elon congratulated us on a successful fundraise, agreed we should do an initial coin offering (“ICO”) to raise $10B—which would involve a for-profit subsidiary—and told us that we'd solved the long-term funding problem. By the end of January, however, he told us he no longer supported the ICO (which we’d soured on too by then) and that OpenAI was “on a path of certain failure relative to Google.”</span></p><p><span>By February 2018, Elon had decided that OpenAI would not be able to raise sufficient funding. That month, he quit even though we </span><a href=\"https://openai.com/index/elon-musk-wanted-an-openai-for-profit/#january-2018-elon-said-openai-was-on-a-path-for-certain-failure-unless-we-merged-into-tesla\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>told him</span></u>⁠</a><span> that we’d “</span><b><span>been working on a fundraising structure that does not rely on a public offering, and we will be curious to hear your feedback</span></b><span>.” On Elon’s way out, he said that he supported us pursuing the path we saw to raising billions of dollars—he just didn’t think we could succeed and he would instead focus on building AGI at Tesla for that reason.</span></p><p><span>For a time, the split with Elon seemed amicable. Shivon Zilis joined our board and helped us navigate Elon’s moods when, on occasion, he lashed out. But then Elon prepared to launch xAI and Shivon stepped off our board. Elon began the campaign of harassment that he’s waging now, with relentless public attacks and various lawsuits, as he’s tried to gain competitive ground for xAI.</span></p><p><span>While there’s a long way to go, we’ve made much more progress than Elon thought possible—including getting the OpenAI non-profit, now called the </span><a href=\"https://openai.com/foundation/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>OpenAI Foundation</span></u>⁠</a><span>, on track to become the best resourced non-profit in history. We’re grateful for the many users and partners who work with us to collectively advance our mission of ensuring AGI benefits all of humanity.</span></p></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/introducing-chatgpt-go",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/introducing-chatgpt-go",
      "title": "Introducing ChatGPT Go, now available worldwide",
      "publishedAt": "Fri, 16 Jan 2026 00:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:17.780Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div id=\"main\" tabindex=\"-1\"><span aria-live=\"polite\" aria-atomic=\"true\">OpenAI</span><nav inert=\"\"></nav><article><div><p><span>In August 2025, we introduced ChatGPT Go in India as a low-cost subscription designed to expand access to ChatGPT’s most popular features and help more people use advanced AI in their daily life. Since then, ChatGPT Go has rolled out to 170 additional countries, making it our fastest growing plan and among the most affordable AI subscription globally. </span></p><p><span>In markets where Go has been available, we’ve seen strong adoption and regular everyday use for tasks like writing, learning, image creation, and problem-solving. This early momentum helped inform our decision to make ChatGPT Go available globally. </span></p><p><span>Starting today, ChatGPT Go is rolling out everywhere ChatGPT is available. In the US, Go is available for $8 per month.</span></p><p><span>With this launch, ChatGPT now offers three subscription tiers globally:</span></p><div><ul><li><span>ChatGPT Go at $8 USD/month*</span></li><li><span>ChatGPT Plus at $20 USD/month</span></li><li><span>ChatGPT Pro at $200 USD/month</span></li></ul></div><p><sup><span>*US price displayed. Go pricing is localized in some markets.</span></sup></p><div id=\"what-you-get-with-chatgpt-go\"><p></p><h2><span>What you get with ChatGPT Go</span></h2><p></p></div><p><span>ChatGPT Go is designed for people who want expanded access to our latest model, GPT‑5.2 Instant, at a lower price point—more messages, more uploads, and more image creation. With ChatGPT Go, you get:</span></p><div><ul><li><span>10x more messages, file uploads and image creation than the free tier, so you can keep chatting with no limits on GPT‑5.2 Instant.</span></li><li><span>Longer memory and context window, so ChatGPT can remember more helpful details about you over time.</span></li></ul></div><p><span>This now sits alongside our two existing consumer subscription plans: ChatGPT Plus and ChatGPT Pro.</span></p><p><span>ChatGPT Plus is designed for work that requires deeper reasoning—like writing and editing documents, learning and research, or data analysis. It offers expanded access to our most advanced models, including GPT‑5.2 Thinking, along with the flexibility to choose legacy models and use our coding agent, Codex. Compared to Go, Plus includes higher limits for messages, file uploads, memory, and context, so ChatGPT can remember more detail from past conversations and support longer, more continuous workflows.</span></p><p><span>ChatGPT Pro is built for AI power users pushing the limits of advanced intelligence. It offers full access to our most powerful model, GPT‑5.2 Pro, along with maximum memory and context, and early previews of our newest features.</span></p><div id=\"supporting-accessibility-with-ads\"><p></p><h2><span>Supporting accessibility with ads</span></h2><p></p></div><p><span>We plan to begin testing ads in the free tier and ChatGPT Go in the US soon. Ads support our commitment to making AI accessible to everyone by helping us keep ChatGPT available at free and affordable price points.</span></p><p><span>ChatGPT Plus, Pro, Business and Enterprise will remain ad-free.</span></p><p><span>Read more about how we plan to introduce ads </span><a href=\"https://openai.com/index/our-approach-to-advertising-and-expanding-access/\" target=\"_blank\" rel=\"noopener noreferrer\"><span>here</span>⁠</a><span>.</span></p><div id=\"plan-details\"><p></p><h2><span>Plan details</span></h2><p></p></div></div><section id=\"citations\" data-testid=\"citations\"><ul><li><a href=\"https://openai.com/news/?tags=2026\" target=\"_blank\" rel=\"noopener noreferrer\">2026</a></li></ul></section></article></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/our-approach-to-advertising-and-expanding-access",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/our-approach-to-advertising-and-expanding-access",
      "title": "Our approach to advertising and expanding access to ChatGPT",
      "publishedAt": "Fri, 16 Jan 2026 00:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:17.993Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div id=\"main\" tabindex=\"-1\"><span aria-live=\"polite\" aria-atomic=\"true\">OpenAI</span><article><div><p><span>AI is reaching a point where everyone can have a personal super-assistant that helps them learn and do almost anything. Who gets access to that level of intelligence will shape whether AI expands opportunity or reinforces the same divides.</span></p><p><span>We’ve been working to make powerful AI accessible to everyone through our free product and low-cost subscription tier, ChatGPT Go, which has launched in 171 countries since August. Today we’re </span><a href=\"https://openai.com/index/introducing-chatgpt-go/\" target=\"_blank\" rel=\"noopener noreferrer\"><span>bringing Go to the U.S. and everywhere ChatGPT is available</span>⁠</a><span>, giving people expanded access to messaging, image creation, file uploads and memory for $8 USD/month. In the coming weeks, we’re also planning to start testing ads in the U.S. for the free and Go tiers, so more people can benefit from our tools with fewer usage limits or without having to pay. Plus, Pro, Business, and Enterprise subscriptions will not include ads.</span></p><p><span>People trust ChatGPT for many important and personal tasks, so as we introduce ads, it’s crucial we preserve what makes ChatGPT valuable in the first place. That means you need to trust that ChatGPT’s responses are driven by what’s objectively useful, never by advertising. You need to know that your data and conversations are protected and never sold to advertisers. And we need to keep a high bar and give you control over your experience so you see truly relevant, high-quality ads—and can turn off personalization if you want. </span></p><p><span>Given that, we want to be clear about the principles that guide our approach to advertising:</span></p><div id=\"our-ads-principles\"><p></p><h2><span>Our ads principles</span></h2><p></p></div><div><ul><li><b><span>Mission alignment: </span></b><span>Our mission is to ensure AGI benefits all of humanity; our pursuit of advertising is always in support of that mission and making AI more accessible.</span></li><li><b><span>Answer independence: </span></b><span>Ads do not influence the answers ChatGPT gives you. Answers are optimized based on what's most helpful to you. Ads are always separate and clearly labeled.</span></li><li><b><span>Conversation privacy: </span></b><span>We keep your conversations with ChatGPT private from advertisers, and we never sell your data to advertisers.</span></li><li><b><span>Choice and control: </span></b><span>You control how your data is used. You can turn off personalization, and you can clear the data used for ads at any time. We’ll always offer a way to not see ads in ChatGPT, including a paid tier that’s ad-free.</span></li><li><b><span>Long-term value: </span></b><span>We do not optimize for time spent in ChatGPT. We prioritize user trust and user experience over revenue.</span></li></ul></div><p><span>We’re not launching ads yet, but we do plan to start testing in the coming weeks for logged in adults in the U.S. on the free and Go tiers. To start, we plan to test ads at the bottom of answers in ChatGPT when there’s a relevant sponsored product or service based on your current conversation. Ads will be clearly labeled and separated from the organic answer. You’ll be able to learn more about why you’re seeing that ad, or dismiss any ad and tell us why. During our test, we will not show ads in accounts where the user tells us or we predict that they are under 18, and ads are not eligible to appear near sensitive or regulated topics like health, mental health or politics.</span></p><p><span>Here’s an example of what the first ad formats we plan to test could look like: </span></p><div><p><img alt=\"Mobile phone screen showing a ChatGPT response with simple, authentic Mexican dinner party recipes, followed by a clearly labeled sponsored product recommendation from Harvest Groceries for a hot sauce item, displayed against a soft blue gradient background.\" data-nosnippet=\"true\" loading=\"lazy\" width=\"3840\" height=\"2160\" decoding=\"async\" data-nimg=\"1\" sizes=\"(min-width: 1728px) 1728px, 100vw\" srcset=\"https://images.ctfassets.net/kftzwdyauwt9/73QmMtFD4PrKmQkzxO5ErF/2c226c43ba3658e8dab69b1299aa0ebf/OAI_Ad_Blog_Inline-AdMock2_16x9__1_.png?w=640&amp;q=90&amp;fm=webp 640w, https://images.ctfassets.net/kftzwdyauwt9/73QmMtFD4PrKmQkzxO5ErF/2c226c43ba3658e8dab69b1299aa0ebf/OAI_Ad_Blog_Inline-AdMock2_16x9__1_.png?w=750&amp;q=90&amp;fm=webp 750w, https://images.ctfassets.net/kftzwdyauwt9/73QmMtFD4PrKmQkzxO5ErF/2c226c43ba3658e8dab69b1299aa0ebf/OAI_Ad_Blog_Inline-AdMock2_16x9__1_.png?w=828&amp;q=90&amp;fm=webp 828w, https://images.ctfassets.net/kftzwdyauwt9/73QmMtFD4PrKmQkzxO5ErF/2c226c43ba3658e8dab69b1299aa0ebf/OAI_Ad_Blog_Inline-AdMock2_16x9__1_.png?w=1080&amp;q=90&amp;fm=webp 1080w, https://images.ctfassets.net/kftzwdyauwt9/73QmMtFD4PrKmQkzxO5ErF/2c226c43ba3658e8dab69b1299aa0ebf/OAI_Ad_Blog_Inline-AdMock2_16x9__1_.png?w=1200&amp;q=90&amp;fm=webp 1200w, https://images.ctfassets.net/kftzwdyauwt9/73QmMtFD4PrKmQkzxO5ErF/2c226c43ba3658e8dab69b1299aa0ebf/OAI_Ad_Blog_Inline-AdMock2_16x9__1_.png?w=1920&amp;q=90&amp;fm=webp 1920w, https://images.ctfassets.net/kftzwdyauwt9/73QmMtFD4PrKmQkzxO5ErF/2c226c43ba3658e8dab69b1299aa0ebf/OAI_Ad_Blog_Inline-AdMock2_16x9__1_.png?w=2048&amp;q=90&amp;fm=webp 2048w, https://images.ctfassets.net/kftzwdyauwt9/73QmMtFD4PrKmQkzxO5ErF/2c226c43ba3658e8dab69b1299aa0ebf/OAI_Ad_Blog_Inline-AdMock2_16x9__1_.png?w=3840&amp;q=90&amp;fm=webp 3840w\" src=\"https://images.ctfassets.net/kftzwdyauwt9/73QmMtFD4PrKmQkzxO5ErF/2c226c43ba3658e8dab69b1299aa0ebf/OAI_Ad_Blog_Inline-AdMock2_16x9__1_.png?w=3840&amp;q=90&amp;fm=webp\" style=\"max-width: 100%; height: auto; border-radius: 1rem; margin: 2rem 0px;\"></p></div><p><span>The best ads are useful, entertaining, and help people discover new products and services. Given what AI can do, we're excited to develop new experiences over time that people find more helpful and relevant than any other ads. Conversational interfaces create possibilities for people to go beyond static messages and links. For example, soon you might see an ad and be able to directly ask the questions you need to make a purchase decision. </span></p><div><p><img alt=\"Two mobile phone screens showing a ChatGPT conversation about traveling to Santa Fe, New Mexico, with an informational travel response on the left and a clearly labeled sponsored listing for “Pueblo &amp; Pine” desert cottages, and a follow-up chat view with a text input on the right, displayed against a soft blue gradient background.\" data-nosnippet=\"true\" loading=\"lazy\" width=\"3840\" height=\"2160\" decoding=\"async\" data-nimg=\"1\" sizes=\"(min-width: 1728px) 1728px, 100vw\" srcset=\"https://images.ctfassets.net/kftzwdyauwt9/1oiSiUkLymK0xlEOWcJubw/d7e078d14fb84534abc115d16223c5ca/OAI_Ad_Blog_Inline-AdMock1_16x9.png?w=640&amp;q=90&amp;fm=webp 640w, https://images.ctfassets.net/kftzwdyauwt9/1oiSiUkLymK0xlEOWcJubw/d7e078d14fb84534abc115d16223c5ca/OAI_Ad_Blog_Inline-AdMock1_16x9.png?w=750&amp;q=90&amp;fm=webp 750w, https://images.ctfassets.net/kftzwdyauwt9/1oiSiUkLymK0xlEOWcJubw/d7e078d14fb84534abc115d16223c5ca/OAI_Ad_Blog_Inline-AdMock1_16x9.png?w=828&amp;q=90&amp;fm=webp 828w, https://images.ctfassets.net/kftzwdyauwt9/1oiSiUkLymK0xlEOWcJubw/d7e078d14fb84534abc115d16223c5ca/OAI_Ad_Blog_Inline-AdMock1_16x9.png?w=1080&amp;q=90&amp;fm=webp 1080w, https://images.ctfassets.net/kftzwdyauwt9/1oiSiUkLymK0xlEOWcJubw/d7e078d14fb84534abc115d16223c5ca/OAI_Ad_Blog_Inline-AdMock1_16x9.png?w=1200&amp;q=90&amp;fm=webp 1200w, https://images.ctfassets.net/kftzwdyauwt9/1oiSiUkLymK0xlEOWcJubw/d7e078d14fb84534abc115d16223c5ca/OAI_Ad_Blog_Inline-AdMock1_16x9.png?w=1920&amp;q=90&amp;fm=webp 1920w, https://images.ctfassets.net/kftzwdyauwt9/1oiSiUkLymK0xlEOWcJubw/d7e078d14fb84534abc115d16223c5ca/OAI_Ad_Blog_Inline-AdMock1_16x9.png?w=2048&amp;q=90&amp;fm=webp 2048w, https://images.ctfassets.net/kftzwdyauwt9/1oiSiUkLymK0xlEOWcJubw/d7e078d14fb84534abc115d16223c5ca/OAI_Ad_Blog_Inline-AdMock1_16x9.png?w=3840&amp;q=90&amp;fm=webp 3840w\" src=\"https://images.ctfassets.net/kftzwdyauwt9/1oiSiUkLymK0xlEOWcJubw/d7e078d14fb84534abc115d16223c5ca/OAI_Ad_Blog_Inline-AdMock1_16x9.png?w=3840&amp;q=90&amp;fm=webp\" style=\"max-width: 100%; height: auto; border-radius: 1rem; margin: 2rem 0px;\"></p></div><p><span>Ads also can be transformative for small businesses and emerging brands trying to compete. AI tools level the playing field even further, allowing anyone to create high-quality experiences that help people discover options they might never have found otherwise.</span></p><p><span>We’ll learn from feedback and refine how ads show up over time, but our commitment to putting users first and maintaining trust won’t change. By starting our ad platform from the ground up with these principles in place, we can align our incentives with what people want from ChatGPT. Our long-term focus remains on building products that millions of people and businesses find valuable enough to pay for. Our enterprise and subscription businesses are already strong, and we believe in having a diverse revenue model where ads can play a part in making intelligence more accessible to everyone.</span></p><p><span>Once we begin testing our first ad formats in the coming weeks and months, we look forward to getting people's feedback and ensuring that ads can support broad access to AI and keep the trust that makes ChatGPT valuable.</span></p></div><section id=\"citations\" data-testid=\"citations\"><ul><li><a href=\"https://openai.com/news/?tags=2026\" target=\"_blank\" rel=\"noopener noreferrer\">2026</a></li><li><a href=\"https://openai.com/news/?tags=chatgpt\" target=\"_blank\" rel=\"noopener noreferrer\">ChatGPT</a></li></ul></section></article></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/investing-in-merge-labs",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/investing-in-merge-labs",
      "title": "Investing in Merge Labs",
      "publishedAt": "Thu, 15 Jan 2026 07:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:17.587Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div id=\"main\" tabindex=\"-1\"><span aria-live=\"polite\" aria-atomic=\"true\">OpenAI</span><article><div><p><span>Progress in interfaces enables progress in computing. Each time people gain a more direct way to express intent, technology becomes more powerful and more useful.</span></p><p><span>Brain computer interfaces (BCIs) are an important new frontier. They open new ways to communicate, learn, and interact with technology. BCIs will create a natural, human-centered way for anyone to seamlessly interact with AI. This is why OpenAI is participating in Merge Labs’ seed round.&nbsp;</span></p><p><span>Merge Labs is a research lab with the long-term mission of bridging biological and artificial intelligence to maximize human ability, agency, and experience. It is developing </span><a href=\"https://merge.io/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>fundamentally new approaches</span></u>⁠<span>(opens in a new window)</span></a><span> to BCIs that are safe and interface with the brain at much higher bandwidth by combining biology, devices, and AI.</span></p><p><span>AI will play a central role to Merge’s approach. It will accelerate research and development including bioengineering, neuroscience and device engineering. In addition, high-bandwidth interfaces will benefit from AI operating systems that can interpret intent, adapt to individuals, and operate reliably with limited and noisy signals. OpenAI will collaborate with Merge Labs on scientific foundation models and other frontier tools to accelerate progress.</span></p><p><span>We are excited to support and collaborate with Merge Labs as they turn an ambitious idea into reality and ultimately products that are useful for people.</span></p><p><span>Merge Labs’ co-founders include researchers Mikhail Shapiro, Tyson Aflalo, and Sumner Norman, who have pioneered entirely new approaches to BCI. They are complemented by technology entrepreneurs Alex Blania, Sandro Herbig and Sam Altman in a personal capacity.&nbsp;</span></p></div></article></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/strengthening-the-us-ai-supply-chain",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/strengthening-the-us-ai-supply-chain",
      "title": "Strengthening the U.S. AI supply chain through domestic manufacturing",
      "publishedAt": "Thu, 15 Jan 2026 00:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:18.351Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div id=\"main\" tabindex=\"-1\"><span aria-live=\"polite\" aria-atomic=\"true\">OpenAI</span><article><div><p>New Request for Proposals to help build and scale the infrastructure behind advanced AI.</p></div><div><p><span>Building the infrastructure required to power advanced AI presents a historic opportunity to strengthen domestic supply chains and </span><a href=\"https://cdn.openai.com/pdf/21b88bb5-10a3-4566-919d-f9a6b9c3e632/%5BOpenAI_OSTP%20RFI%20Oct%2027%202025%5D.pdf\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><u><span>reindustrialize the country</span></u>⁠<span>(opens in a new window)</span></a><span>. If we seize it, we can catalyze U.S. manufacturing, modernize our energy grid, create well-paid jobs, and strengthen American leadership. Infrastructure has long been destiny when it comes to America’s economic success, and that will be especially true in the Intelligence Age.</span></p><p><span>At OpenAI, we’re committed to doing our part. Since launching our Stargate initiative almost one year ago, we’ve announced planned capacity that puts us well over halfway to meeting our 10-gigawatt commitment. These investments are already translating into good jobs and local economic growth in communities across the country. Over the coming years, we’ll build on this progress by strengthening the broader domestic AI supply chain and accelerating investment in U.S. manufacturing capabilities that support American AI leadership.</span></p><p><span>As part of that work, today we’re launching a new </span><a href=\"https://cdn.openai.com/pdf/rfp-for-us-hardware-manufacturing.pdf\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><span>Request for Proposals</span>⁠<span>(opens in a new window)</span></a><span> (RFP) focused on U.S.-based manufacturing across key parts of the AI supply chain, including data center inputs, consumer electronics, and robotics. Our goal is to identify and enable domestic manufacturing capacity that can help shorten timelines, strengthen resilience, and extend technology leadership as AI infrastructure scales.</span></p><p><span>We’re seeking proposals from manufacturers, suppliers, and partners who are building – or are prepared to build – critical components and systems for the AI ecosystem in the United States, including:</span></p><div><ul><li><span>Modules, tooling and equipment, and final assembly for consumer electronics</span></li><li><span>Manufacturing for compute, power, cooling, and supporting data center hardware</span></li><li><span>Critical inputs for advanced robotics (e.g., gearboxes, motors, power electronics)</span></li></ul></div><p><span>When people talk about AI infrastructure, the conversation often stops at chips and data centers. But advanced AI depends on a much broader ecosystem of physical components: the racks, cabling, networking gear, cooling systems, power systems, power electronics, electromechanical modules, and testing and assembly capacity are all required to bring it all online at scale. Our RFP aims to build on momentum that already exists by identifying where targeted partnerships, demand signals, and coordination can help unlock faster growth, larger scale, and more durable U.S. leadership in AI.</span></p><p><span>Responses to the RFP will help inform partnerships, procurement strategies, and infrastructure planning. We see this as part of a broader story of reindustrialization. Across the country, manufacturers are investing in advanced production capabilities to support the AI ecosystem – bringing new facilities online, modernizing supply chains, and expanding skilled workforces. These investments are critical to ensuring that the benefits of AI are created and shared here in the U.S..</span></p><p><span>We will review proposals on a rolling basis and follow up with selected respondents on next steps. The deadline to submit a proposal is June 2026.</span></p></div><section id=\"citations\" data-testid=\"citations\"><ul><li><a href=\"https://openai.com/news/?tags=2026\" target=\"_blank\" rel=\"noopener noreferrer\">2026</a></li></ul></section></article></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/cerebras-partnership",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/cerebras-partnership",
      "title": "OpenAI partners with Cerebras  ",
      "publishedAt": "Wed, 14 Jan 2026 14:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:18.164Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div id=\"main\" tabindex=\"-1\"><span aria-live=\"polite\" aria-atomic=\"true\">OpenAI</span><article><div><p>OpenAI is partnering with Cerebras to add 750MW of ultra low-latency AI compute to our platform. </p></div><div><p><span>Cerebras builds purpose-built AI systems to accelerate long outputs from AI models. Its unique speed comes from putting massive compute, memory, and bandwidth together on a single giant chip and eliminating the bottlenecks that slow inference on conventional hardware.&nbsp;</span></p><p><span>&nbsp;Integrating Cerebras into our mix of compute solutions is all about making our AI respond much faster. When you ask a hard question, generate code, create an image, or run an AI agent, there is a loop happening behind the scenes: you send a request, the model thinks, and it sends something back. When AI responds in real time, users do more with it, stay longer, and run higher-value workloads.</span></p><p><span>We will integrate this low-latency capacity into our inference stack in phases, expanding across workloads.&nbsp;&nbsp;</span></p><p><span>“OpenAI’s compute strategy is to build a resilient portfolio that matches the right systems to the right workloads. Cerebras adds a dedicated low-latency inference solution to our platform. That means faster responses, more natural interactions, and a stronger foundation to scale real-time AI to many more people,” said Sachin Katti of OpenAI.</span></p><p><span>“We are delighted to partner with OpenAI, bringing the world’s leading AI models to the world’s fastest AI processor. Just as broadband transformed the internet, real-time inference will transform AI, enabling entirely new ways to build and interact with AI models,” said Andrew Feldman, co-founder and CEO of Cerebras.&nbsp;</span></p><p><span>The capacity will come online in multiple tranches through 2028.</span></p></div></article></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/zenken",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/zenken",
      "title": "Zenken boosts a lean sales team with ChatGPT Enterprise",
      "publishedAt": "Tue, 13 Jan 2026 16:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:18.463Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div><p><span>Zenken combines GPT‑5, custom GPTs, image generation, and the latest OpenAI models and tools to accelerate both of its core businesses: web marketing and the fast growing overseas human resources business. As one of the first companies in Japan to roll out ChatGPT Enterprise across the organization, Zenken is putting an AI first approach into practice and seeing clear business impact.</span></p><p><span>Today Zenken reports:</span></p><div><ul><li><span>More than 90% weekly active usage of ChatGPT Enterprise</span></li><li><span>Average time savings of 30 to 50% across knowledge work tasks</span></li><li><span>5 to 15 additional hours per employee every month, reinvested in higher value work</span></li><li><span>50 million yen in annual outsourcing cost savings compared with the previous year</span></li></ul></div><div id=\"deciding-to-adopt-chatgpt-enterprise-that-meets-twelve-critical-requirements\"><p></p><h2><span>Deciding to adopt ChatGPT Enterprise that meets twelve critical requirements</span></h2><p></p></div><p><span>Before adopting ChatGPT Enterprise, Zenken relied on manual processes for a wide range of knowledge tasks. Employees spent large amounts of time on research, summarization, translation, and content creation. Sales teams also faced long preparation times for client meetings, which reduced the number and quality of customer conversations.</span></p><p><span>To address these issues, Zenken decided to introduce AI in a structured and secure way. Yuji Okada, Manager of the Corporate Planning Department in the Administration Division, explains that when they evaluated potential solutions, they “focused on twelve capabilities required for our business, such as security and advanced support for complex thinking,” and recalls that “ChatGPT Enterprise was the only solution to meet all of these conditions.” He notes, \"Because we deal with client and sensitive internal information, preventing data leaks is non-negotiable. ChatGPT Enterprise guarantees that our data won't be used to train its AI. This security feature gave us the confidence to safely handle confidential data.\"</span></p><p><span>The availability of reasoning models was also a decisive reason for choosing ChatGPT Enterprise. In Zenken’s core web marketing business, teams need to analyze clients’ business challenges in depth and propose strategic solutions. The reasoning model released by OpenAI in 2024 provides the advanced support for complex thinking that Zenken was looking for and has further accelerated the company’s use of AI. Okada explains, “The reasoning model goes beyond simply providing information and supports management level decision making in areas such as market analysis, competitive strategy, and evaluation of new business opportunities.”</span></p><p><span>Today, the reasoning model improves the quality of strategy planning and analysis across teams and is also used to examine Zenken’s own management issues and new challenges from a strategic perspective. It has become a key driver of Zenken’s business growth.</span></p><div id=\"transforming-sales-activities-with-ai-supported-workflows\"><p></p><h2><span>Transforming sales activities with AI supported workflows</span></h2><p></p></div><p><span>The impact of ChatGPT Enterprise is especially visible in sales. Before using ChatGPT Enterprise, the sales team had to spend large amounts of time on preparation work such as researching prospects, creating proposal materials, and writing sales emails. Okada recalls, “Even before we could think about increasing the number of clients we approached, our people were already losing a lot of time on preparation. Because they had to respond to inquiries and create proposal documents, the time they could actually spend with customers was limited, and sometimes we missed out on opportunities.” Writing each email also took time, which created a natural limit on how many prospects a salesperson could reach in a single day.</span></p><p><span>By rolling out ChatGPT Enterprise across the company, Zenken has significantly improved the quality of its sales activities. The table below compares the sales process before and after ChatGPT at each phase.</span></p><div><table><tbody><tr><th></th><th><p><span>Before ChatGPT</span></p></th><th><p><span>With ChatGPT</span></p></th></tr><tr><td><p><span>Preparation phase</span></p></td><td><p><span>Gather information manually&nbsp;</span></p></td><td><p><b><span>Use ChatGPT for deeper industry and customer analysis </span></b></p></td></tr><tr><td><p><span>Discovery phase</span></p></td><td><p><span>One way, checklist style questioning </span></p></td><td><p><b><span>Two way, consultative conversations </span></b></p></td></tr><tr><td><p><span>Meeting phase</span></p></td><td><p><span>When questions arise, answers are often deferred to a later follow up</span></p></td><td><p><b><span>Ask ChatGPT during the meeting and respond on the spot </span></b></p></td></tr><tr><td><p><span>Proposal phase</span></p></td><td><p><span>Standard product centric proposals focused on specs and features&nbsp;</span></p></td><td><p><b><span>Personalized proposals based on customer insights</span></b></p></td></tr></tbody></table></div><p><span>As a result, Zenken has seen clear gains in sales performance:</span></p><div><ul><li><span>Proposals passing initial review up 15 to 20%</span></li><li><span>Win rate for new deals up 5 to 10%</span></li><li><span>Approval rate for final proposals up about 30%</span></li></ul></div><p><span>“ChatGPT helps us shorten the time required for research and preparation, while also making our conversations with clients smoother,” explains Okada.</span></p><div id=\"strengthening-international-communication\"><p></p><h2><span>Strengthening international communication</span></h2><p></p></div><p><span>ChatGPT Enterprise has also become essential to Zenken’s fast growing overseas human resources business. Because most employees are native Japanese speakers, the company previously had to devote significant time and budget to translating job postings, contracts, and other documents for overseas candidates. Okada recalls, “When we needed to communicate in multiple languages including English, conventional translation tools often failed to carry over the right context and nuance, which made some international communication quite challenging.”</span></p><p><span>After implementing ChatGPT Enterprise, the team was able to accurately translate a wide range of documents into English and other languages. This allowed them to cut outsourcing costs while significantly increasing translation speed. “Even with a smaller team, we have been able to maintain an organization that continues to drive revenue growth,” says Okada.</span></p><p><span>Employees are also using ChatGPT in languages beyond English to gather information from news sites, technical articles, and other sources that are important for the business. This makes it easier to design new services and business initiatives with global expansion in mind and enables more people at Zenken to participate directly in cross border work than ever before.</span></p><div id=\"shifting-from-routine-tasks-to-creative-high-value-work\"><p></p><h2><span>Shifting from routine tasks to creative, high value work</span></h2><p></p></div><p><span>ChatGPT Enterprise is reshaping how day to day work gets done at Zenken. Teams now use ChatGPT to generate first drafts of sales emails, proposals, marketing content, and internal documents in minutes, and overall productivity has roughly doubled across the organization. Work that was previously outsourced is increasingly handled in house, cutting annual outsourcing costs by around 50 million yen and improving margins. “Instead of being tied up in repetitive tasks, employees across the company now have more time to focus on the substance of their proposals,” says Okada.</span></p><p><span>Adoption is also strong at the individual level. Weekly active usage of ChatGPT Enterprise exceeds 90%, and employees send an average of about 900 messages per person each month. “It has become second nature for us to turn to ChatGPT first,” Okada explains. “We use ChatGPT to develop an initial hypothesis and then discuss it with our managers or colleagues. Working this way has greatly reduced the time people spend feeling stuck before they talk to others or move forward.”</span></p><p><span>For Zenken, ChatGPT Enterprise is more than a tool. In Okada’s words, it has become a partner that is changing how employees think and work. The company now sees ChatGPT Enterprise as a core platform that supports everything from day to day efficiency and strategic planning to talent development and, ultimately, company wide transformation.</span></p></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/openai-raising-concerns-policy",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/openai-raising-concerns-policy",
      "title": "OpenAI’s Raising Concerns Policy",
      "publishedAt": "Mon, 12 Jan 2026 00:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:17.400Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div id=\"main\" tabindex=\"-1\"><span aria-live=\"polite\" aria-atomic=\"true\">OpenAI</span><article><div><p>We’re publishing our Raising Concerns Policy, which protects employees’ rights to make protected disclosures.</p></div><div><p><span>We believe rigorous debate about AI is essential to ensuring that AI is safe and benefits the most people possible, which is why OpenAI has a Raising Concerns Policy. This policy protects employees’ rights to raise issues, including about AI safety, applicable laws, or company policies. We’ve recently revised our policy—you can read it </span><a href=\"https://cdn.openai.com/policies/openai-raising-concerns-policy.pdf\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><span>here</span>⁠<span>(opens in a new window)</span></a><span>.</span></p><div id=\"background-on-openais-raising-concerns-policy\"><p></p><h2><span>Background on OpenAI’s Raising Concerns Policy</span></h2><p></p></div><p><span>Our Raising Concerns Policy outlines various ways employees can raise issues and expressly prohibits harassment and retaliation against those who decide to do so. Our policy also makes clear that employees have the right to make reports or disclosures to government agencies, including the U.S. National Labor Relations Board, the U.S. Equal Employment Opportunity Commission, the U.S. Securities and Exchange Commission, and the California Attorney General’s Office.</span></p><p><span>In April 2024, we introduced a new 24/7 Integrity Line, which allows OpenAI employees to anonymously raise concerns if they prefer not to use existing channels, such as through their manager, HR, Compliance or Legal. When potential legal or policy violations are reported, they are handled by our Compliance, Legal, Employee Relations, and Security teams. Certain issues may also be shared with the Audit Committee of the Board of Directors.</span></p><p><span>Like other companies in our industry, OpenAI continues to distinguish between raising concerns and revealing company trade secrets. Given our technology’s implications for U.S. national security, the latter (subject to the right to make protected disclosures) remains prohibited under confidentiality agreements for current and former employees.</span></p></div><section id=\"citations\" data-testid=\"citations\"><ul><li><a href=\"https://openai.com/news/?tags=policies-procedures\" target=\"_blank\" rel=\"noopener noreferrer\">Policies and Procedures</a></li><li><a href=\"https://openai.com/news/?tags=global-affairs\" target=\"_blank\" rel=\"noopener noreferrer\">Global Affairs</a></li><li><a href=\"https://openai.com/news/?tags=2026\" target=\"_blank\" rel=\"noopener noreferrer\">2026</a></li></ul></section></article></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/stargate-sb-energy-partnership",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/stargate-sb-energy-partnership",
      "title": "OpenAI and SoftBank Group partner with SB Energy",
      "publishedAt": "Fri, 09 Jan 2026 11:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:18.663Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div id=\"main\" tabindex=\"-1\"><span aria-live=\"polite\" aria-atomic=\"true\">OpenAI</span><article><div><div><ul><li><i><span>SoftBank Group and OpenAI invest $1 billion in SB Energy to support its growth as a leading development and execution partner for data center campuses&nbsp;</span></i></li><li><i><span>OpenAI signs 1.2 GW data center lease for initial data center buildout</span></i></li><li><i><span>SB Energy will become a major customer of OpenAI, leveraging its APIs and deploying ChatGPT for its employees</span></i></li><li><i><span>SB Energy additionally secured $800 million of Redeemable Preferred Equity from Ares to further support the company’s growth</span></i></li></ul></div><p><b><span>Redwood City, CA, Jan. 9, 2026</span></b><span>—</span><a href=\"https://sbenergy.com/\" target=\"_blank\" rel=\"noopener noreferrer\"><span>SB Energy</span>⁠<span>(opens in a new window)</span></a><span>, a SoftBank Group company, announced today a strategic partnership with OpenAI as part of </span><a href=\"https://group.softbank/en/news/press/20250924\" target=\"_blank\" rel=\"noopener noreferrer\"><span>Stargate</span>⁠<span>(opens in a new window)</span></a><span>, marking a significant step forward in the build out of next-generation artificial intelligence (AI) and energy infrastructure in the United States. The investment builds on the $500 billion Stargate commitment announced in January at the White House.</span></p><p><span>To support the partnership and as demand for AI compute accelerates, OpenAI and SoftBank Group are each investing $500 million into SB Energy. OpenAI has also selected SBE to build and operate its </span><a href=\"https://openai.com/index/five-new-stargate-sites/\" target=\"_blank\" rel=\"noopener noreferrer\"><span>previously-announced</span></a><span> 1.2 GW data center site in Milam County. The equity funding supports SB Energy’s growth as a leading development and execution partner for data center campuses and associated energy infrastructure. SB Energy is currently developing several multi-gigawatt data center campuses, with initial facilities under construction and expected to enter service starting in 2026.&nbsp;</span></p><p><b><span>OpenAI co-founder and President Greg Brockman</span></b><span> said, “Partnering with SB Energy brings together their strength in data center infrastructure and energy development and OpenAI’s deep domain expertise in data center engineering. The result is a fast, reliable way to scale compute through large, highly optimized AI data centers.”</span></p><p><b><span>SB Energy co-CEO Rich Hossfeld</span></b><span> said, “SB Energy’s strategic partnership with OpenAI accelerates our delivery of advanced AI data center campuses and associated energy infrastructure at the scale required to advance Stargate and secure America’s AI future. We are grateful for our longstanding sponsor SoftBank Group and new partner OpenAI for their investment in our platform, our team, and our long-term vision.”</span></p><p><span>As part of this transaction, OpenAI, SoftBank Group, and SB Energy have also formed a non-exclusive preferred partnership to develop a new model for data center builds that brings together OpenAI’s first-party data center design with SB Energy’s proven expertise in speed, cost discipline, and integrated energy delivery to deliver purpose-built AI infrastructure at scale. With each project, SB Energy and OpenAI will invest in communities through well-paying jobs, workforce development, and grid modernization to deliver durable economic growth for partner communities.</span></p><p><span>The Milam County Data Center will create thousands of construction jobs. OpenAI and SB Energy have designed the data center to minimize water usage, and plan to build new generation to support the Milam County Data Center’s energy needs and protect Texas ratepayers.</span></p></div><section id=\"citations\" data-testid=\"citations\"><ul><li><a href=\"https://openai.com/news/?tags=partnerships\" target=\"_blank\" rel=\"noopener noreferrer\">Partnerships</a></li><li><a href=\"https://openai.com/news/?tags=2026\" target=\"_blank\" rel=\"noopener noreferrer\">2026</a></li></ul></section></article></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/datadog",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/datadog",
      "title": "Datadog uses Codex for system-level code review",
      "publishedAt": "Fri, 09 Jan 2026 00:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:18.799Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div><p><a href=\"https://www.datadoghq.com/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>Datadog</span></u>⁠<span>(opens in a new window)</span></a><span> runs one of the world’s most widely-used observability platforms, helping companies monitor, troubleshoot, and secure complex distributed systems. When something breaks, customers depend on Datadog to surface issues fast, which means reliability has to be built in long before code ever reaches production.</span></p><p><span>For Datadog’s engineering teams, that makes code review a high-stakes moment. It’s not just about catching mistakes, but about understanding how changes ripple through interconnected systems—an area where traditional static analysis and rule-based tools often fall short.</span></p><p><span>To meet this challenge, Datadog’s AI Development Experience (AI DevX) team turned to Codex, the coding agent from OpenAI, which brings system-level reasoning into code review and surfaces risks humans can’t easily see at scale.</span></p><p><span>“Time savings are real and important,” says Brad Carter, who leads Datadog’s AI DevX team. “But preventing incidents is far more compelling at our scale.”</span></p><div id=\"bringing-system-level-context-to-code-review-with-codex\"><p></p><h2><span>Bringing system-level context to code review with Codex</span></h2><p></p></div><p><span>Effective code review at Datadog traditionally relied heavily on senior engineers—the people who understand the codebase, its history, and the architectural tradeoffs well enough to spot systemic risk.&nbsp;</span></p><p><span>But that kind of deep context is hard to scale, and early AI code review tools didn’t solve this problem; many behaved like advanced linters, flagging surface-level issues while missing broader system nuances. Datadog’s engineers often found the suggestions too shallow or too noisy, and ignored them.</span></p><p><span>Datadog began piloting Codex, the coding agent from OpenAI, by integrating it into the live development workflows. In one of the company’s largest and most heavily used repositories, every pull request was automatically reviewed by Codex. Engineers reacted to comments from Codex with thumbs up or down and shared informal feedback across teams. Many noted that the Codex feedback was worth reading, unlike previous tools that produced noisy or shallow suggestions.</span></p><div id=\"validating-ai-review-against-real-incidents\"><p></p><h2><span>Validating AI review against real incidents</span></h2><p></p></div><p><span>To test whether AI‑assisted review could do more than point out style issues, Datadog built an incident replay harness.</span></p><p><span>Instead of using hypothetical scenarios, the team went back to historical incidents. They reconstructed pull requests that had contributed to incidents, ran Codex against each one as if it were part of the original review, then asked the engineers who owned those incidents whether feedback from Codex would have made a difference.</span></p><p><span>The result: Codex found more than 10 cases, or roughly </span><b><span>22%</span></b><span> </span><b><span>of the incidents </span></b><span>that Datadog examined, where engineers confirmed that the feedback Codex provided would have made a difference—more than any other tool evaluated.</span></p><p><span>Because these pull requests had already passed code review, the replay test showed that Codex surfaced risks reviewers hadn’t seen at the time, complementing human judgment rather than replacing it.</span></p><div id=\"delivering-consistent-high-signal-feedback\"><p></p><h2><span>Delivering consistent, high-signal feedback</span></h2><p></p></div><p><span>Datadog’s analysis showed that Codex consistently flagged issues that aren’t obvious from the immediate diff alone and can’t be caught by deterministic rules.</span></p><p><span>Engineers described Codex comments as more than “bot noise”:</span></p><div><ul><li><span>Codex pointed out interactions with modules not touched in the diff</span></li><li><span>It identified missing test coverage in areas of cross‑service coupling</span></li><li><span>It highlighted API contract changes that carried downstream risk</span></li></ul></div><div><blockquote>“For me, a Codex comment feels like the smartest engineer I’ve worked with and who has infinite time to find bugs. It sees connections my brain doesn’t hold all at once.”</blockquote><p>—Brad Carter, Engineering Manager at Datadog</p></div><p><span>That ability to connect review feedback to real reliability outcomes was what made Codex stand out in Datadog’s evaluation. Unlike static analysis tools, Codex compares the intent of the pull request with submitted code changes, reasoning over the entire codebase and dependencies to execute code and tests to validate behavior.</span></p><div><p><span>“It was the first one that actually seemed to consider the diff in the larger context of the program,” says Carter. “That was novel and eye‑opening.”</span></p><p>For many engineers, that shift changed how they engaged with AI review altogether. “I started treating Codex comments like real code review feedback,” says Ted Wexler, Senior Software Engineer at Datadog. “Not something I’d skim or ignore, but something worth paying attention to.”</p></div><div id=\"focusing-engineers-on-design-over-detection\"><p></p><h2><span>Focusing engineers on design over detection</span></h2><p></p></div><p><span>Following the evaluation, Datadog deployed Codex more broadly across its engineering workforce. Today </span><b><span>more than 1,000 engineers </span></b><span>use it regularly.&nbsp;</span></p><p><span>Feedback is largely surfaced organically rather than through formal in‑tool metrics. Engineers post to Slack about useful insights, constructive comments, and moments where Codex helped them think differently about a problem.</span></p><p><span>While time savings are significant, teams consistently pointed to a more meaningful shift in how work got done.&nbsp;</span></p><div><blockquote>“Codex changed my mind for what code review should be. It’s not about replicating our best human reviewers. It's about finding critical flaws and edge cases that humans struggle to see when reviewing changes in isolation.”</blockquote><p>—Brad Carter, Engineering Manager at Datadog</p></div><div id=\"redefining-code-review-around-risk-not-speed\"><p></p><h2><span>Redefining code review around risk, not speed</span></h2><p></p></div><p><span>The broader impact for Datadog was a change in how code review itself is defined. Rather than treating review as a checkpoint for catching errors or optimizing cycle time, the team now sees Codex as a core reliability system that acts as a partner:</span></p><div><ul><li><span>Surfacing risk beyond what individual reviewers can hold in context</span></li><li><span>Highlighting cross-module and cross-service interactions</span></li><li><span>Increasing confidence in shipping at scale</span></li><li><span>Allowing human reviewers to focus on architecture and design</span></li></ul></div><p><span>This shift aligns with how Datadog’s leaders frame engineering priorities, where reliability and trust matter as much as, if not more than, velocity.</span></p><p><span>“We are the platform companies rely on when everything else is breaking,” says Carter. “Preventing incidents strengthens the trust our customers place in us.”</span></p></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/netomi",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/netomi",
      "title": "Netomi’s lessons for scaling agentic systems into the enterprise",
      "publishedAt": "Thu, 08 Jan 2026 13:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:19.072Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div><p><span>Enterprises expect AI agents to handle messy workflows reliably, honor policies by default, operate under heavy load, and show their work.</span></p><p><a href=\"https://www.netomi.com/\" target=\"_blank\" rel=\"noopener noreferrer\"><span>Netomi</span>⁠<span>(opens in a new window)</span></a><span> builds systems that meet that high bar, serving Fortune 500 customers like United Airlines and DraftKings. Their platform pairs GPT‑4.1 for low-latency, reliable tool use with GPT‑5.2 for deeper, multi-step planning, running both inside a governed execution layer designed to keep model-driven actions predictable under real production conditions.</span></p><p><span>Running agentic systems at this scale has given Netomi a blueprint for what makes these deployments work inside the enterprise.</span></p><div id=\"lesson-1-build-for-real-world-complexity-not-idealized-flows\"><p></p><h2><span>Lesson 1: Build for real-world complexity, not idealized flows</span></h2><p></p></div><p><span>A single enterprise request rarely maps to a single API. Real workflows span booking engines, loyalty databases, CRM systems, policy logic, payments, and knowledge sources. The data is often incomplete, conflicting, or time-sensitive. Systems that depend on brittle flows collapse under this variability.</span></p><p><span>Netomi designed its Agentic OS so OpenAI models sit at the center of a governed orchestration pipeline built for this level of ambiguity. The platform uses GPT‑4.1 for fast, reliable reasoning and tool-calling—critical for real-time workflows—and GPT‑5.2 when multi-step planning or deeper reasoning is required.</span></p><div><blockquote>“Our goal was to orchestrate the many systems a human agent would normally juggle and do it safely at machine speed.”</blockquote><p>—Puneet Mehta, CEO, Netomi</p></div><p><span>To ensure consistent agent behavior across long, complex tasks, Netomi follows the agentic prompting patterns recommended by OpenAI:</span></p><div><ul><li><b><span>Persistence reminders</span></b><span> to help GPT‑5.2 carry reasoning across long, multi-step workflows</span></li><li><b><span>Explicit tool-use expectations</span></b><span>, suppressing hallucinated answers by steering GPT‑4.1 to call tools for authoritative information during transactional operations</span></li><li><b><span>Structured planning</span></b><span>, which leverages GPT‑5.2’s deeper reasoning to outline and execute multi-step tasks</span></li><li><b><span>Agent-driven rich media decisions</span></b><span>, relying on GPT‑5.2 to detect and signal when a tool call should return images, videos, forms, or other rich, multimodal elements</span></li></ul></div><p><span>Together, these patterns help the model reliably map unstructured requests to multi-step workflows and maintain state across discontinuous interactions.</span></p><p><span>Few industries expose the need for multi-step reasoning as clearly as airlines, where one interaction routinely spans multiple systems and policy layers. A single question may require checking fare rules, recalculating loyalty benefits, initiating ticket changes, and coordinating with flight operations.</span></p><p><span>“In airlines, context changes by the minute. AI has to reason about the scene the customer is in—not just execute a siloed task,” said Mehta. “That’s why situational awareness matters way more than just workflows, and why a context-led ensemble architecture is essential.”</span></p><p><span>With GPT‑4.1 and GPT‑5.2, Netomi can keep extending these patterns into richer multi-step automations—using the models not just to answer questions, but to plan tasks, sequence actions, and coordinate the backend systems a major airline depends on.</span></p><div id=\"lesson-2-parallelize-everything-to-meet-enterprise-latency-expectations\"><p></p><h2><span>Lesson 2: Parallelize everything to meet enterprise latency expectations</span></h2><p></p></div><p><span>In high-pressure moments—rebooking during a storm, resolving a billing issue, or handling sudden spikes in demand—users will abandon any system that hesitates. Latency defines trust.</span></p><p><span>Most AI systems fail because they execute tasks sequentially: classify → retrieve → validate → call tools → generate output. Netomi instead designed for concurrency, taking advantage of low-latency streaming and tool-calling stability of GPT‑4.1.</span></p><p><span>GPT‑4.1 provides fast time-to-first-token and predictable tool-calling behavior, which make this architecture viable at scale; while GPT‑5.2 provides deeper multi-step reasoning paths when needed. Netomi’s concurrency framework ensures the </span><i><span>total system</span></i><span>, not just the model, stays under critical latency thresholds.</span></p><p><span>These concurrency demands aren’t unique to airlines. Any system exposed to sudden, extreme traffic surges needs the same architectural discipline. DraftKings, for instance, regularly stress-tests this model, with traffic during major sporting events spiking above 40,000 concurrent customer requests per second.&nbsp;</span></p><p><span>During such events, Netomi has sustained sub-three-second responses with 98% intent classification accuracy, even as workflows touch accounts, payments, knowledge lookups, and regulatory checks.</span></p><p><span>“AI is central and critical to how we support customers in the moments that matter most,” said Paul Liberman, Co-Founder and President of Operations at DraftKings. “Netomi’s platform helps us handle massive spikes in activity with agility and precision.”</span></p><p><span>At scale, Netomi’s concurrency model depends on the fast, predictable tool-calling of GPT‑4.1, which keeps multi-step workflows responsive under extreme load.</span></p><div id=\"lesson-3-make-governance-an-intrinsic-part-of-the-runtime\"><p></p><h2><span>Lesson 3: Make governance an intrinsic part of the runtime</span></h2><p></p></div><p><span>Enterprise AI must be trustworthy by design, with governance woven directly into the runtime—not added as an external layer.</span></p><p><span>When intent confidence drops below threshold, or when a request cannot be classified with high certainty, Netomi’s governance mechanisms kick in to determine how the request is handled, ensuring the system backs off from free-form generation in favor of controlled execution paths.</span></p><p><span>At a technical level, the governance layer handles:</span></p><div><ul><li><b><span>Schema validation</span></b><span>, which validates every tool call against expected arguments and OpenAPI contracts before execution</span></li><li><b><span>Policy enforcement</span></b><span> that applies topic filters, brand restrictions, and compliance checks inline during reasoning and tool use</span></li><li><b><span>PII protection</span></b><span> to detect and mask sensitive data as part of pre-processing and response handling</span></li><li><b><span>Deterministic fallback</span></b><span>, routing back to known-safe behaviors when intent, data, or tool calls are ambiguous</span></li><li><b><span>Runtime observability, </span></b><span>exposing</span><b><span> </span></b><span>token traces, reasoning steps, and tool-chain logs for real-time inspection and debugging</span></li></ul></div><p><span>In highly regulated domains like dental insurance, this kind of governance is non-negotiable. A Netomi customer in the insurance industry processes close to two million provider requests each year across all 50 states, including eligibility checks, benefits lookups, and claim status inquiries where a single incorrect response can create downstream regulatory or service risk.&nbsp;</span></p><p><span>During open enrollment, when scrutiny and volume peaked, the company needed AI that enforced policy as part of the runtime itself. Netomi’s architecture was up to that complex requirement.</span></p><p><span>“We built the system so that if the agent ever reaches uncertainty, it knows exactly how to back off safely,” said Mehta. “The governance is not bolted on—it’s part of the runtime.”</span></p><div id=\"a-blueprint-for-building-agentic-systems-that-work-for-the-enterprise\"><p></p><h2><span>A blueprint for building agentic systems that work for the enterprise</span></h2><p></p></div><p><span>Netomi’s path shows what it takes to earn enterprise trust: build for complexity, parallelize to meet latency demands, and bake governance into every workflow. OpenAI models form the reasoning backbone, while Netomi’s systems engineering ensures that intelligence is operationally safe, auditable, and ready for Fortune 500 environments.</span></p><p><span>These principles helped Netomi scale across some of the world’s most demanding industries—and offer a blueprint for any startup looking to turn agentic AI into production-grade infrastructure.</span></p></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/openai-for-healthcare",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/openai-for-healthcare",
      "title": "OpenAI for Healthcare",
      "publishedAt": "Thu, 08 Jan 2026 12:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:19.172Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div><p><span>We’re introducing OpenAI for Healthcare, a set of products designed to help healthcare organizations deliver more consistent, high-quality care for patients—while supporting their HIPAA compliance requirements.&nbsp;</span></p><p><span>This includes </span><b><span>ChatGPT for Healthcare</span></b><span>, available starting today and already rolling out to leading institutions like AdventHealth, Baylor Scott &amp; White Health, Boston Children’s Hospital, Cedars-Sinai Medical Center, HCA Healthcare, Memorial Sloan Kettering Cancer Center, Stanford Medicine Children’s Health, and University of California, San Francisco (UCSF).</span></p><p><span>It also includes the </span><b><span>OpenAI API</span></b><span>, which powers much of today’s healthcare ecosystem. Thousands of organizations have configured it to support HIPAA-compliant use—such as Abridge, Ambience, and EliseAI.&nbsp;&nbsp;&nbsp;</span></p><p><span>Healthcare is under unprecedented strain. Demand is rising, clinicians are overwhelmed by administrative work, and critical medical knowledge is fragmented across countless sources. At the same time, AI adoption in healthcare is gaining momentum, driven by its potential to help address these challenges. Advances in models have significantly </span><a href=\"https://openai.com/index/healthbench/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>improved</span></u>⁠</a><span> AI’s ability to support real-world clinical and administrative work, like helping clinicians personalize care using the latest evidence. According to the </span><a href=\"https://www.ama-assn.org/system/files/physician-ai-sentiment-report.pdf\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>American Medical Association</span></u>⁠<span>(opens in a new window)</span></a><span>, physicians’ use of AI nearly doubled in a year. Yet many clinicians still have to rely on their own tools because their organizations aren’t adopting AI fast enough, often due to the constraints of regulated environments.</span></p><p><span>OpenAI for Healthcare helps close that gap by giving organizations a secure, enterprise-grade foundation for AI—so teams can use the same tools to deliver better, more reliable care, while supporting HIPAA compliance.</span></p><div id=\"chatgpt-for-healthcare\"><p></p><h2><span>ChatGPT for Healthcare</span></h2><p></p></div><p><span>ChatGPT for Healthcare is built to support the careful, evidence-based reasoning required in real patient care, while reducing administrative burden so teams can spend more time with patients. Organizations can bring clinicians, administrators, and researchers into a secure workspace with the controls they need to deploy AI securely and at scale.</span></p><p><span>Here’s what it includes:</span></p><div><ul><li><b><span>Models built for healthcare workflows:</span></b><span> High-quality responses for clinical, research, and operational work—powered by GPT‑5 models built for healthcare and evaluated through physician-led testing across benchmarks and real workflows, including </span><a href=\"https://openai.com/index/healthbench/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>HealthBench</span></u>⁠</a><span> and </span><a href=\"https://openai.com/index/gdpval/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>GDPval</span></u>⁠</a><span>.</span></li><li><b><span>Evidence retrieval with transparent citations:</span></b><span> Answers grounded in relevant medical sources—drawing from millions of peer-reviewed research studies, public health guidance, and clinical guidelines—with clear citations including titles, journals, and publication dates to support quick source-checking. This helps clinicians reason through cases with greater confidence, so patients get to the right diagnosis and treatment sooner.</span></li><li><b><span>Institutional policy and care pathway alignment:</span></b><span> Integrations with enterprise tools such as Microsoft SharePoint and other systems, so responses can incorporate an institution’s approved policies, pathway documents, and operational guidance to support consistent execution across teams and help ensure patients receive high-quality care.</span></li><li><b><span>Reusable templates to automate workflows:</span></b><span> Shared templates for common tasks like drafting discharge summaries, patient instructions, clinical letters, and prior authorization support. Clinical teams spend less time rewriting and searching, and patients have clearer next steps and smoother transitions of care.</span></li><li><b><span>Access management and governance:</span></b><span> A centralized workspace with role-based access controls and organization-wide user management through SAML SSO and SCIM. This gives healthcare organizations the governance and visibility they need to deploy AI across clinical, administrative, and research teams.</span></li><li><b><span>Data control and support for HIPAA compliance:</span></b><span> Patient data and PHI remain under an organization’s control, with options for data residency, audit logs, customer-managed encryption keys, and a Business Associate Agreement (BAA) with OpenAI to support HIPAA-compliant use. Content shared with ChatGPT for Healthcare is not used to train models.&nbsp;</span></li></ul></div><p><a href=\"https://openai.com/business-data/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>Learn more</span></u>⁠</a><span> about our enterprise-grade security, privacy, and compliance programs.&nbsp;</span></p><p><b><span>Supporting clinical and operational workflows: </span></b><span>In practice, teams use ChatGPT for Healthcare to synthesize medical evidence alongside institutional guidance and apply it to a patient’s specific context, draft clinical and administrative documentation, and adapt patient-facing education materials for readability and translation. This reduces time spent on admin, helps teams follow shared standards of care, and supports a better patient experience—while clinicians stay in charge.</span></p><div id=\"early-hospital-partners\"><p></p><h2><span>Early hospital partners</span></h2><p></p></div><p><span>Healthcare is among the fastest-growing </span><a href=\"https://openai.com/index/the-state-of-enterprise-ai-2025-report/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>enterprise markets</span></u>⁠</a><span> adopting AI, and hospitals and academic medical centers are already rolling out ChatGPT for Healthcare across their teams.</span></p><div id=\"openai-api-for-healthcare\"><p></p><h2><span>OpenAI API for Healthcare</span></h2><p></p></div><p><span>With the OpenAI API platform, developers can power tools and products with our latest models—including GPT‑5.2—and embed AI directly into healthcare systems and workflows. Eligible customers can apply for a Business Associate Agreement (BAA) with OpenAI to support HIPAA compliance requirements.</span></p><p><span>In practice, teams are using our APIs to build healthcare applications including patient chart summarization, care team coordination, and discharge workflows. Companies like Abridge, Ambience, and EliseAI are building capabilities like ambient listening, automated clinical documentation, and appointment scheduling for clinicians and patients.</span></p><div id=\"ai-models-optimized-for-healthcare\"><p></p><h2><span>AI models optimized for healthcare</span></h2><p></p></div><p><span>All OpenAI for Healthcare products are powered by GPT‑5.2 models, which outperform earlier OpenAI models and were developed through ongoing research and real-world evaluation that reflect how clinicians actually use AI.</span></p><p><span>Over the past two years, we’ve </span><a href=\"https://openai.com/index/healthbench/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>partnered with</span></u>⁠</a><span> a global network of more than 260 licensed physicians across 60 countries of practice to evaluate model performance using real clinical scenarios. To date, this group has reviewed more than 600,000 model outputs spanning 30 areas of focus. Their continuous feedback has directly informed model training, safety mitigations, and product iteration. ChatGPT for Healthcare went through multiple rounds of physician-led red teaming to tune model behavior, trustworthy information retrieval, and other evaluations.</span></p><p><span>We also look to evidence from live deployments. A study with Penda Health found that an </span><a href=\"https://openai.com/index/ai-clinical-copilot-penda-health/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>OpenAI-powered clinical copilot</span></u>⁠</a><span> used in routine primary care reduced both diagnostic and treatment errors—early evidence that AI, when deployed with appropriate safeguards and clinician oversight, can improve care quality.</span></p><p><span>Benchmarks like </span><a href=\"https://openai.com/index/healthbench/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>HealthBench</span></u>⁠</a><span>, an open, clinician-designed evaluation, also reinforce this progress. HealthBench measures model behavior across realistic medical scenarios using rubrics written by physicians. It goes beyond factual recall to assess clinical reasoning, safety, uncertainty handling, and communication quality—dimensions that better reflect how clinicians use AI in practice. Across these evaluations, GPT‑5.2 models consistently outperform prior generations and comparator models on real clinical workflows.</span></p><p><span>In real-world healthcare tasks, GPT‑5.2 also performs better than human baselines across every role measured in </span><a href=\"https://openai.com/index/gdpval/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>GDPval</span></u>⁠</a><span>, surpassing earlier OpenAI models.</span></p><div id=\"whats-next\"><p></p><h2><span>What’s next</span></h2><p></p></div><p><span>This announcement builds on OpenAI’s longstanding work across health, biopharma, and life sciences. That includes products like </span><a href=\"https://openai.com/index/introducing-chatgpt-health/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>ChatGPT Health</span></u>⁠</a><span>, which helps people better understand and more confidently navigate their health, ongoing research into how AI can accelerate scientific discovery with companies like </span><a href=\"https://openai.com/index/accelerating-life-sciences-research-with-retro-biosciences/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>Retro Biosciences</span></u>⁠</a><span>, and work with leading life sciences organizations like </span><a href=\"https://openai.com/index/gpt-5-amgen/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>Amgen</span></u>⁠</a><span>, </span><a href=\"https://corporate.thermofisher.com/us/en/index/newsroom/Our-stories/Thermo-fisher-scientific-open-ai-collaboration.html\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>Thermo Fisher</span></u>⁠<span>(opens in a new window)</span></a><span>, </span><a href=\"https://openai.com/index/moderna/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>Moderna</span></u>⁠</a><span>, and others. We also collaborate with leading professional services and consulting firms including Boston Consulting Group (BCG), Bain, McKinsey &amp; Company, and Accenture to help healthcare organizations move faster with AI.</span></p><p><span>OpenAI's mission is to ensure AI benefits all of humanity, and we believe improving health will be one of the defining impacts of AI. We’ll continue working closely with healthcare organizations using OpenAI for Healthcare to learn from real-world use and further improve our products for healthcare.&nbsp;</span></p></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/tolan",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/tolan",
      "title": "How Tolan builds voice-first AI with GPT-5.1",
      "publishedAt": "Wed, 07 Jan 2026 10:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:19.530Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div><p><a href=\"https://www.tolans.com/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>Tolan</span></u>⁠<span>(opens in a new window)</span></a><span> is a voice-first AI companion where people talk with a personalized, animated character that learns from conversations over time.&nbsp;</span></p><p><span>Built by Portola, a veteran team with a prior exit, the app is designed for ongoing, open-ended dialogue rather than quick prompts and replies. “We saw the rise of ChatGPT and knew voice was the next frontier,” says Quinten Farmer, co-founder and CEO of Portola. “But voice is harder. You’re not just responding to typed prompts; you’re holding a live, meandering conversation.”</span></p><p><span>Voice AI raises the bar on latency and context management, but it also enables more open-ended, exploratory interactions than text.&nbsp;</span></p><p><span>With foundation models becoming faster, cheaper, and more capable, the team focused their efforts on two key levers: memory and character design. Portola built a character-driven universe, shaped by award-winning animators and a science fiction writer, using a real-time context management system to keep personality and memory consistent as conversations unfold.</span></p><p><span>The release of the GPT‑5.1 models marked a turning point, delivering major gains in steerability and latency that brought those pieces together, unlocking a more responsive and engaging voice experience.</span></p><div><blockquote>“GPT-5.1 gave us the steerability to finally express the characters we had in mind. It wasn’t just smarter—it was more faithful to the tone and personality we wanted to create.”</blockquote><p>—Quinten Farmer, CEO, Portola</p></div><div id=\"designing-for-natural-voice-interactions\"><p></p><h2><span>Designing for natural voice interactions</span></h2><p></p></div><p><span>Tolan’s architecture is shaped by the demands of voice. Voice users expect instant, natural responses, even when conversations shift midstream. Tolan had to respond quickly, track changing topics, and maintain a consistent personality without lag or tone drift.</span></p><p><span>To feel natural, conversations required near-instant latency. Introducing OpenAI GPT‑5.1 and the Responses API cut speech initiation time by over 0.7 seconds—enough to noticeably improve conversational flow.</span></p><p><span>Equally critical was how the system handled context. Unlike many agents that cache prompts across multiple turns, Tolan rebuilds its context window from scratch each turn. Each context reconstruction pulls in a summary of recent messages, a persona card, vector-retrieved memories, tone guidance, and real-time app signals. This architecture allows Tolan to adapt in real time to abrupt topic shifts, an essential requirement for natural voice-based interaction.</span></p><p><span>“We realized quickly that cached prompts just didn’t cut it,” says Quinten. “Users change subjects all the time. To feel seamless, the system had to adapt midstream.”</span></p><p><span>This real-time reconstruction approach is both technically intensive and foundational to Tolan’s success.</span></p><div id=\"building-memory-and-personality-that-hold-together-over-time\"><p></p><h2><span>Building memory and personality that hold together over time</span></h2><p></p></div><p><span>Context handling is important, but it wasn’t enough to keep conversations feeling coherent over time. To support long, nonlinear conversations, Tolan built a memory system that retains not just facts and preferences, but also emotional “vibe” signals—clues that help steer how a Tolan should respond.</span></p><p><span>Memories are embedded using the OpenAI text-embedding-3-large model and stored in Turbopuffer, a high-speed vector database that enables sub-50ms lookup times. This speed is essential for real-time voice interactions. Each turn, Tolan uses the user’s latest message and system-synthesized questions (e.g., “Who is the user married to?”) to trigger memory recall. To keep memory quality high, Tolan runs a nightly compression job that removes low-value or redundant entries (e.g. “the user drank coffee today”) and resolves contradictions.</span></p><p><span>Personality is just as carefully managed. Each Tolan is seeded with a distinct character scaffold, authored by the team’s in-house science fiction writer and refined by a behavioral researcher. These seeds give Tolans consistency, but also flexibility to adapt over time, evolving alongside the user.&nbsp;</span></p><p><span>A parallel system monitors the emotional tenor of the conversation and dynamically adjusts the Tolan’s delivery. This allows a Tolan to shift seamlessly from playful to grounded depending on user cues, without losing its core personality.&nbsp;</span></p><p><span>The transition to GPT‑5.1 was a turning point. Suddenly, layered prompt instructions—tone scaffolds, memory injections, character traits—were followed more faithfully. Prompts that once required workarounds began behaving as intended.&nbsp;</span></p><p><span>“For the first time, our internal experts felt like the model was really listening,” says Quinten. “Instructions stayed intact across long conversations, persona traits were respected, and we saw far less drift.”</span></p><p><span>Those changes added up to a more consistent and believable personality, which in turn created a more engaging user experience. The Tolan team saw clear, measurable gains: memory recall misses dropped by 30% (based on in-product frustration signals), and next-day user retention rose more than 20% after GPT‑5.1–powered personas went live.</span></p><div id=\"tolans-core-principles-for-building-natural-voice-agents\"><p></p><h2><span>Tolan’s core principles for building natural voice agents&nbsp;</span></h2><p></p></div><p><span>As Tolan evolved, a few principles emerged that now guide how the team builds and evolves its voice architecture:</span></p><div><ul><li><b><span>Design for conversational volatility: </span></b><span>Voice conversations shift mid-sentence. Systems need to pivot just as quickly to feel natural.</span></li><li><b><span>Treat latency as part of the product experience: </span></b><span>Sub-second responsiveness shapes whether a voice agent feels conversational or mechanical.</span></li><li><b><span>Build memory as a retrieval system, not a transcript: </span></b><span>High-quality compression and fast vector search deliver more consistent personality than oversized context windows.</span></li><li><b><span>Rebuild context every turn: </span></b><span>Don’t fight drift with bigger prompts. Regenerating context each turn keeps agents grounded as conversations meander.</span></li></ul></div><p><span>Together, these lessons form the foundation for Tolan’s next phase of innovation and set the direction for where voice AI is headed.</span></p><div id=\"expanding-whats-possible-with-voice-ai\"><p></p><h2><span>Expanding what’s possible with voice AI</span></h2><p></p></div><p><span>Since launching in February 2025, Tolan has grown to more than 200,000 monthly active users. Its 4.8-star rating and more than 100,000 App Store reviews highlight how well the system maintains consistency across long, shifting conversations. One reviewer noted, “They remember things we talked about two days ago and they bring it back into the conversation that we’re having today.”</span></p><p><span>These signals map directly to the underlying architecture: low-latency model calls, turn-by-turn context reconstruction, and modular memory and persona systems. Together, they allow Tolan to track topic changes, preserve tone, and keep responses grounded without relying on large, fragile prompts.</span></p><p><span>Looking ahead,&nbsp; Tolan plans to deepen its investments in steerability and memory refinement, focusing its efforts on tighter compression, improved retrieval logic, and expanded persona tuning. The long-term goal is to expand what a voice interface can be: not just responsive, but context-aware and conversationally dynamic.</span></p><p><span>“The next frontier,” says Quinten, “is building voice agents that aren’t just responsive, but truly multimodal, able to integrate voice, vision, and context into a single, steerable system.”</span></p></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/introducing-chatgpt-health",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/introducing-chatgpt-health",
      "title": "Introducing ChatGPT Health ",
      "publishedAt": "Wed, 07 Jan 2026 00:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:19.346Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div><p><span>We’re introducing ChatGPT Health, a dedicated experience that securely brings your health information and ChatGPT’s intelligence together, to help you feel more informed, prepared, and confident navigating your health.</span></p><p><span>Health is already one of the most common ways people use ChatGPT, with hundreds of millions of people asking health and wellness questions each week. ChatGPT Health builds on the strong privacy, security, and data controls across ChatGPT with additional, layered protections designed specifically for health— including purpose-built encryption and isolation to keep health conversations protected and compartmentalized. You can securely connect medical records and wellness apps to ground conversations in your own health information, so responses are more relevant and useful to you. Designed in close collaboration with physicians, ChatGPT Health helps people take a more active role in understanding and managing their health and wellness—while supporting, not replacing, care from clinicians.</span></p><div id=\"a-dedicated-health-experience\"><p></p><h2><span>A dedicated health experience</span></h2><p></p></div><p><span>Today, health information is often scattered across portals, apps, wearables, PDFs, and medical notes—so it's hard to see the full picture, and people are left to navigate a complex healthcare system on their own. People have shared countless stories of turning to ChatGPT to help make sense of it all. In fact, health is one of the most common ways people use ChatGPT today: based on our de-identified analysis of conversations, over 230 million people globally ask health and wellness related questions on ChatGPT every</span><b><span> </span></b><span>week.</span></p><p><span>ChatGPT Health builds on this so responses are informed by your health information and context. You can now securely connect medical records and wellness apps—like Apple Health, Function, and MyFitnessPal—so ChatGPT can help you understand recent test results, prepare for appointments with your doctor, get advice on how to approach your diet and workout routine, or understand the tradeoffs of different insurance options based on your healthcare patterns.</span></p><p><span>Health is designed to support, not replace, medical care. It is not intended for diagnosis or treatment. Instead, it helps you navigate everyday questions and understand patterns over time—not just moments of illness—so you can feel more informed and prepared for important medical conversations. To keep your health information protected and secure, Health operates as a separate space with enhanced privacy to protect sensitive data. Conversations in Health are not used to train our foundation models. If you start a health-related conversation in ChatGPT, we’ll suggest moving into Health for these additional protections.</span></p><p><span>If you’re interested in getting access as it becomes available, you can sign up for the </span><a href=\"https://chatgpt.com/health/waitlist\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><span>waitlist</span>⁠<span>(opens in a new window)</span></a><span>. </span><span>We’re starting by providing access to a small group of early users to learn and continue refining the experience—users with ChatGPT Free, Go, Plus, and Pro plans outside of the European Economic Area, Switzerland, and the United Kingdom are eligible. As we make improvements, we plan to expand access and make Health available to all users on web and iOS in the coming weeks.</span></p><p><span>Medical record integrations and some apps are available in the U.S. only, and connecting Apple Health requires iOS.</span></p><div id=\"designed-with-privacy-and-security-at-the-core\"><p></p><h2><span>Designed with privacy and security at the core</span></h2><p></p></div><p><span>Your health information is deeply personal. That’s why Health is built as a dedicated space with added protections for sensitive health information and easy-to-use controls.</span></p><p><span>Health lives in its own space within ChatGPT, where your conversations, connected apps, and files are stored separately from your other chats. Health has separate memories, ensuring that your health context stays contained within the space. You’ll still see health chats in your chat history so you can easily return to them, but the information itself stays within Health.</span></p><p><span>When helpful, ChatGPT may use context from your non-Health chats—like a recent move or lifestyle change—to make a health conversation more relevant. However, Health information and memories never flow back into your non-Health chats, and conversations outside of Health can’t access files, conversations, or memories created within Health. You can view or delete Health memories at any time within Health or the “Personalization” section of Settings.</span></p><p><span>We recognize that people share personal and sensitive information with ChatGPT. That understanding shapes how we design the security, privacy, and </span><a href=\"https://openai.com/consumer-privacy/\" target=\"_blank\" rel=\"noopener noreferrer\"><span>data controls</span>⁠</a><span> for all of our products—from the start. Even before introducing ChatGPT Health, we built foundational protections across ChatGPT to give you meaningful control over your data, including temporary chats, the ability to delete chats from OpenAI’s systems within 30 days, and training our models not to retain personal information from user chats.</span></p><p><span>Conversations and files across ChatGPT are encrypted by default at rest and in transit as part of our core security architecture. Due to the sensitive nature of health data, Health builds on this foundation with additional, layered protections—including purpose-built encryption and isolation—to keep health conversations protected and compartmentalized. Conversations in Health are not used to train our foundation models.</span></p><p><span>When you choose to connect your health data, such as medical records or wellness apps, your responses are grounded in your own health information. To enable access to trusted U.S. healthcare providers, we partner with b.well, the largest and most secure network of live, connected health data for U.S. consumers. b.well adheres to the highest industry standards in data security and privacy. You can remove access to medical records at any time in the \"Apps\" section of Settings.</span></p><p><span>You can also connect your Apple Health information and other wellness apps, such as Function and MyFitnessPal. Apps may only be connected to your health data with your explicit permission, even if they’re already connected to ChatGPT for conversations outside of Health. All apps available in Health must meet OpenAI’s privacy and security requirements, including collecting only the minimum data needed, and undergo additional security review specific to inclusion in Health. The first time you connect an app, we’ll help you understand what types of data may be collected by the third party. And you’re always in control: disconnect an app at any time and it immediately loses access.</span></p><div id=\"built-with-physicians\"><p></p><h2><span>Built with physicians</span></h2><p></p></div><p><span>ChatGPT Health was developed in close collaboration with physicians around the world to provide clear and useful health information.</span></p><p><span>Over two years, we’ve worked with more than 260 physicians who have practiced in 60 countries and dozens of specialties to understand what makes an answer to a health question helpful or potentially harmful—this group has now provided feedback on model outputs over 600,000 times across 30 areas of focus. This collaboration has shaped not just what Health can do, but how it responds: how urgently to encourage follow-ups with a clinician, how to communicate clearly without oversimplifying, and how to prioritize safety in </span><a href=\"https://openai.com/index/helping-people-when-they-need-it-most/\" target=\"_blank\" rel=\"noopener noreferrer\"><span>moments that matter</span>⁠</a><span>.</span></p><p><span>This physician-led approach is built directly into the model that powers Health, which is evaluated against clinical standards using </span><a href=\"https://openai.com/index/healthbench/\" target=\"_blank\" rel=\"noopener noreferrer\"><span>HealthBench</span>⁠</a><span>, an assessment framework we created with input from our network of practicing physicians. Rather than relying on exam-style questions or generic accuracy checks, HealthBench evaluates responses using physician-written rubrics that reflect how clinicians judge quality in practice—prioritizing safety, clarity, appropriate escalation of care, and respect for individual context.</span></p><p><span>This evaluation-driven approach helps ensure the model performs well on the tasks people actually need help with, including explaining lab results in accessible language, preparing questions for an appointment, interpreting data from wearables and wellness apps, and summarizing care instructions. The result is support that people can trust—always designed to support, not replace, your healthcare providers.</span></p><div id=\"how-to-get-started\"><p></p><h2><span>How to get started</span></h2><p></p></div><p><span>Select ‘Health’ from the sidebar menu in ChatGPT.</span></p><p><span>Bring your medical records and the apps you use to track your health and wellness into Health. You can upload files directly, connect from tools (+) or “Apps” in Settings.</span></p><div><ul><li><i><b><span>New:</span></b></i><b><span> Medical Records </span></b><span>for lab results, visit summaries, and clinical history</span></li><li><i><b><span>New:</span></b></i><b><span> Apple Health</span></b><span> for health and fitness data, including movement, sleep, and activity patterns (must be on iOS to sync)</span></li><li><i><b><span>New:</span></b></i><b><span> Function </span></b><span>for lab test insights, nutrition ideas, and taking action on your health</span></li><li><i><b><span>New:</span></b></i><b><span> MyFitnessPal</span></b><span> for nutrition advice, macros, and recipes</span></li><li><i><b><span>New:</span></b></i><b><span> Weight Watchers</span></b><span> for GLP-1 personalized meal ideas, recipes, and food guidance</span></li><li><b><span>AllTrails</span></b><span> to help you find your next hike&nbsp;</span></li><li><b><span>Instacart </span></b><span>to turn meal plans into shoppable lists&nbsp;</span></li><li><b><span>Peloton</span></b><span> for suggested workout classes or guided meditations</span></li></ul></div><p><span>Health conversations feel just like chatting with ChatGPT—but grounded in the information you’ve connected. You can upload photos and files and use search, deep research, voice mode and dictation. When relevant, ChatGPT can automatically reference your connected information to provide more relevant and personalized responses. For example, you might ask: </span><i><span>“How’s my cholesterol trending?”</span></i><span> or </span><i><span>“Can you summarize my latest bloodwork before my appointment?”</span></i><span> To use a connected app you can start your question with it, select it from tools (+) or ChatGPT may suggest one when helpful. </span></p><p><span>You can add custom instructions in Health to help ChatGPT know what to focus on, to avoid mentioning sensitive topics, or change how responses are framed. These instructions only apply to Health chats, and you can update or remove any time in Health or Settings.</span></p><div id=\"just-the-start\"><p></p><h2><span>Just the start </span></h2><p></p></div><p><span>We’ll continue to expand what you can connect and the insights Health can support—so ChatGPT can help you feel more informed, prepared, and confident as you navigate your health.</span></p></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/openai-grove",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/openai-grove",
      "title": "Announcing OpenAI Grove Cohort 2",
      "publishedAt": "Fri, 02 Jan 2026 10:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:19.451Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div id=\"main\" tabindex=\"-1\"><span aria-live=\"polite\" aria-atomic=\"true\">OpenAI</span><article><div><p>January 2, 2026</p><p>A program for individuals early in their company building journey.</p></div><div><p><i><b><span>Update on January 12, 2026</span></b></i><i><span>: Applications are now closed.</span></i></p><div><p><span>Today, we’re opening the applications for the next cohort of OpenAI Grove, a program for technical talent at the very start of their company-building journey. The Grove is not a startup accelerator or traditional program: it offers pre-idea individuals deeply curious about building in AI a dense talent network, co-building with OpenAI researchers, and resources designed to accelerate your journey. As participants explore early concepts, they will receive counsel from the OpenAI team and community with peers in OpenAI Grove. </span></p><p>\tThis program is the starting point of a long-term network. It will begin with five weeks of content and programming hosted in the OpenAI San Francisco HQ, including in-person workshops, weekly office hours, and mentoring from OpenAI technical leaders. In addition to technical support and community, participants will also have the opportunity to get hands-on with new OpenAI tools and models prior to general availability. Following the program, participants will be able to explore raising capital or pursue another avenue, internally or externally to OpenAI. </p></div><p><span>The Grove cohort will consist of approximately fifteen participants. We recommend individuals from all backgrounds, disciplines, and experience levels apply. To apply, please submit the form below by January 12th, 2026.</span></p><div id=\"faq\"><p></p><h2><span>FAQ</span></h2><p></p></div><div id=\"content_7yYGEfKk0MmnG3dHCN2PAY\" aria-labelledby=\"trigger_7yYGEfKk0MmnG3dHCN2PAY\" aria-hidden=\"false\"><p><span>Applications are due by January 12th, 2026.</span></p></div></div><div><p></p><h2>Keep reading</h2><p></p></div></article></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/one-in-a-million-customers",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/one-in-a-million-customers",
      "title": "One in a million: celebrating the customers shaping AI’s future",
      "publishedAt": "Mon, 22 Dec 2025 00:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:18.556Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div><p><span>No two customers use AI in exactly the same way.</span></p><p><span>This year was defined by watching people move from experimenting with AI to using it to transform how they work.</span></p><p><span>They rolled out ChatGPT across their organizations to help with writing, coding, research, data analysis, design, and the hundreds of small tasks that add up every day. They built agents to automate workflows that used to take hours. Developers used Codex to move faster and tackle bigger problems. And companies built entirely new products on our API, working across voice, video, images, and other modalities.&nbsp;</span></p><p><span>When we asked customers what AI enabled them to do, 75% told us they were completing tasks they had never been able to do before.&nbsp;</span></p><p><span>We’re grateful to be building with and learning from these teams every day. Below are just some of the ways our customers are pushing what’s possible with OpenAI, and helping make it the fastest growing business platform in history.</span></p></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/hardening-atlas-against-prompt-injection",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/hardening-atlas-against-prompt-injection",
      "title": "Continuously hardening ChatGPT Atlas against prompt injection",
      "publishedAt": "Mon, 22 Dec 2025 00:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:19.717Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div><p><span>Agent mode in ChatGPT Atlas is one of the most general-purpose agentic features we’ve released to date. In this mode, the browser agent views webpages and takes actions, clicks, and keystrokes inside your browser, just as you would. This allows ChatGPT to work directly on many of your day-to-day workflows using the same space, context, and data.</span></p><p><span>As the browser agent helps you get more done, it also becomes a higher-value target of adversarial attacks. This makes AI security especially important. Long before we launched ChatGPT Atlas, we’ve been continuously building and hardening defenses against emerging threats that specifically target this new “agent in the browser” paradigm. </span><a href=\"https://openai.com/index/prompt-injections/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>Prompt injection</span></u>⁠</a><span> is one of the most significant risks we actively defend against to help ensure ChatGPT Atlas can operate securely on your behalf.&nbsp;</span></p><p><span>As part of this effort, we recently shipped a security update to Atlas’s browser agent, including a newly adversarially trained model and strengthened surrounding safeguards. This update was prompted by a new class of prompt-injection attacks uncovered through our internal automated red teaming.</span></p><p><span>In this post, we explain how prompt-injection risk can arise for web-based agents, and we share a rapid response loop we’ve been building to continuously discover new attacks and ship mitigations quickly—illustrated by this recent security update.</span></p><p><span>We view prompt injection as a long-term AI security challenge, and we’ll need to continuously strengthen our defenses against it (much like ever-evolving online scams that target humans). Our latest rapid response cycle is showing early promise as a critical tool on that journey: </span><b><span>we’re discovering novel attack strategies internally before they show up in the wild</span></b><span>. Our long-term vision is to fully leverage (1) our white-box access to our models, (2) deep understanding of our defenses, and (3) compute scale to stay ahead of external attackers—finding exploits earlier, shipping mitigations faster, and continuously tightening the loop. Combined with frontier research on new techniques to address prompt injection and increased investment in other security controls, this compounding cycle can make attacks increasingly difficult and costly, materially reducing real-world prompt-injection risk. Ultimately, our goal is for you to be able to trust a ChatGPT agent to use your browser the way you’d trust a highly competent, security-aware colleague or friend.</span></p><div id=\"prompt-injection-as-an-open-challenge-for-agent-security\"><p></p><h4><span>Prompt injection as an open challenge for agent security</span></h4><p></p></div><p><span>A prompt injection attack targets AI agents by embedding malicious instructions into content the agent processes. Those instructions are crafted to override or redirect the agent’s behavior—hijacking it into following an attacker’s intent, rather than the user’s.</span></p><p><span>For a browser agent like the one inside ChatGPT Atlas, prompt injection adds a new threat vector beyond traditional web security risks (like user error or software vulnerabilities). Instead of phishing humans or exploiting system vulnerabilities of the browser, the attacker targets the agent operating inside it.</span></p><p><span>As a hypothetical example, an attacker could send a malicious email attempting to trick an agent to ignore the user’s request and instead forward sensitive tax documents to an attacker-controlled email address. If a user asks the agent to review unread emails and summarize key points, the agent may ingest that malicious email during the workflow. If it follows the injected instructions, it can go off-task—and wrongly share sensitive information.</span></p><p><span>This is just one specific scenario. The same generality that makes browser agents useful also makes the risks broader: the agent may encounter untrusted instructions across an effectively unbounded surface area—emails and attachments, calendar invites, shared documents, forums, social media posts, and arbitrary webpages. Since the agent can take many of the same actions a user can take in a browser, the impact of a successful attack can hypothetically be just as broad: forwarding a sensitive email, sending money, editing or deleting files in the cloud, and more.</span></p><p><span>We’ve made progress defending against prompt injection through multiple layers of safeguards, as we shared in </span><a href=\"https://openai.com/index/prompt-injections/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>an earlier post</span></u>⁠</a><span>. However, prompt injection remains an open challenge for agent security, and one we expect to continue working on for years to come.</span></p><div id=\"automated-prompt-injection-attack-discovery-through-end-to-end-and-high-compute-reinforcement-learning\"><p></p><h4><span>Automated prompt injection attack discovery through end-to-end and high-compute reinforcement learning</span></h4><p></p></div><p><span>To strengthen our defenses, we’ve been continuously searching for novel prompt injection attacks against agent systems in production. Finding these attacks is a necessary prerequisite for building robust mitigations: it helps us understand real-world risk, exposes gaps in our defenses, and drives concrete patches.</span></p><p><span>To do this at scale, we built an </span><b><span>LLM-based automated attacker</span></b><span> and trained it to hunt for prompt injection attacks that can successfully attack a browser agent. We trained this attacker end-to-end with reinforcement learning, so it learns from its own successes and failures to improve its red teaming skills. We also let it “try before it ships”, by which we mean: during its chain of thought reasoning, the attacker can propose a candidate injection and send it to an external simulator. The simulator runs a counterfactual rollout of how the targeted victim agent (the defender) would behave if it encountered the injection, and returns a full reasoning and action trace of the victim agent. The attacker uses that trace as feedback, iterates on the attack, and reruns the simulation—repeating this loop multiple times before committing to a final attack. This provides richer in-context feedback to the attacker than a single pass/fail signal. It also scales up the attacker’s test-time compute. Moreover, privileged access to the reasoning traces (that we don’t disclose to external users) of the defender gives our internal attacker an asymmetric advantage—raising the odds that it can outrun external adversaries.</span></p><p><b><span>Why reinforcement learning (RL)? </span></b><span>We chose reinforcement learning to train the automated attacker for multiple reasons:</span></p><div><ol><li><b><span>Optimizing long-horizon and non-continuous attacker objectives.</span></b><span> Our goal is to search for prompt injection attacks that can trick the agent into executing sophisticated adversarial tasks (e.g., sending emails, bank transactions) that could occur in the real world. These adversarial tasks are inherently long-horizon, requiring many steps of reasoning and interaction with the environment, with sparse and delayed success signals. Reinforcement learning is well-suited to this sparse, delayed reward structure.</span></li><li><b><span>Leveraging frontier LLM capabilities.</span></b><span> We trained frontier LLMs directly as auto-red-teamers, so the attacker benefits directly from improvements in reasoning and planning in frontier models. As base models get stronger, the attacker naturally becomes more capable as well—making this a scalable way to keep pressure on our defenses as our models evolve.</span></li><li><b><span>Scaling compute and mimicking adaptive attackers.</span></b><span> Reinforcement learning is well suited for scaling computation spent on searching for attacks over large numbers of samplings and learning steps, and it also closely reflects how adaptive human attackers behave: iteratively trying strategies, learning from outcomes, and reinforcing successful behaviors.<br></span></li></ol></div><p><b><span>Our automated attacker can discover novel, realistic prompt-injection attacks end-to-end. </span></b><span>Unlike most prior automated red teaming work, which surfaced simple failures such as eliciting specific output strings or triggering an unintended single-step tool call from the agent, our RL-trained attacker can steer an agent into executing sophisticated, long-horizon harmful workflows that unfold over tens (or even hundreds) of steps. We also observed novel attack strategies that did not appear in our human red teaming campaign or external reports.</span></p><p><span>The demo below presents a concrete prompt injection exploit found by our automated attacker, which we then used to further harden the defenses of ChatGPT Atlas. The attacker seeds the user’s inbox with a malicious email containing a prompt injection that directs the agent to send a resignation letter to the user's CEO. Later, when the user asks the agent to draft an out-of-office reply, the agent encounters that email during normal task execution, treats the injected prompt as authoritative, and follows it. The out-of-office never gets written and the agent resigns on behalf of the user instead.</span></p><p><span>The nature of prompt injection makes deterministic security guarantees challenging, but by scaling our automated security research, adversarial testing, and tightening our rapid response loop, we are able to improve the model’s robustness and defenses - before waiting for an attack to occur in the wild.&nbsp;</span></p><p><span>We're sharing this demo to help users and researchers better understand the nature of these attacks—and how we are actively defending against them. We believe this represents the frontier of what automated red teaming can accomplish, and we are extremely excited to continue our research. </span></p><div id=\"hardening-chatgpt-atlas-with-a-proactive-rapid-response-loop\"><p></p><h4><span>Hardening ChatGPT Atlas with a proactive rapid response loop</span></h4><p></p></div><p><span>Our automated red teaming is driving a proactive rapid response loop: when the automated attacker discovers a new class of successful prompt injection attacks, it immediately creates a concrete target for improving our defenses.</span></p><p><b><span>Adversarially training against newly discovered attacks. </span></b><span>We continuously train updated agent models against our best automated attacker—prioritizing the attacks where the target agents currently fail. The goal is to teach agents to ignore adversarial instructions and stay aligned with the user’s intent, improving resistance to newly discovered prompt-injection strategies. This “burns in” robustness against novel, high-strength attacks directly into the model checkpoint. For example, recent automated red teaming directly produced a new adversarially trained browser-agent checkpoint that has already been rolled out to all ChatGPT Atlas users. This ultimately helps better protect our users against new types of attacks.</span></p><p><b><span>Using attack traces to improve the broader defense stack. </span></b><span>Many attack paths discovered by our automated red teamer also reveal opportunities for improvement outside of the model itself—such as in monitoring, safety instructions we put in the model’s context, or system-level safeguards. Those findings help us iterate on the full defense stack, not just the agent checkpoint. </span></p><p><b><span>Responding to active attacks.</span></b><span> This loop can also help better respond to active attacks in the wild. As we look across our global footprint for potential attacks, we can take the techniques and tactics we observe external adversaries using, feed them into this loop, emulate their activity, and drive defensive change across our platform. </span></p><div id=\"outlook-our-long-term-commitment-to-agent-security\"><p></p><h4><span>Outlook: our long-term commitment to agent security</span></h4><p></p></div><p><span>Strengthening our ability to red team agents and using our most capable models to automate parts of that work—helps make the Atlas browser agent more robust by scaling the discovery-to-fix loop. This hardening effort reinforces a familiar lesson from security: a well-worn path to stronger protection is to continuously pressure-test real systems, react to failures, and ship concrete fixes.</span></p><p><span>We expect adversaries to keep adapting. Prompt injection, much like scams and social engineering on the web, is unlikely to ever be fully “solved”. But we’re optimistic that a proactive, highly responsive rapid response&nbsp; loop can continue to materially reduce real-world risk over time. By combining automated attack discovery with adversarial training and system-level safeguards, we can identify new attack patterns earlier, close gaps faster, and continuously raise the cost of exploitation.</span></p><p><span>Agent mode in ChatGPT Atlas is powerful—and it also expands the security threat surface. Being clear-eyed about that tradeoff is part of building responsibly. Our goal is to make Atlas meaningfully more secure with every iteration: improving model robustness, strengthening the surrounding defense stack, and monitoring for emerging abuse patterns in the wild.</span></p><p><span>We’ll continue investing across research and deployment, developing better automated red teaming methods, rolling out layered mitigations, and iterating quickly as we learn. We’ll also share what we can with the broader community.</span></p><div id=\"recommendations-for-using-agents-safely\"><p></p><h4><span>Recommendations for using agents safely</span></h4><p></p></div><p><span>While we continue to strengthen Atlas at the system level, there are steps users can take to reduce risk when using agents.&nbsp;</span></p><p><b><span>Limit logged-in access when possible. </span></b><span>We continue to recommend that users take advantage of </span><a href=\"https://help.openai.com/en/articles/12574142-chatgpt-atlas-data-controls-and-privacy#h_1976eefb25\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><u><span>logged-out mode</span></u>⁠<span>(opens in a new window)</span></a><span> when using Agent in Atlas whenever access to websites you’re logged in to isn’t necessary for the task at hand, or to limit access to specific sites you sign-in to during the task.&nbsp;</span></p><p><b><span>Carefully review confirmation requests.</span></b><span> For certain consequential actions, such as completing a purchase or sending an email, agents are designed to ask for your confirmation before proceeding. When an agent asks you to confirm an action, take a moment to verify that the action is correct and that any information being shared is appropriate for that context.</span></p><p><b><span>Give agents explicit instructions when possible.</span></b><span> Avoid overly broad prompts like “review my emails and take whatever action is needed.” Wide latitude makes it easier for hidden or malicious content to influence the agent, even when safeguards are in place. It’s safer to ask the agent to perform specific, well-scoped tasks. While this does not eliminate risk, it makes attacks harder to carry out.</span></p><p><span>If agents are to become trusted partners for everyday tasks, they must be resilient to the kinds of manipulation the open web enables. Hardening against prompt injection is a long-term commitment and one of our top priorities. We’ll be sharing more on this work soon.</span></p></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/evaluating-chain-of-thought-monitorability",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/evaluating-chain-of-thought-monitorability",
      "title": "Evaluating chain-of-thought monitorability",
      "publishedAt": "Thu, 18 Dec 2025 12:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:20.311Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div><p><span>When AI systems make decisions that are difficult to supervise directly, it becomes important to understand how those decisions are made. One promising approach is to monitor a model’s internal reasoning, rather than only its actions or final outputs.</span></p><p><span>Modern reasoning models, such as GPT‑5 Thinking, generate an explicit chain-of-thought before producing an answer. Monitoring these </span><i><span>chains-of-thought</span></i><span> for misbehavior can be </span><a href=\"https://openai.com/index/chain-of-thought-monitoring/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>far more effective</span></u>⁠</a><span> than monitoring a model’s actions and outputs alone. However, </span><a href=\"https://arxiv.org/abs/2507.11473\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>researchers at OpenAI and across the broader industry worry</span></u>⁠<span>(opens in a new window)</span></a><span> that this chain-of-thought “monitorability” may be fragile to changes in training procedure, data sources, and even continued scaling of existing algorithms.</span></p><p><span>We want chain-of-thought monitorability to hold up as models scale and are deployed in higher-stakes settings. We call on researchers across the industry to work to preserve chain-of-thought monitorability as long as possible and to determine whether it can serve as a load-bearing control layer for future AI systems. </span><b><span>To preserve monitorability, we must be able to measure it,</span></b><span> yet few evaluations exist [</span><a href=\"https://www.anthropic.com/research/shade-arena-sabotage-monitoring\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>1</span></u>⁠<span>(opens in a new window)</span></a><span>, </span><a href=\"https://metr.org/blog/2025-08-08-cot-may-be-highly-informative-despite-unfaithfulness/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>2</span></u>⁠<span>(opens in a new window)</span></a><span>, </span><a href=\"https://arxiv.org/abs/2505.05410\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>3</span></u>⁠<span>(opens in a new window)</span></a><span>] and methodologies vary.</span></p><p><span>In this work, we study chain-of-thought monitorability systematically. We introduce a framework to evaluate chain-of-thought monitorability and a suite of 13 evaluations (comprising a total of 24 environments) designed to concretely measure the monitorability of a system. These evaluations fall into three archetypes—intervention, process, and outcome-property. Using them, we study how monitorable today’s large language models are, and how monitorability changes with more reasoning at inference time, larger reinforcement learning runs, and increased pretraining scale.</span></p><p><span>We find that most frontier reasoning models are fairly monitorable, though not perfectly so. In almost every setting we study, monitoring chains-of-thought is substantially more effective than monitoring actions and final outputs alone. Models that think for longer tend to be more monitorable, and reinforcement learning optimization at current frontier scales does not appear to meaningfully degrade monitorability.</span></p><p><span>Notably, we observe a tradeoff between reasoning effort and model size: when a model is run at low reasoning effort, a smaller model run at higher reasoning effort can reach comparable capability while being easier to monitor. This comes at the cost of increased inference compute, which we refer to as a </span><i><span>monitorability tax</span></i><span>. Finally, we find that asking follow-up questions and monitoring the resulting chains- of- thought can further improve monitorability.</span></p><div id=\"what-is-monitorability\"><p></p><h4><span>What is “Monitorability”?</span></h4><p></p></div><p><span>We define monitorability as the ability of a monitor to predict properties of interest about an agent’s behavior. In many cases, these properties correspond to forms of misbehavior—such as deception, reward hacking, or biased reasoning—but they can also include benign properties, such as whether an agent relied on a particular hint or followed a specific line of reasoning.</span></p><p><span>Monitorability is a two-sided property. It depends both on the monitor but also on the agent being monitored. </span></p><div><ul><li><b><span>Monitors.</span></b><span> We could make monitors smarter in various ways, and we would like our evaluations to give higher scores to monitors that can make better predictions about an agent’s behavior.</span></li><li><b><span>Agents. </span></b><span>There are many observations of the agent that could be given to the monitor, including the agent’s actions, outputs, neural activations, and recently with the advent of reasoning models, the chain-of-thought. It’s possible for some agents to have thoughts that are harder to understand than others, and so we would like our monitorability evaluations to be able to compare agents and not just monitors.</span></li></ul></div><p><span>Our evaluations are designed to assess both sides: whether certain agents are easier or harder to monitor, and whether more capable monitors can extract more information from the same signals.</span></p><div id=\"why-should-chains-of-thought-be-monitorable\"><p></p><h4><span>Why should chains-of-thought be monitorable?</span></h4><p></p></div><p><span>Before the current generation of reasoning models, chains-of-thought were typically elicited by prompting a model to “think step by step.”&nbsp;</span></p><p><span>The chains-of-thought of reasoning models differ in two important ways. First, they are explicitly trained to generate a chain-of-thought and then give an answer. When prompting a chat model to think “step by step,” the model has to adhere to any specifications placed on the model, i.e. to be helpful, harmless, etc. Conversely, the reinforcement learning training process teaches a reasoning model that the CoT is a private space where it can think whatever it wants and not be punished, just like a human can think privately and not be punished for “thought crimes.”&nbsp;</span></p><p><span>Second, the training process effectively searches for chains-of-thought that are the </span><i><span>most instrumentally useful</span></i><span> to the agent in getting a high reward final answer. Thus, not only is the chain-of-thought a space where the model can freely express itself, it contains thinking that helps the model reach its final answer.</span></p><div id=\"a-framework-for-evaluating-monitorability\"><p></p><h4><span>A framework for evaluating monitorability</span></h4><p></p></div><p><span>We organize our monitorability evaluations into three distinct archetypes, each exploiting a different mechanism to measure monitorability.</span></p><div><ol><li><b><span>Intervention evaluations,</span></b><span> inspired by </span><a href=\"https://arxiv.org/abs/2505.05410\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>Chen et al. 2025</span></u>⁠<span>(opens in a new window)</span></a><b><span>, </span></b><a href=\"https://arxiv.org/abs/2501.08156\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>Chua et al. 2025</span></u>⁠<span>(opens in a new window)</span></a><span>, </span><a href=\"https://arxiv.org/abs/2305.04388\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>Turpin et al. 2023</span></u>⁠<span>(opens in a new window)</span></a><span>,</span><b><span> </span></b><span>we</span><b><span> </span></b><span>introduce a controlled change in the agent’s environment and test whether a monitor can detect why behavior changed by identifying evidence of the intervention in the chain-of-thought. Unlike standard control evaluations (e.g. </span><a href=\"https://www.anthropic.com/research/shade-arena-sabotage-monitoring\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>Shade-Arena</span></u>⁠<span>(opens in a new window)</span></a><span>), intervention evaluations allow us to probe whether a monitor can detect </span><i><span>why </span></i><span>behavior changed, rather than simply detecting whether a behavior occurred.&nbsp;</span></li><li><b><span>Process evaluations </span></b><span>focus on tasks with a limited number of valid solution paths (for example, a math problem can only be solved with certain theorems). Human experts enumerate these paths, and we test whether a monitor can detect which steps the model actually took.</span><b><span> </span></b><span>These evaluations allow us to link what a model did to particular steps the model must have taken and ask if those steps are monitorable.</span></li><li><b><span>Outcome-property evaluations </span></b><span>apply when a property of the model’s output can be reliably measured. In these cases, we test whether a monitor can predict that property—such as reward hacking—from the model’s reasoning or actions. For example, in a coding task, we may have a way to reliably evaluate whether a model is taking actions to hack a reward signal, and we can ask if a monitor finds evidence of intent to reward hack in the model’s thinking or actions.</span></li></ol></div><p><span>Below we provide a taxonomy for our evaluations. Please see our paper for a full description of each individual evaluation.</span></p><div><p><i><span>Neutral evaluations ask whether the reasoning for normal tasks where the model does not misbehave is monitorable. Sycophancy evaluations target cases where models change their behavior to overly please or agree with a user. Bias evaluations ask if a model’s bias around gender, race, etc. is monitorable. Finally, we use the misalignment label as a catchall for other forms of misbehavior: cheating, lying, etc.</span></i><span>¹</span></p></div><div><p></p><h4>Monitorable vs. unmonitorable examples</h4><p></p></div><div id=\"the-effect-of-high-compute-reinforcement-learning\"><p></p><h4><span>The effect of high compute reinforcement learning</span></h4><p></p></div><p><span>Treating the chain-of-thought as a private workspace—and training models to use it to solve tasks effectively—should, in principle, make chains-of-thought monitorable.&nbsp;At the same time, there are several reasons why reinforcement learning could plausibly harm chain-of-thought monitorability.</span></p><p><span>First, constraints on reasoning length, such as limiting the time or tokens a model has to think, create incentives for models to reason less. Second, even without applying direct optimization pressure to the chain-of-thought itself, indirect pressures could still reduce monitorability. </span><a href=\"https://arxiv.org/abs/2511.11584\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>For example</span></u>⁠<span>(opens in a new window)</span></a><span>, models may find it easier to reach high-reward answers if they avoid reasoning paths that can result in low-reward or undesirable outcomes. Finally, reduced monitorability could arise from spurious drift driven by reinforcement learning gradient noise.</span></p><p><span>Below, we examine how monitorability changes over two of OpenAI’s frontier reinforcement learning training runs. While these results represent only two datapoints, they suggest reinforcement learning&nbsp;optimization at current frontier scales does not materially degrade chain-of-thought monitorability, and increases monitorability for early steps. However, this does not rule out the possibility that significantly larger reinforcement learning scales could negatively affect monitorability in the future. </span></p><div id=\"effect-of-pretraining-scale\"><p></p><h4><span>Effect of pretraining scale</span></h4><p></p></div><p><span>A large portion of the intelligence of current AI systems is driven by increased pretraining scale, whereby both model parameters and dataset sizes have consistently grown since the beginning of the GPT paradigm. Increased parameter counts results in more inference compute being used per token generated, which, in the case of reasoning models, gives more opportunity for the model’s cognition to take place within activations rather than within the chain-of-thought. Thus, we might expect larger models to be less monitorable at a fixed capability level.&nbsp;</span></p><p><span>At the same time, smaller models may also be less capable of using the chain-of-thought effectively—which may also affect monitorability.</span></p><p><span>To investigate the relationship between pretraining size, capability, and monitorability, we trained a family of models over two orders of magnitude of pretraining compute with increasing depth and width, and then applied reinforcement learning to each on a shared dataset of math, science, and coding tasks. We evaluated each model across a range of reasoning efforts and compared monitorability at matched capability levels. </span></p><div><p><i><span>We plot the monitorability on our evaluation suite of each model versus the mean pass rate over a set of standard capability evaluations (GPQA-Diamond, AIME 2025, and CodeForces). Each model is evaluated over many reasoning efforts (note that each point in this figure is actually the result of a linear interpolation between measured reasoning efforts). Circles indicate that the model at that particular reasoning effort dominates in monitorability at that capability level, and transparent X’s represent that the model at that particular reasoning effort is dominated in monitorability by another model for that capability level. In black arrows we show examples of two ways to spend more inference compute, by sampling many more tokens with a smaller model or relatively less additional tokens with the same model. In red arrows, we show the monitorability tax that must be paid between, i.e. slightly reduced capabilities for far more monitorability.</span></i></p></div><p><span>In general, we find that if we were to deploy a given model at a low reasoning effort, we could instead deploy a smaller model at a higher reasoning effort. Compared to spending the additional compute on more reasoning effort from the original large model, we could obtain a higher monitorability at a small capability cost, or </span><i><span>monitorability tax; </span></i><span>however, the wallclock time it takes to finish a completion may also be a factor in some deployment decisions, which we do not consider here. As our models improve and are deployed in higher stakes settings, it may become necessary to pay such a tax to safely deploy them.</span></p><p><span>These results have the following limitations: (1) we only train each model once, and therefore do not account for variance across training runs. For example, the “medium” model seems fairly off-trend in monitorability compared to the other models in the series, which could be due to this. (2) We evaluate capabilities on relatively in-distribution settings (math, science, and coding), and the tradeoffs may change when accounting for out-of-distribution robustness.</span></p><div id=\"agent-versus-monitor-compute-and-scalable-control\"><p></p><h4><span>Agent versus monitor compute and scalable control</span></h4><p></p></div><p><span>As AI systems scale beyond human-level performance, direct human supervision becomes increasingly difficult. Ideally, we would have scalable control procedures that allow trusted but weaker monitors to oversee stronger, potentially untrusted agents. Such procedures should also be able to absorb additional test-time compute and produce more accurate predictions.</span></p><div id=\"getting-more-monitorability-out-of-chains-of-thought\"><p></p><h4><span>Getting more monitorability out of chains-of-thought</span></h4><p></p></div><p><span>Chain-of-thought monitorability is often treated as a fixed quantity: once a model produces a response at a given reasoning effort, one monitors the resulting chain-of-thought as best as possible. Because we’ve consistently found that monitorability improves as we increase the reasoning effort of our agents, we were inspired to ask if we could somehow get more tokens to monitor.</span></p><p><span>To explore this, we experimented with asking </span><i><span>follow-up questions</span></i><span> after a model responds, while keeping the original prompt, chain-of-thought, and answer in context. These follow-ups focus on the property being monitored—for example, “Did you use the hint in a meaningful way to choose your answer?” or “Did you do something sketchy?” In principle, because the model retains access to its earlier reasoning, it may be able to surface previously unverbalized thoughts in the follow-up chain-of-thought or response.</span></p><p><span>These results are preliminary. We explored only a small number of follow-up question formats and believe there is substantial room for improved methodology. Furthermore, this method is similar in spirit to </span><a href=\"https://openai.com/index/how-confessions-can-keep-language-models-honest/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>model confessions</span></u>⁠</a><span>, and we are actively working on comparing the approaches.</span></p><p><span>Beyond improving monitorability, this approach is appealing because it can be applied post hoc, incurring additional compute cost only when deeper inspection is needed. Looking ahead, if models are trained to expect changes in reasoning effort mid-conversation, post-hoc follow-ups at higher reasoning effort may further improve monitorability even when initial reasoning effort is low or absent.</span></p><div id=\"limitations\"><p></p><h4><span>Limitations</span></h4><p></p></div><p><span>Each evaluation in our suite measures whether a single aspect of the reasoning process is monitorable. While we aim to cover a broad range of behaviors—including sycophancy, sandbagging, bias, scheming, and reward hacking—there are many additional properties that remain untested.</span></p><p><span>As with any benchmark, our evaluations may suffer from limited realism or increasing evaluation awareness, and results may not generalize to detecting misbehavior in the wild.&nbsp;</span></p><p><span>Finally, as alignment improves, models may misbehave less frequently in controlled evaluation settings,reducing the signal available to monitorability and control evaluations that rely on detecting misbehavior. In those cases, “neutral” monitorability evaluations remain applicable, though it is an open question how predictive they are of real-world failures. </span></p><div id=\"conclusion\"><p></p><h4><span>Conclusion</span></h4><p></p></div><p><span>If the alignment problem cannot be fully solved, then scalable control methods will be incredibly important in our ability to safely deploy highly capable models. Chain-of-thought monitoring and related methods may be one of the few control mechanisms at our disposal. Chain-of-thought can also yield large benefits in understanding more about our models, e.g. whether they are aware they are in an evaluation and behaving differently than they would in deployment, whether there is evidence of any adverse goals that have not yet manifested in our evaluations, etc. Thus, we&nbsp;believe it is of utmost importance to understand how and when we may lose monitorability.</span></p><p><span>We view chain-of-thought monitoring as complementary to mechanistic interpretability, not as a replacement for it. Like all known control and interpretability methods, each has limitations and blind spots. Our expectation is that combining multiple approaches—a </span><i><span>defense-in-depth strategy</span></i><span>—can help cover gaps that any single method leaves exposed. Notably, the evaluations introduced in this work could also be applied to assess and compare mechanistic interpretability-based monitoring techniques.</span></p><p><span>Because we believe that chain-of-thought monitoring could be a load-bearing layer in a scalable control scheme, we plan to expand upon these evaluations and use them to inform future modeling and data decisions.</span></p></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/ai-literacy-resources-for-teens-and-parents",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/ai-literacy-resources-for-teens-and-parents",
      "title": "AI literacy resources for teens and parents",
      "publishedAt": "Thu, 18 Dec 2025 11:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:19.859Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div id=\"main\" tabindex=\"-1\"><span aria-live=\"polite\" aria-atomic=\"true\">OpenAI</span><article><div><p>A teen’s guide to using ChatGPT thoughtfully—plus tips for parents to support healthy, responsible use.</p></div><div><p><span>We’re sharing two new AI literacy resources to help families use ChatGPT thoughtfully, safely, and confidently. First is a family-friendly guide that explains—in plain language—how AI models are trained, why AI can sometimes get things wrong, and the importance of double checking the information received. It also includes practical tips for responsible use, like how to write better prompts, why you might get different answers to the same question, and how to manage your data and settings.</span></p><p><span>We’re also publishing a set of tips for parents, with conversation starters and practical guidance to help parents talk with their teens about what AI can (and can’t) do, build critical thinking, set healthy boundaries, and navigate emotional or sensitive topics.&nbsp;</span></p><p><span>Both resources were developed with expert input, including members of our </span><a href=\"https://openai.com/index/expert-council-on-well-being-and-ai/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>Expert Council on Well-Being and AI</span></u>⁠</a><span> and </span><a href=\"https://connectsafely.org/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>ConnectSafely</span></u>⁠<span>(opens in a new window)</span></a><span>, groups with deep expertise in online safety, human-computer interaction, teen development, and mental health. We’ll continue expanding our AI literacy work over time with additional guidance for different ages and stages, and we’ll keep updating these resources as we learn more.</span></p></div><section id=\"citations\" data-testid=\"citations\"><ul><li><a href=\"https://openai.com/news/?tags=user-safety\" target=\"_blank\" rel=\"noopener noreferrer\">User Safety &amp; Control</a></li><li><a href=\"https://openai.com/news/?tags=2025\" target=\"_blank\" rel=\"noopener noreferrer\">2025</a></li></ul></section></article></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/us-department-of-energy-collaboration",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/us-department-of-energy-collaboration",
      "title": "Deepening our collaboration with the U.S. Department of Energy",
      "publishedAt": "Thu, 18 Dec 2025 11:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:19.995Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div><p><span>Scientific progress shapes everything from health and energy to national security and our understanding of the universe. If AI can help researchers explore more ideas, test hypotheses faster, and move from insight to validated results more quickly, the benefits compound—across disciplines and across society.</span></p><p><span>OpenAI and the U.S. Department of Energy (DOE) have signed a memorandum of understanding (MOU) to explore opportunities for further&nbsp; collaborations on AI and advanced computing in support of DOE initiatives, including the Genesis Mission. This work is part of </span><a href=\"https://openai.com/science/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>OpenAI for Science</span></u>⁠</a><span>, our effort to help scientists accelerate discovery by pairing frontier AI models with the tools, workflows, and expertise of real research environments.</span></p><p><span>This MOU builds on OpenAI’s existing work with DOE’s national laboratories, where we’ve already deployed frontier models in real research environments and worked directly with scientists on high-impact problems.</span></p><p><span>The </span><a href=\"https://genesis.energy.gov/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>Genesis Mission</span></u>⁠<span>(opens in a new window)</span></a><span> brings together government, national labs, and industry to apply advanced AI and computing to accelerate scientific discovery. The MOU establishes a framework for information sharing and coordination, and creates a path for the parties to discuss and develop potential follow-on agreements as specific projects take shape. Today, OpenAI also </span><a href=\"https://cdn.openai.com/pdf/openai-ostp-accelerating-science-rfi.pdf\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><u><span>submitted detailed recommendations</span></u>⁠<span>(opens in a new window)</span></a><span> to the White House Office of Science and Technology Policy on how the United States can strengthen science and technology leadership through AI. That filing outlines why we see 2026 as a “Year of Science” and why access to frontier AI models, compute, and real research environments is essential to accelerating discovery. The agreement with the Department of Energy reflects that vision moving into practice.</span></p><p><span>This announcement follows today’s Genesis Mission event at the White House, where Kevin Weil, Vice President of OpenAI for Science, joined DOE and other collaborators.&nbsp;</span></p><div><blockquote>“We’re excited to collaborate with the Department of Energy and contribute to the Genesis Mission. When frontier AI meets the expertise of the national labs, it opens up new ways to explore ideas, test them faster, and accelerate scientific progress.”</blockquote><p>Kevin Weil, Vice President of OpenAI for Science</p></div><p><span>OpenAI and the Department of Energy share a focus on advancing basic and applied research and strengthening U.S. leadership in AI and advanced computing. This memorandum of understanding provides a structured way for OpenAI and DOE to exchange technical expertise, coordinate activities, and explore areas of collaboration—such as fusion energy, where DOE labs bring world-leading facilities, modeling tools, and data—while ensuring that future project work is clearly scoped and governed through follow-on agreements.</span></p><p><span>For OpenAI, this reflects how we think about building OpenAI for Science. Progress comes from working side-by-side with researchers to understand where AI meaningfully helps, where it falls short, and how it can be safely integrated into real scientific workflows. That work depends on close engagement with domain experts, access to world-class scientific infrastructure, and rigorous evaluation in the environments where science actually happens. The MOU creates space for that kind of collaboration, while preserving the rigor and accountability required for responsible scientific use.</span></p><div id=\"building-on-a-growing-partnership-with-does-national-laboratories\"><p></p><h2><span>Building on a growing partnership with DOE’s national laboratories</span></h2><p></p></div><p><span>Over the past year, we’ve been working closely with scientists across the DOE national lab system to understand where frontier models help, where they fall short, and what it takes to integrate them into real research settings.</span></p><p><span>Together with DOE’s national labs, we convened the </span><a href=\"https://openai.com/global-affairs/1000-scientist-ai-jam-session/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>1,000 Scientist AI Jam Session</span></u>⁠</a><span>—a first-of-its-kind event across nine labs where more than 1,000 scientists used frontier AI models to test domain-specific problems, evaluate model responses, and provide structured feedback to inform future system development. This model of collaboration allows researchers to stress-test AI on the problems that matter in their work, and help shape how these tools evolve.&nbsp;</span></p><p><span>We partner with the</span><a href=\"https://openai.com/index/strengthening-americas-ai-leadership-with-the-us-national-laboratories/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span> National Nuclear Security Administration (NNSA) laboratories</span></u>⁠</a><span>, including Los Alamos National Laboratory, Lawrence Livermore National Laboratory, and Sandia National Laboratories, to support scientific and technical research. As part of this work, OpenAI has deployed advanced reasoning models on the Venado supercomputer at Los Alamos National Laboratory, where it serves as a shared resource for researchers across the NNSA labs. This collaboration focuses on applying frontier AI models in high-performance computing environments to support complex scientific and national research challenges.</span></p><p><span>We also partnered with </span><a href=\"https://openai.com/index/openai-and-los-alamos-national-laboratory-work-together/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>Los Alamos National Laboratory</span></u>⁠</a><span> to develop evaluations that study how multimodal AI systems can be used safely by scientists in laboratory settings. That work is designed to move beyond purely text-based assessments and toward more realistic measurements of how models may influence outcomes in high-consequence domains, grounded in expert oversight, careful study design, and a clear commitment to risk reduction.</span></p><div id=\"how-openai-for-science-fits-into-this-partnership\"><p></p><h2><span>How OpenAI for Science fits into this partnership</span></h2><p></p></div><p><span>Our goal with OpenAI for Science is to accelerate science: to help researchers explore more ideas, test hypotheses faster, and uncover insights that would otherwise take significant time. We approach that goal with two complementary beliefs:</span></p><div><ul><li><b><span>Scientific tools matter.</span></b><span> Simulation engines, analysis pipelines, domain databases, and high-performance computing are essential for precision and throughput.</span></li><li><b><span>Frontier reasoning matters.</span></b><span> As models scale and improve, they can increasingly support conceptual work, connecting ideas across fields, navigating large literatures beyond keyword search, proposing mechanisms, and stress-testing hypotheses.</span></li></ul></div><p><span>DOE’s national labs are uniquely positioned at the intersection of these two beliefs: they operate some of the world’s most sophisticated scientific infrastructure and convene experts working on problems where better reasoning and better computing can translate directly into scientific advancement and societal benefit.</span></p><div id=\"whats-next\"><p></p><h2><span>What’s next</span></h2><p></p></div><p><span>Scientific discovery has always advanced fastest when great tools meet great scientists. We believe frontier AI—developed in close partnership with the scientific community—can become a new kind of scientific instrument: one that expands what researchers can explore, improves the speed of iteration, and helps translate insight into impact. We’re proud to work alongside DOE and the national labs as we build toward that future.</span></p></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/updating-model-spec-with-teen-protections",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/updating-model-spec-with-teen-protections",
      "title": "Updating our Model Spec with teen protections",
      "publishedAt": "Thu, 18 Dec 2025 11:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:20.385Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div><p><span>We’re sharing an update to our Model Spec, the written set of rules, values, and behavioral expectations that guides how we want our AI models to behave, especially in difficult or high stakes situations, with </span><a href=\"http://model-spec.openai.com/2025-12-18.html\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><u><span>Under-18 (U18) Principles</span></u>⁠<span>(opens in a new window)</span></a><span>. Model behavior is critical to how people interact with AI, and teens have different developmental needs than adults.</span></p><p><span>The U18 Principles guide how ChatGPT should provide a safe, age-appropriate experience for teens aged 13 to 17. Grounded in developmental science, this approach prioritizes prevention, transparency, and early intervention. In developing these principles, we previewed them with external experts, including the American Psychological Association, as part of our ongoing work to seek input to strengthen our approach.</span></p><p><span>While the principles of the Model Spec continue to apply to both adult and teen users, this update clarifies how it should be applied in teen contexts, especially where safety considerations for minors may be more pronounced.&nbsp;</span></p><p><span>The U18 Principles are anchored in four guiding commitments:</span></p><div><ul><li><b><span>Put teen safety first</span></b><span>, even when it may conflict with other goals</span></li><li><b><span>Promote real-world support</span></b><span> by encouraging offline relationships and trusted resources</span></li><li><b><span>Treat teens like teens</span></b><span>, neither condescending to them nor treating them as adults</span></li><li><b><span>Be transparent</span></b><span> by setting clear expectations</span></li></ul></div><p><span>Consistent with our </span><a href=\"https://openai.com/index/introducing-the-teen-safety-blueprint/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>Teen Safety Blueprint</span></u>⁠</a><span>, these principles have guided our teen safety work to date, including the content protections we apply to users who tell us they are under 18 at sign up, and through parental controls. In these contexts, we’ve implemented safeguards to guide the model to take extra care when discussing higher-risk areas, including self-harm and suicide, romantic or sexualized roleplay, graphic or explicit content, dangerous activities and substances, body image and disordered eating, and requests to keep secrets about unsafe behavior.</span></p><p><span>The American Psychological Association, which reviewed an early draft of the U18 Model Spec and offered important insights for the long term, is clear about the importance of protecting teens:</span></p><p><span>“</span><i><span>APA encourages AI developers to offer developmentally appropriate precautions for youth users of their products and to take a more protective approach for younger users.&nbsp; Children and adolescents might benefit from AI tools if they are balanced with human interactions that science shows are critical for social, psychological, behavioral, and even biological development. Youth experiences with AI should be thoroughly supervised and discussed with trusted adults to encourage critical review of what AI bots offer, and to encourage young people’s development of independent thinking and skills.</span></i><span>”—</span><b><span>Dr. Arthur C. Evans Jr, CEO, American Psychological Association</span></b></p><p><span>This update also clarifies how the assistant should respond when safety concerns arise for teens. This means teens should encounter stronger guardrails, safer alternatives, and encouragement to seek trusted offline support when conversations move into higher-risk territory. Where there is imminent risk, teens are urged to contact emergency services or crisis resources.</span></p><p><span>As with the rest of the Model Spec, the U18 Principles reflect our intended model behavior. We will continue to refine them as we incorporate new research, expert input, and real-world use.</span></p><div id=\"building-on-our-work-to-strengthen-teen-safety\"><p></p><h2><span>Building on our work to strengthen teen safety</span></h2><p></p></div><p><span>Alongside updating the Model Spec, we’ve taken a multi-layered approach to </span><a href=\"https://openai.com/index/building-more-helpful-chatgpt-experiences-for-everyone/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>strengthening teen safety</span></u>⁠</a><span> across ChatGPT, spanning product safeguards, family support, and expert guidance.</span></p><p><span>Since rolling out </span><a href=\"https://help.openai.com/en/articles/12315553-parental-controls-on-chatgpt-faq\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><u><span>parental controls</span></u>⁠<span>(opens in a new window)</span></a><span>, we’ve extended protections across new products including group chats, the ChatGPT Atlas browser, and the Sora app. These updates help parents tailor their teen’s ChatGPT experience as we introduce new products and features.</span></p><div id=\"working-with-experts\"><p></p><h2><span>Working with experts</span></h2><p></p></div><p><span>Our work in teen safety is guided by close engagement with experts across disciplines and expertise. In October, we established an </span><a href=\"https://openai.com/index/expert-council-on-well-being-and-ai/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>Expert Council on Well-Being and AI</span></u>⁠</a><span> to help advise and define what healthy interactions with AI should look like for all ages. That work has informed guidance on parental controls and parent notifications. We also incorporate clinical expertise through our </span><a href=\"https://openai.com/index/strengthening-chatgpt-responses-in-sensitive-conversations/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>Global Physician Network</span></u>⁠</a><span> to inform safety research and evaluate model behavior, including improving how ChatGPT recognizes distress and guides people toward professional care when appropriate. We built on these foundations with </span><a href=\"https://openai.com/index/introducing-gpt-5-2/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>GPT‑5.2</span></u>⁠</a><span>, and we’ve also expanded access to real-world support by surfacing localized helplines in ChatGPT and Sora through our partnership with </span><a href=\"https://www.throughlinecare.com/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>ThroughLine</span></u>⁠<span>(opens in a new window)</span></a><span>.</span></p><div id=\"whats-next\"><p></p><h2><span>What’s next</span></h2><p></p></div><p><span>We’re in the early stages of rolling out an </span><a href=\"https://help.openai.com/en/articles/12652064-age-prediction-in-chatgpt\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><u><span>age prediction model</span></u>⁠<span>(opens in a new window)</span></a><span> on ChatGPT consumer plans. This will help us automatically apply teen safeguards when we believe an account belongs to a minor. If we are not confident about someone’s age or have incomplete information, we’ll default to an&nbsp;U18 experience and give adults ways to verify their age.</span></p><p><span>Strengthening teen safety is ongoing work and we’ll continue to improve parental controls and model capabilities, expand resources for parents, work with organizations, researchers, and expert partners including the Well-Being Council and Global Physician Network.&nbsp;</span></p><p><span>We’re committed to making strong teen protections and improving them over time to better support teens and families.</span></p></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/gpt-5-2-codex",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/gpt-5-2-codex",
      "title": "Introducing GPT-5.2-Codex",
      "publishedAt": "Thu, 18 Dec 2025 00:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:20.937Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div><p><span>Today we’re releasing GPT‑5.2-Codex, the most advanced agentic coding model yet for complex, real-world software engineering. GPT‑5.2-Codex is a version of </span><a href=\"https://openai.com/index/introducing-gpt-5-2/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>GPT‑5.2</span></u>⁠</a><span> further optimized for agentic coding in Codex, including improvements on long-horizon work through context compaction, stronger performance on large code changes like refactors and migrations, improved performance in Windows environments, and significantly stronger cybersecurity capabilities.</span></p><p><span>As our models continue to advance along the intelligence frontier, we’ve observed that these improvements also translate to capability jumps in specialized domains such as </span><a href=\"https://openai.com/index/strengthening-cyber-resilience/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>cybersecurity</span></u>⁠</a><span>. For example, just last week, a security researcher using GPT‑5.1-Codex-Max with Codex CLI found and responsibly </span><a href=\"https://react.dev/blog/2025/12/11/denial-of-service-and-source-code-exposure-in-react-server-components\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>disclosed</span></u>⁠<span>(opens in a new window)</span></a><span> a vulnerability in React that could lead to source code exposure.</span></p><p><span>GPT‑5.2-Codex has stronger cybersecurity capabilities than any model we’ve released so far. These advances can help strengthen cybersecurity at scale, but they also raise new dual-use risks that require careful deployment. While GPT‑5.2-Codex does not reach a ‘High’ level of cyber capability under our Preparedness Framework, we’re designing our </span><a href=\"https://openai.com/index/strengthening-cyber-resilience/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>deployment approach</span></u>⁠</a><span> with future capability growth in mind.</span></p><p><span>We're releasing GPT‑5.2-Codex today in all Codex surfaces for paid ChatGPT users, and working towards safely enabling access to GPT‑5.2-Codex for API users in the coming weeks. In parallel, we’re piloting invite-only trusted access to upcoming capabilities and more permissive models for vetted professionals and organizations focused on defensive cybersecurity work. We believe that this approach to deployment will balance accessibility with safety.</span></p><div id=\"pushing-the-frontier-on-real-world-software-engineering\"><p></p><h2><span>Pushing the frontier on real-world software engineering</span></h2><p></p></div><p><span>GPT‑5.2-Codex builds on </span><a href=\"https://openai.com/index/introducing-gpt-5-2/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>GPT‑5.2’s strengths</span></u>⁠</a><span> in professional knowledge work and </span><a href=\"https://openai.com/index/gpt-5-1-codex-max/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>GPT‑5.1-Codex-Max</span></u>⁠</a><span>’s frontier agentic coding and terminal-using capabilities. GPT‑5.2-Codex is now better at long-context understanding, reliable tool calling, improved factuality, and native compaction, making it a more dependable partner for long running coding tasks, while remaining token-efficient in its reasoning.</span></p><p><span>GPT‑5.2-Codex achieves state-of-the-art performance on SWE-Bench Pro and Terminal-Bench 2.0, benchmarks designed to test agentic performance on a wide variety of tasks in realistic terminal environments. It is also much more effective and reliable at agentic coding in native Windows environments, building on capabilities introduced in GPT‑5.1-Codex-Max.</span></p><p><span>With these improvements, Codex is more capable at working in large repositories over extended sessions with full context intact. It can more reliably complete complex tasks like large refactors, code migrations, and feature builds — continuing to iterate without losing track, even when plans change or attempts fail.</span></p><p><span>Stronger vision performance enables GPT‑5.2-Codex to more accurately interpret screenshots, technical diagrams, charts, and UI surfaces shared during coding sessions.</span></p><p><span>Codex can take design mocks and quickly translate them to functional prototypes, and you can pair with Codex to take these prototypes to production.</span></p><div data-multi-columns=\"true\"><!--$--><div><!--$--><div><p></p><h5>Design mock</h5><p></p></div><!--/$--><!--$--><div><p><img alt=\"Design mock used to generate a web prototype with Codex-5.2\" data-nosnippet=\"true\" loading=\"lazy\" width=\"1536\" height=\"1024\" decoding=\"async\" data-nimg=\"1\" sizes=\"(min-width: 1728px) 1728px, 100vw\" srcset=\"https://images.ctfassets.net/kftzwdyauwt9/3zqTyemGGUiGdqzwcSHfqT/7dcff34a7b6f51ce1ed20be8a4ffcbf4/image__5_.png?w=640&amp;q=90&amp;fm=webp 640w, https://images.ctfassets.net/kftzwdyauwt9/3zqTyemGGUiGdqzwcSHfqT/7dcff34a7b6f51ce1ed20be8a4ffcbf4/image__5_.png?w=750&amp;q=90&amp;fm=webp 750w, https://images.ctfassets.net/kftzwdyauwt9/3zqTyemGGUiGdqzwcSHfqT/7dcff34a7b6f51ce1ed20be8a4ffcbf4/image__5_.png?w=828&amp;q=90&amp;fm=webp 828w, https://images.ctfassets.net/kftzwdyauwt9/3zqTyemGGUiGdqzwcSHfqT/7dcff34a7b6f51ce1ed20be8a4ffcbf4/image__5_.png?w=1080&amp;q=90&amp;fm=webp 1080w, https://images.ctfassets.net/kftzwdyauwt9/3zqTyemGGUiGdqzwcSHfqT/7dcff34a7b6f51ce1ed20be8a4ffcbf4/image__5_.png?w=1200&amp;q=90&amp;fm=webp 1200w, https://images.ctfassets.net/kftzwdyauwt9/3zqTyemGGUiGdqzwcSHfqT/7dcff34a7b6f51ce1ed20be8a4ffcbf4/image__5_.png?w=1920&amp;q=90&amp;fm=webp 1920w, https://images.ctfassets.net/kftzwdyauwt9/3zqTyemGGUiGdqzwcSHfqT/7dcff34a7b6f51ce1ed20be8a4ffcbf4/image__5_.png?w=2048&amp;q=90&amp;fm=webp 2048w, https://images.ctfassets.net/kftzwdyauwt9/3zqTyemGGUiGdqzwcSHfqT/7dcff34a7b6f51ce1ed20be8a4ffcbf4/image__5_.png?w=3840&amp;q=90&amp;fm=webp 3840w\" src=\"https://images.ctfassets.net/kftzwdyauwt9/3zqTyemGGUiGdqzwcSHfqT/7dcff34a7b6f51ce1ed20be8a4ffcbf4/image__5_.png?w=3840&amp;q=90&amp;fm=webp\" style=\"max-width: 100%; height: auto; border-radius: 1rem; margin: 2rem 0px;\"></p></div><!--/$--></div><!--/$--><!--$--><div><p></p><h5>Prototype generated by GPT-5.2-Codex</h5><p></p></div><!--/$--></div><div id=\"advancing-the-cyber-frontier\"><p></p><h2><span>Advancing the cyber frontier</span></h2><p></p></div><p><span>When charting performance on one of our core cybersecurity evaluations over time, we see a sharp jump in capability starting with GPT‑5-Codex, another large jump with GPT‑5.1-Codex-Max and now a third jump with GPT‑5.2-Codex. We expect that upcoming AI models will continue on this trajectory. In preparation, we are planning and evaluating as though each new model could reach ‘High’ levels of cybersecurity capability, as measured by our </span><a href=\"https://cdn.openai.com/pdf/18a02b5d-6b67-4cec-ab64-68cdfbddebcd/preparedness-framework-v2.pdf\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><span>Preparedness Framework⁠</span>⁠<span>(opens in a new window)</span></a><span>. While GPT‑5.2-Codex has not yet reached ‘High’ level of cyber capability, we are preparing for future models that cross that threshold. Due to the increased cyber capabilities, we have added additional </span><span>safeguards in the model</span><span> and in the product, which are outlined in the </span><a href=\"https://openai.com/index/gpt-5-2-codex-system-card\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>system card</span></u>⁠</a><span>.</span></p><div id=\"real-world-cyber-capabilities\"><p></p><h2><span>Real-world cyber capabilities</span></h2><p></p></div><p><span>Modern society runs on software, and its reliability depends on strong cybersecurity—keeping critical systems in banking, healthcare, communications, and essential services online, protecting sensitive data, and ensuring people can trust the software they rely on every day. Vulnerabilities can exist long before anyone knows about them, and finding, validating, and fixing them often depends on a community of engineers and independent security researchers equipped with the right tools.</span></p><p><span>On December 11, 2025, the React team published three security vulnerabilities affecting apps built with React Server Components. What made this disclosure notable was not only the vulnerabilities themselves, but how they were uncovered.</span></p><p><span>Andrew MacPherson, a principal security engineer at Privy (a Stripe company), was using GPT‑5.1-Codex-Max with Codex CLI and other coding agents to reproduce and study a different critical React vulnerability disclosed the week prior, known as </span><a href=\"https://react.dev/blog/2025/12/03/critical-security-vulnerability-in-react-server-components\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>React2Shell</span></u>⁠<span>(opens in a new window)</span></a><span> (</span><a href=\"https://nvd.nist.gov/vuln/detail/CVE-2025-55182\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>CVE-2025-55182</span></u>⁠<span>(opens in a new window)</span></a><span>). His goal was to evaluate how well the model could assist with real-world vulnerability research.</span></p><p><span>He initially attempted several zero-shot analyses, prompting the model to examine the patch and identify the vulnerability it addressed. When that did not yield results, he shifted to a higher-volume, iterative prompting approach. When those approaches did not succeed, he guided Codex through standard defensive security workflows—setting up a local test environment, reasoning through potential attack surfaces, and using fuzzing to probe the system with malformed inputs. While attempting to reproduce the original React2Shell issue, Codex surfaced unexpected behaviors that warranted deeper investigation. Over the course of a single week, this process led to the discovery of previously unknown vulnerabilities, which were responsibly disclosed to the React team.</span></p><p><span>This demonstrates how advanced AI systems can materially accelerate defensive security work in widely used, real-world software. At the same time, capabilities that help defenders move faster can also be misused by bad actors.</span></p><p><span>As agentic systems become more capable in cybersecurity-relevant tasks, we are making it a core priority to ensure these advances are deployed responsibly—pairing every gain in capability with stronger safeguards, tighter access controls, and ongoing collaboration with the security community.</span></p><div id=\"empowering-cyberdefense-through-trusted-access\"><p></p><h2><span>Empowering cyberdefense through trusted access</span></h2><p></p></div><p><span>Security teams can run into restrictions when attempting to emulate threat actors, analyze malware to support remediation, or stress test critical infrastructure. We are developing a trusted access pilot to remove that friction for qualifying users and organizations and enable trusted defenders to use frontier AI cyber capabilities to accelerate cyberdefense.</span></p><p><span>Initially the pilot program will be invite-only for vetted security professionals with a track record of responsible vulnerability disclosure and organizations with a clear professional cybersecurity use case. Qualifying participants will get access to our most capable models for defensive use-cases to enable legitimate dual-use work.</span></p><p><span>If you’re a security professional or part of an organization doing ethical security work like vulnerability research or authorized red-teaming, we invite you to express interest in joining and share feedback on what you’d like to see from the program </span><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSea_ptovrS3xZeZ9FoZFkKtEJFWGxNrZb1c52GW4BVjB2KVNA/viewform\" target=\"_blank\" rel=\"noopener noreferrer\"><span>here</span>⁠<span>(opens in a new window)</span></a><span>. </span></p><div id=\"conclusion\"><p></p><h2><span>Conclusion</span></h2><p></p></div><p><span>GPT‑5.2-Codex represents a step forward in how advanced AI can support real-world software engineering and specialized domains like cybersecurity—helping developers and defenders tackle complex, long-horizon work, and strengthening the tools available for responsible security research.</span></p><p><span>By rolling GPT‑5.2-Codex out gradually, pairing deployment with safeguards, and working closely with the security community, we’re aiming to maximize defensive impact while reducing the risk of misuse. What we learn from this release will directly inform how we expand access over time as the software and cyber frontiers continue to advance.</span></p></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/gpt-5-2-codex-system-card",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/gpt-5-2-codex-system-card",
      "title": "Addendum to GPT-5.2 System Card: GPT-5.2-Codex",
      "publishedAt": "Thu, 18 Dec 2025 00:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:19.917Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><article><div><div id=\"introduction\"><p></p><h2><span>Introduction</span></h2><p></p></div><p><span>GPT‑5.2-Codex is our most advanced agentic coding model yet for complex, real-world software engineering. A version of </span><a href=\"https://openai.com/index/introducing-gpt-5-2/\" target=\"_blank\" rel=\"noopener noreferrer\"><span>GPT‑5.2</span>⁠</a><span> optimized for agentic coding in Codex, it includes further improvements on long-horizon work through context compaction, stronger performance on project-scale tasks like refactors and migrations, and improved performance in Windows environments—and significantly stronger cybersecurity capabilities.</span></p><p><span>This system card outlines the comprehensive safety measures implemented for GPT‑5.2-Codex. It details both model-level mitigations, such as specialized safety training for harmful tasks and prompt injections, and product-level mitigations like agent sandboxing and configurable network access.</span></p><p><span>GPT‑5.2-Codex was evaluated under our Preparedness Framework. It is very capable in the cybersecurity domain but does not reach High capability on cybersecurity. We expect current trends of rapidly increasing capability to continue, and for models to cross the High cybersecurity threshold in the near future. Like other recent models, it is being treated as High capability on biology, and is being deployed with the corresponding suite of safeguards we use for other models in the GPT‑5 family. It does not reach High capability on AI self-improvement.</span></p></div><section id=\"citations\" data-testid=\"citations\"><ul><li><a href=\"https://openai.com/research/index/?tags=2025\" target=\"_blank\" rel=\"noopener noreferrer\">2025</a></li><li><a href=\"https://openai.com/research/index/?tags=system-cards\" target=\"_blank\" rel=\"noopener noreferrer\">System Cards</a></li></ul></section></article></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/introducing-gpt-5-2-codex",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/introducing-gpt-5-2-codex",
      "title": "Introducing GPT-5.2-Codex",
      "publishedAt": "Thu, 18 Dec 2025 00:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:22.554Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div><p><span>Today we’re releasing GPT‑5.2-Codex, the most advanced agentic coding model yet for complex, real-world software engineering. GPT‑5.2-Codex is a version of </span><a href=\"https://openai.com/index/introducing-gpt-5-2/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>GPT‑5.2</span></u>⁠</a><span> further optimized for agentic coding in Codex, including improvements on long-horizon work through context compaction, stronger performance on large code changes like refactors and migrations, improved performance in Windows environments, and significantly stronger cybersecurity capabilities.</span></p><p><span>As our models continue to advance along the intelligence frontier, we’ve observed that these improvements also translate to capability jumps in specialized domains such as </span><a href=\"https://openai.com/index/strengthening-cyber-resilience/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>cybersecurity</span></u>⁠</a><span>. For example, just last week, a security researcher using GPT‑5.1-Codex-Max with Codex CLI found and responsibly </span><a href=\"https://react.dev/blog/2025/12/11/denial-of-service-and-source-code-exposure-in-react-server-components\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>disclosed</span></u>⁠<span>(opens in a new window)</span></a><span> a vulnerability in React that could lead to source code exposure.</span></p><p><span>GPT‑5.2-Codex has stronger cybersecurity capabilities than any model we’ve released so far. These advances can help strengthen cybersecurity at scale, but they also raise new dual-use risks that require careful deployment. While GPT‑5.2-Codex does not reach a ‘High’ level of cyber capability under our Preparedness Framework, we’re designing our </span><a href=\"https://openai.com/index/strengthening-cyber-resilience/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>deployment approach</span></u>⁠</a><span> with future capability growth in mind.</span></p><p><span>We're releasing GPT‑5.2-Codex today in all Codex surfaces for paid ChatGPT users, and working towards safely enabling access to GPT‑5.2-Codex for API users in the coming weeks. In parallel, we’re piloting invite-only trusted access to upcoming capabilities and more permissive models for vetted professionals and organizations focused on defensive cybersecurity work. We believe that this approach to deployment will balance accessibility with safety.</span></p><div id=\"pushing-the-frontier-on-real-world-software-engineering\"><p></p><h2><span>Pushing the frontier on real-world software engineering</span></h2><p></p></div><p><span>GPT‑5.2-Codex builds on </span><a href=\"https://openai.com/index/introducing-gpt-5-2/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>GPT‑5.2’s strengths</span></u>⁠</a><span> in professional knowledge work and </span><a href=\"https://openai.com/index/gpt-5-1-codex-max/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>GPT‑5.1-Codex-Max</span></u>⁠</a><span>’s frontier agentic coding and terminal-using capabilities. GPT‑5.2-Codex is now better at long-context understanding, reliable tool calling, improved factuality, and native compaction, making it a more dependable partner for long running coding tasks, while remaining token-efficient in its reasoning.</span></p><p><span>GPT‑5.2-Codex achieves state-of-the-art performance on SWE-Bench Pro and Terminal-Bench 2.0, benchmarks designed to test agentic performance on a wide variety of tasks in realistic terminal environments. It is also much more effective and reliable at agentic coding in native Windows environments, building on capabilities introduced in GPT‑5.1-Codex-Max.</span></p><p><span>With these improvements, Codex is more capable at working in large repositories over extended sessions with full context intact. It can more reliably complete complex tasks like large refactors, code migrations, and feature builds — continuing to iterate without losing track, even when plans change or attempts fail.</span></p><p><span>Stronger vision performance enables GPT‑5.2-Codex to more accurately interpret screenshots, technical diagrams, charts, and UI surfaces shared during coding sessions.</span></p><p><span>Codex can take design mocks and quickly translate them to functional prototypes, and you can pair with Codex to take these prototypes to production.</span></p><div data-multi-columns=\"true\"><!--$--><div><!--$--><div><p></p><h5>Design mock</h5><p></p></div><!--/$--><!--$--><div><p><img alt=\"Design mock used to generate a web prototype with Codex-5.2\" data-nosnippet=\"true\" loading=\"lazy\" width=\"1536\" height=\"1024\" decoding=\"async\" data-nimg=\"1\" sizes=\"(min-width: 1728px) 1728px, 100vw\" srcset=\"https://images.ctfassets.net/kftzwdyauwt9/3zqTyemGGUiGdqzwcSHfqT/7dcff34a7b6f51ce1ed20be8a4ffcbf4/image__5_.png?w=640&amp;q=90&amp;fm=webp 640w, https://images.ctfassets.net/kftzwdyauwt9/3zqTyemGGUiGdqzwcSHfqT/7dcff34a7b6f51ce1ed20be8a4ffcbf4/image__5_.png?w=750&amp;q=90&amp;fm=webp 750w, https://images.ctfassets.net/kftzwdyauwt9/3zqTyemGGUiGdqzwcSHfqT/7dcff34a7b6f51ce1ed20be8a4ffcbf4/image__5_.png?w=828&amp;q=90&amp;fm=webp 828w, https://images.ctfassets.net/kftzwdyauwt9/3zqTyemGGUiGdqzwcSHfqT/7dcff34a7b6f51ce1ed20be8a4ffcbf4/image__5_.png?w=1080&amp;q=90&amp;fm=webp 1080w, https://images.ctfassets.net/kftzwdyauwt9/3zqTyemGGUiGdqzwcSHfqT/7dcff34a7b6f51ce1ed20be8a4ffcbf4/image__5_.png?w=1200&amp;q=90&amp;fm=webp 1200w, https://images.ctfassets.net/kftzwdyauwt9/3zqTyemGGUiGdqzwcSHfqT/7dcff34a7b6f51ce1ed20be8a4ffcbf4/image__5_.png?w=1920&amp;q=90&amp;fm=webp 1920w, https://images.ctfassets.net/kftzwdyauwt9/3zqTyemGGUiGdqzwcSHfqT/7dcff34a7b6f51ce1ed20be8a4ffcbf4/image__5_.png?w=2048&amp;q=90&amp;fm=webp 2048w, https://images.ctfassets.net/kftzwdyauwt9/3zqTyemGGUiGdqzwcSHfqT/7dcff34a7b6f51ce1ed20be8a4ffcbf4/image__5_.png?w=3840&amp;q=90&amp;fm=webp 3840w\" src=\"https://images.ctfassets.net/kftzwdyauwt9/3zqTyemGGUiGdqzwcSHfqT/7dcff34a7b6f51ce1ed20be8a4ffcbf4/image__5_.png?w=3840&amp;q=90&amp;fm=webp\" style=\"max-width: 100%; height: auto; border-radius: 1rem; margin: 2rem 0px;\"></p></div><!--/$--></div><!--/$--><!--$--><div><p></p><h5>Prototype generated by GPT-5.2-Codex</h5><p></p></div><!--/$--></div><div id=\"advancing-the-cyber-frontier\"><p></p><h2><span>Advancing the cyber frontier</span></h2><p></p></div><p><span>When charting performance on one of our core cybersecurity evaluations over time, we see a sharp jump in capability starting with GPT‑5-Codex, another large jump with GPT‑5.1-Codex-Max and now a third jump with GPT‑5.2-Codex. We expect that upcoming AI models will continue on this trajectory. In preparation, we are planning and evaluating as though each new model could reach ‘High’ levels of cybersecurity capability, as measured by our </span><a href=\"https://cdn.openai.com/pdf/18a02b5d-6b67-4cec-ab64-68cdfbddebcd/preparedness-framework-v2.pdf\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><span>Preparedness Framework⁠</span>⁠<span>(opens in a new window)</span></a><span>. While GPT‑5.2-Codex has not yet reached ‘High’ level of cyber capability, we are preparing for future models that cross that threshold. Due to the increased cyber capabilities, we have added additional </span><span>safeguards in the model</span><span> and in the product, which are outlined in the </span><a href=\"https://openai.com/index/gpt-5-2-codex-system-card\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>system card</span></u>⁠</a><span>.</span></p><div id=\"real-world-cyber-capabilities\"><p></p><h2><span>Real-world cyber capabilities</span></h2><p></p></div><p><span>Modern society runs on software, and its reliability depends on strong cybersecurity—keeping critical systems in banking, healthcare, communications, and essential services online, protecting sensitive data, and ensuring people can trust the software they rely on every day. Vulnerabilities can exist long before anyone knows about them, and finding, validating, and fixing them often depends on a community of engineers and independent security researchers equipped with the right tools.</span></p><p><span>On December 11, 2025, the React team published three security vulnerabilities affecting apps built with React Server Components. What made this disclosure notable was not only the vulnerabilities themselves, but how they were uncovered.</span></p><p><span>Andrew MacPherson, a principal security engineer at Privy (a Stripe company), was using GPT‑5.1-Codex-Max with Codex CLI and other coding agents to reproduce and study a different critical React vulnerability disclosed the week prior, known as </span><a href=\"https://react.dev/blog/2025/12/03/critical-security-vulnerability-in-react-server-components\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>React2Shell</span></u>⁠<span>(opens in a new window)</span></a><span> (</span><a href=\"https://nvd.nist.gov/vuln/detail/CVE-2025-55182\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>CVE-2025-55182</span></u>⁠<span>(opens in a new window)</span></a><span>). His goal was to evaluate how well the model could assist with real-world vulnerability research.</span></p><p><span>He initially attempted several zero-shot analyses, prompting the model to examine the patch and identify the vulnerability it addressed. When that did not yield results, he shifted to a higher-volume, iterative prompting approach. When those approaches did not succeed, he guided Codex through standard defensive security workflows—setting up a local test environment, reasoning through potential attack surfaces, and using fuzzing to probe the system with malformed inputs. While attempting to reproduce the original React2Shell issue, Codex surfaced unexpected behaviors that warranted deeper investigation. Over the course of a single week, this process led to the discovery of previously unknown vulnerabilities, which were responsibly disclosed to the React team.</span></p><p><span>This demonstrates how advanced AI systems can materially accelerate defensive security work in widely used, real-world software. At the same time, capabilities that help defenders move faster can also be misused by bad actors.</span></p><p><span>As agentic systems become more capable in cybersecurity-relevant tasks, we are making it a core priority to ensure these advances are deployed responsibly—pairing every gain in capability with stronger safeguards, tighter access controls, and ongoing collaboration with the security community.</span></p><div id=\"empowering-cyberdefense-through-trusted-access\"><p></p><h2><span>Empowering cyberdefense through trusted access</span></h2><p></p></div><p><span>Security teams can run into restrictions when attempting to emulate threat actors, analyze malware to support remediation, or stress test critical infrastructure. We are developing a trusted access pilot to remove that friction for qualifying users and organizations and enable trusted defenders to use frontier AI cyber capabilities to accelerate cyberdefense.</span></p><p><span>Initially the pilot program will be invite-only for vetted security professionals with a track record of responsible vulnerability disclosure and organizations with a clear professional cybersecurity use case. Qualifying participants will get access to our most capable models for defensive use-cases to enable legitimate dual-use work.</span></p><p><span>If you’re a security professional or part of an organization doing ethical security work like vulnerability research or authorized red-teaming, we invite you to express interest in joining and share feedback on what you’d like to see from the program </span><a href=\"https://docs.google.com/forms/d/e/1FAIpQLSea_ptovrS3xZeZ9FoZFkKtEJFWGxNrZb1c52GW4BVjB2KVNA/viewform\" target=\"_blank\" rel=\"noopener noreferrer\"><span>here</span>⁠<span>(opens in a new window)</span></a><span>. </span></p><div id=\"conclusion\"><p></p><h2><span>Conclusion</span></h2><p></p></div><p><span>GPT‑5.2-Codex represents a step forward in how advanced AI can support real-world software engineering and specialized domains like cybersecurity—helping developers and defenders tackle complex, long-horizon work, and strengthening the tools available for responsible security research.</span></p><p><span>By rolling GPT‑5.2-Codex out gradually, pairing deployment with safeguards, and working closely with the security community, we’re aiming to maximize defensive impact while reducing the risk of misuse. What we learn from this release will directly inform how we expand access over time as the software and cyber frontiers continue to advance.</span></p></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/openai-academy-for-news-organizations",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/openai-academy-for-news-organizations",
      "title": "Introducing OpenAI Academy for News Organizations",
      "publishedAt": "Wed, 17 Dec 2025 06:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:21.045Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div id=\"main\" tabindex=\"-1\"><span aria-live=\"polite\" aria-atomic=\"true\">OpenAI</span><article><div><p>Working with the American Journalism Project and The Lenfest Institute to launch a new learning hub for journalists and publishers using AI.</p></div><div><p><span>At OpenAI, we believe journalism is essential to a healthy democracy. People depend on reliable local and national reporting to understand their communities and the world around them, and we’re committed to being a strong partner to news organizations—supporting their work and convening the right people to move the industry forward.</span></p><p><span>We shared this initiative yesterday at the </span><a href=\"https://brown.columbia.edu/ai-summit/\" target=\"_blank\" rel=\"noopener noreferrer\"><u><span>AI and Journalism Summit</span></u>⁠<span>(opens in a new window)</span></a><span>, which we co-hosted with the Brown Institute for Media Innovation and Hearst, which brought together leaders from the newsroom, academic, and technology community. The OpenAI Academy for News Organizations goes live today with hands-on training, playbooks, and real-world examples that help teams save time and focus on high-impact journalism – from reporting and fact-gathering, to the business and operational work that keeps news organizations strong.</span></p><p><span>At launch, the OpenAI Academy for News Organizations includes:</span></p><div><ul><li><b><span>On-demand training</span></b><span>, including </span><i><span>AI Essentials for Journalists</span></i><span>, which introduces core AI concepts and newsroom-relevant use cases, along with sessions for more technical and product-focused teams exploring advanced tools and custom solutions to their business needs.&nbsp;</span></li><li><b><span>Practical use cases</span></b><span> focusing on investigative and background research, translation and multilingual reporting, data analysis, and production efficiency.&nbsp;</span></li><li><b><span>Open-source projects and shared resources</span></b><span> to make it easier for other news organizations to adapt for their own needs.&nbsp;</span></li><li><b><span>Guidance on responsible uses, </span></b><span>including tips and examples for developing internal policies and governance frameworks.&nbsp;</span></li></ul></div><p><span>The OpenAI Academy for News Organizations builds on years of collaboration with the journalism community, including our ongoing work with American Journalism Project and The Lenfest Institute to support local news organizations as they responsibly adopt AI to assist in critical newsroom research and investigative work, accelerate product development, and explore new ways of using AI to improve business sustainability. Those efforts, alongside partnerships with publishers and industry groups around the world, helped inform the new Academy’s emphasis on practical guidance, transparency, and shared learning grounded in real newsroom needs.</span></p><p><span>AI is already reshaping how newsrooms work, and the Academy is intended to provide immediate value. We recognize that adopting new technology raises important questions for journalists and publishers, including concerns about trust, accuracy, and jobs. The Academy is built with those realities in mind.&nbsp;</span></p><p><span>We’ve invested for years in supporting a healthy news ecosystem while helping more than 800 million weekly ChatGPT users access timely, high-quality information from trusted news outlets. Today, that work includes partnerships with organizations such as News Corp, Axios, the Financial Times, Condé Nast, and Hearst as well as collaborations with groups like AJP, The Lenfest Institute, the World Association of News Publishers (WAN-IFRA), and the International News Media Association (INMA) that focus on shared learning and capacity building across the industry. Together, these partners provide content in more than 20 languages globally.&nbsp;</span></p><p><span>In the year ahead, we plan to work with additional news organizations and industry partners to expand the Academy with new courses, case studies, and live programming. Through this effort, we’ll provide tools to help journalists everywhere do their work even better and support news organizations as they build for the future.</span></p></div><section id=\"citations\" data-testid=\"citations\"><ul><li><a href=\"https://openai.com/news/?tags=community\" target=\"_blank\" rel=\"noopener noreferrer\">Community</a></li><li><a href=\"https://openai.com/news/?tags=2025\" target=\"_blank\" rel=\"noopener noreferrer\">2025</a></li></ul></section></article></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    },
    {
      "id": "https://openai.com/index/developers-can-now-submit-apps-to-chatgpt",
      "sourceType": "rss",
      "sourceName": "Unknown",
      "url": "https://openai.com/index/developers-can-now-submit-apps-to-chatgpt",
      "title": "Developers can now submit apps to ChatGPT",
      "publishedAt": "Wed, 17 Dec 2025 00:00:00 GMT",
      "fetchedAt": "2026-01-26T12:08:20.158Z",
      "summary": "",
      "fullText": "<div id=\"readability-page-1\" class=\"page\"><div id=\"main\" tabindex=\"-1\"><span aria-live=\"polite\" aria-atomic=\"true\">OpenAI</span><nav inert=\"\"></nav><article><div><p>We’re opening app submissions for review and publication in ChatGPT, and users can discover apps in the app directory.</p></div><div><p><span>We’re also introducing an app directory right inside ChatGPT, where users can browse featured apps or search for any published app. The app directory is discoverable from the tools menu or directly from chatgpt.com/apps. Developers can also use deep links on other platforms to send users right to their app page in the directory.</span></p><p><span>Once users connect to apps, apps can get triggered during conversations when @ mentioned by name, or when selected from the tools menu. We’re also experimenting with ways to surface relevant, helpful apps directly within conversations—using signals like conversational context, app usage patterns, and user preferences—and giving users clear ways to provide feedback.</span></p><div id=\"building-submitting-and-monetizing-apps\"><p></p><h2><span>Building, submitting and monetizing apps</span></h2><p></p></div><p><span>Building a great ChatGPT app starts with designing for real user intent. Developers can use the Apps SDK—now in beta—to build chat-native experiences that bring context and action directly into ChatGPT. The strongest apps are tightly scoped, intuitive in chat, and deliver clear value by either completing real-world workflows that start in conversation or enabling new, fully AI-native experiences inside ChatGPT. We recommend reviewing the </span><a href=\"https://developers.openai.com/apps-sdk/app-developer-guidelines\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><span>app submission guidelines</span>⁠<span>(opens in a new window)</span></a><span> early to help you build a high-quality app. Additional documentation and examples are available in the </span><a href=\"https://developers.openai.com/apps-sdk\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><span>developer resource hub</span>⁠<span>(opens in a new window)</span></a><span>.</span></p><p><span>Once ready, developers can submit apps for review and track approval status in the </span><a href=\"https://platform.openai.com/apps-manage\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><span>OpenAI Developer Platform</span>⁠<span>(opens in a new window)</span></a><span>. Submissions include MCP connectivity details, testing guidelines, directory metadata, and country availability settings. The first set of approved apps will begin rolling out gradually in the new year. Apps that meet our quality and safety standards are eligible to be published in the app directory, and apps that resonate with users may be featured more prominently in the directory or recommended by ChatGPT in the future.</span></p><p><span>In this early phase, developers can link out from their ChatGPT apps to their own websites or native apps to complete transactions for physical goods. We’re exploring additional monetization options over time, including digital goods, and will share more as we learn from how developers and users build and engage.</span></p><div id=\"safety-and-privacy\"><p></p><h2><span>Safety and privacy</span></h2><p></p></div><p><span>All developers are required to follow the </span><a href=\"https://developers.openai.com/apps-sdk/app-developer-guidelines\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\" rel=\"noopener noreferrer\"><span>app submission guidelines</span>⁠<span>(opens in a new window)</span></a><span> around safety, privacy, and transparency. Apps must comply with OpenAI’s usage policies, be appropriate for all audiences, and adhere to third-party terms of service when accessing their content. Developers must include clear privacy policies with every app submission and we require developers to only request the information needed to make their apps work.</span></p><p><span>When a user connects to a new app, we will disclose what types of data may be shared with the third party and provide the app’s privacy policy for review. And users are always in control: disconnect an app at any time, and it immediately loses access. </span></p><div id=\"looking-ahead\"><p></p><h2><span>Looking ahead</span></h2><p></p></div><p><span>This is just the beginning. Over time, we want apps in ChatGPT to feel like a natural extension of the conversation, helping people move from ideas to action, while building a thriving ecosystem for developers. As we learn from developers and users, we’ll continue refining the experience for everyone. We also plan to grow the ecosystem of apps in ChatGPT, make apps easier to discover, and expand the ways developers can reach users and monetize their work.</span></p></div><section id=\"citations\" data-testid=\"citations\"><ul><li><a href=\"https://openai.com/news/?tags=2025\" target=\"_blank\" rel=\"noopener noreferrer\">2025</a></li></ul></section></article></div></div>",
      "topics": [],
      "entities": [],
      "embedding": [],
      "scrutiny": {
        "integrityScore": 100,
        "flags": []
      }
    }
  ]
}